[{"categories":["tech"],"content":"How to test gRPC service with production data in testing environments, cloning (or \"shadowing\") traffic via NGINX\n","date":"2021-10-25","objectID":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/","tags":["kubernetes","infrastructure","SRE"],"title":"gRPC Traffic Mirroring With Ingress-Nginx on K8s","uri":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/"},{"categories":["tech"],"content":"In a previous post we saw an NGINX configuration to allow gRPC traffic mirroring. Is the same technique applicable on Kubernetes? Yes! Using the ingress-nginx ingress controller! ","date":"2021-10-25","objectID":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/:0:0","tags":["kubernetes","infrastructure","SRE"],"title":"gRPC Traffic Mirroring With Ingress-Nginx on K8s","uri":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/"},{"categories":["tech"],"content":"Traffic mirroringUse the following configurations snippets in the ingress-nginx configMap and in the Ingress manifest to mirror all traffic to a separate gRPC server. ConfigMapReplace grpc-backend.company.net and grpc-mirror.company.net with the original and mirror endpoint, respectively. http-snippet:|server { listen 127.0.0.1:9443 ssl http2; server_name grpc-backend.company.net; grpc_ssl_protocols TLSv1.3; ssl_certificate_by_lua_block { certificate.call() } location / { grpc_pass grpcs://grpc-mirror.company.net:443; } } IngressAdd the following annotations in the Ingress manifest defining the endpoint of your gRPC service. nginx.ingress.kubernetes.io/backend-protocol:GRPCnginx.ingress.kubernetes.io/configuration-snippet:|mirror /mirror;nginx.ingress.kubernetes.io/server-snippet:|location = /mirror { internal; proxy_set_header X-Mirrored-From $http_host; proxy_pass https://127.0.0.1:9443$request_uri; } NGINX configuration detailsCompared to the non-K8s version we have some differences: In the main nginx.conf file, applied through the configMap, we have a weird section. ssl_certificate_by_lua_block { certificate.call() } This is a something I discovered while looking in the ingress-nginx source: it’s a helper used to load the right TLS certificate which is impossible to do otherwise, because TLS certificates are stored in Kubernetes secrets, instead of normal files. This replaces the TLS certificate loading directives. The rest is unchanged. The Ingress resource manifest contains the annotation to configure a gRPC backend nginx.ingress.kubernetes.io/backend-protocol: GRPC and has 2 important settings. The first snippet nginx.ingress.kubernetes.io/configuration-snippet: | mirror /mirror; adds the mirroring directive to the virtual server location, to copy the gRPC traffic to the /mirror internal location. The second snippet nginx.ingress.kubernetes.io/server-snippet: | location = /mirror { internal; proxy_set_header X-Mirrored-From $http_host; proxy_pass https://127.0.0.1:9443$request_uri; } creates the internal location that will proxy the traffic to the additional server created in the configMap above. That’s it for copying all traffic from an ingress to a separate server! But what if we’d like to only mirror a portion of the traffic? At the end of a previous post I left as a homework for the readers to discover how to copy only a percentage of traffic. Read on to see how to achieve it. ","date":"2021-10-25","objectID":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/:0:1","tags":["kubernetes","infrastructure","SRE"],"title":"gRPC Traffic Mirroring With Ingress-Nginx on K8s","uri":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/"},{"categories":["tech"],"content":"Bonus: mirror a part of trafficNGINX has a split_clients module that is capable of setting a variable based on the distribution of an input. The variable can be used in virtual servers to apply conditional configurations. The syntax to configure the module is split_clients \u003cinput string\u003e \u003cvariable\u003e { 5% something; 10% nothing; * \"\"; } with the value of \u003cvariable\u003e being set based on the hash of \u003cinput string\u003e: this can be anything that NGINX assigns when processing a request. The important detail to understand of the above configurations, is how to choose the input string: the percentage defines the portion of hash values that will yield in \u003cvariable\u003e the value to its right. Let’s have a look at the configs. NGINX configMap: http-snippet:|split_clients \"${remote_addr}mirror${request_uri}\" $mirror_backend { 10% 1; * \"\"; } server { listen 127.0.0.1:9443 ssl http2; server_name original.domain.com; grpc_ssl_protocols TLSv1.3; ssl_certificate_by_lua_block { certificate.call() } location / { grpc_pass grpcs://destination.domain.com:443; } } Ingress manifest: nginx.ingress.kubernetes.io/backend-protocol:GRPCnginx.ingress.kubernetes.io/configuration-snippet:|mirror /mirror;nginx.ingress.kubernetes.io/server-snippet:|location = /mirror { internal; if ($mirror_backend = \"\") { return 200; } proxy_set_header X-Mirrored-From $http_host; proxy_pass https://127.0.0.1:9443$request_uri; } The additions to the previous config: with split_clients in the main nginx.conf file we set a variable $mirror_backend with a non-empty string when the hash of \"${remote_addr}mirror${request_uri}\" falls in the first 10% of all possible hash values. The if added in the Ingress manifest will only proxy traffic when the $mirror_backend variable is not empty. The hash value can be from 0 to 4294967295 (NGINX uses MurMurHash2, returning a 32-bit integer); the percentages written in the configuration create segments of the whole hash space in a contiguous manner, starting from 0. In the example above where we defined 5%, 10% and *, you will have 3 ranges of possible values for \u003cvariable\u003e: 5% -\u003e hash values from 0 to 214748364 10% -\u003e hash values from 214748365 to 429496728 * -\u003e all remaining hash values from 429496729 to 4294967295 Therefore, the probability of getting each value is not exactly the same of the percentage configured, because the distribution of hashes tightly depends on the input. For example, if you have most of your traffic from a few $remote_addresses, you don’t want to set it as input alone. The more you have a sparse distribution of values in the input variable, the better the filter will work. This is why in the example configuration, I added $request_uri to the input, concatenating with a constant string mirror: this highly increases the entropy of the hashes, making the percentage more reliable. An important property of hashing the input is that it’s deterministic, so if you want to mirror exactly for a subset of requests, you can do it: define the percentages to include only the portion of hashes that you want to be copied. For example, if you want to mirror only traffic for a certain URI, as in: $request_uri = `/bank.Service/askForTransactions` murmurhash2 = 1227040391 range percentage = 28.57% then the split_clients config would be split_clients $request_uri $mirror_backend { 28.5699% \"\"; 28.57% 1; * \"\"; } ","date":"2021-10-25","objectID":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/:0:2","tags":["kubernetes","infrastructure","SRE"],"title":"gRPC Traffic Mirroring With Ingress-Nginx on K8s","uri":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/"},{"categories":["tech"],"content":"Drawbacks and limitationsCopying traffic using this technique is simple and effective, but it has a cost: we have a number of TCP connections that are dedicated to serving the cloned traffic, even if going through the loopback interface. You will notice from the ingress-nginx controller metrics that enabling the virtual server via the configMap does not create more connections immediately, but as soon as you configure an Ingress to mirror using the new server, there will be an increase in the average open connections. This is expected, because of the non-native way we are doing mirroring for gRPC traffic. The same applies to memory and CPU usage: handling more connections, and decrypting and re-encrypting every gRPC call will come with a resource overhead. One more thing to note: this technique might be working with gRPC streams, but I was only able to test it with unary RPCs. ","date":"2021-10-25","objectID":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/:0:3","tags":["kubernetes","infrastructure","SRE"],"title":"gRPC Traffic Mirroring With Ingress-Nginx on K8s","uri":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/"},{"categories":["tech"],"content":"CreditThanks again to Joni (@mejofi) for helping me find the original gRPC traffic mirroring configuration. The partial mirroring addition is taken from this nice blog post by Alex Dzyoba https://alex.dzyoba.com/blog/nginx-mirror/ ","date":"2021-10-25","objectID":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/:0:4","tags":["kubernetes","infrastructure","SRE"],"title":"gRPC Traffic Mirroring With Ingress-Nginx on K8s","uri":"/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/"},{"categories":["tech"],"content":"An NGINX configuration trick to mirror traffic to gRPC backends","date":"2021-05-30","objectID":"/post/grpc-traffic-mirroring-using-nginx/","tags":["SRE","infrastructure"],"title":"gRPC Traffic Mirroring Using NGINX","uri":"/post/grpc-traffic-mirroring-using-nginx/"},{"categories":["tech"],"content":"Recently at work with the Optimyze team we faced the necessity of copying traffic from our current customer-facing environment to a new environment. We have assumptions and ideas about architectural changes that cannot be validated only with synthetic tests and require cloning traffic to a separate, internal testing environment. There is no better test than the one performed with real-world data: when you hear speaking about testing in production, a deployment of a new feature to “see what happens” is not what I have in mind. I think of techniques that allow testing with production such as traffic mirroring (or shadowing), canary releases, A/B testing and feature flags. These techniques will allow gathering data directly from your customers, in the form of raw input data (web/API requests) or feedback recorded from customers' interaction with a new product version. In the case of traffic mirroring we don’t introduce any changes to a production system, we simply copy the traffic (entirely or a portion of it) into a different system, for internal analysis. When applying this technique, security considerations apply: you do not want to leak customers data into a non-production system, so ensure you apply the same security policies to the shadow infrastructure receiving a copy of production traffic. ","date":"2021-05-30","objectID":"/post/grpc-traffic-mirroring-using-nginx/:0:0","tags":["SRE","infrastructure"],"title":"gRPC Traffic Mirroring Using NGINX","uri":"/post/grpc-traffic-mirroring-using-nginx/"},{"categories":["tech"],"content":"TL;DRIf you’re in a rush and don’t care about the details, the solution is here. ","date":"2021-05-30","objectID":"/post/grpc-traffic-mirroring-using-nginx/:0:1","tags":["SRE","infrastructure"],"title":"gRPC Traffic Mirroring Using NGINX","uri":"/post/grpc-traffic-mirroring-using-nginx/"},{"categories":["tech"],"content":"The problemWe ♥ NGINX! We knew it has the ability to serve as reverse proxy for gRPC services, and it has built-in support for mirroring. What we didn’t know is that mirroring gRPC traffic has a caveat: URI rewrite is not possible within the grpc_pass directive. This means that traffic copied over to a new location (as described in the mirroring module docs) will have an URI set with the mirror location, making the service unable to map it with any RPC when received. ","date":"2021-05-30","objectID":"/post/grpc-traffic-mirroring-using-nginx/:0:2","tags":["SRE","infrastructure"],"title":"gRPC Traffic Mirroring Using NGINX","uri":"/post/grpc-traffic-mirroring-using-nginx/"},{"categories":["tech"],"content":"Debugging NGINX capabilitiesNGINX documentation is not very rich of examples or references to complex configurations, it is often left to the user to build the desired outcome using the various building blocks offered by the modules. In the case of traffic mirroring, the documentation states: Sets the URI to which an original request will be mirrored. Several mirrors can be specified on the same configuration level. In first instance we thought we could simply copy directly the traffic together with grpc_pass in a naive configuration like the following (not working as intended!): server { listen 443 ssl http2; server_name grpc-backend.company.net; ssl_certificate /etc/letsencrypt/live/company.net/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/company.net/privkey.pem; root /var/www/default; location / { mirror /grpc-mirror; grpc_pass grpc://production-upstream:12345; } location = /grpc-mirror { internal; grpc_pass grpcs://mirror-upstream:443; } } Note 2 things here: we are using NGINX to do TLS termination and we have a production service listening at production-upstream:12345 for unencrypted traffic we are re-encrypting traffic when copying over to the mirror because we want to emulate the full end-to-end traffic that a client would generate, and that includes TLS termination This may not apply to you but I prefer to sponsor a secure-by-default configuration, you can get free and automated TLS certificates with LetsEncrypt. We noticed the shadow environment was not processing data, so it was time to enable some debugging and see what was happening. We add some logging to help us troubleshoot what is going on. location / { mirror /grpc-mirror; grpc_pass grpc://production-upstream:12345; } location = /grpc-mirror { internal; grpc_pass grpcs://mirror-upstream:443; } error_log debug; We can now find these messages in the logs: 2021/05/27 20:15:51 [debug] 15765#15765: *1 http upstream request: \"/grpc-mirror?\" 2021/05/27 20:15:51 [debug] 15765#15765: *1 grpc header: \"grpc-message: malformed method name: \"/grpc-mirror\"\" As documented, the mirror-upstream server receives a request with URI /grpc-mirror, making it useless on the receiving end and creating a path mismatch. At this point we thought native gRPC mirroring was not possible with NGINX and we tried to copy the traffic via the kernel network stack. So we introduced a new server on a different port, willing to direct there the traffic outside of NGINX. server { listen 9443 ssl http2; ssl_certificate /etc/letsencrypt/live/company.net/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/company.net/privkey.pem; root /var/www/default; server_name grpc-backend.company.net; location / { grpc_pass grpcs://mirror-upstream:443; } } server { listen 443 ssl http2; ssl_certificate /etc/letsencrypt/live/company.net/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/company.net/privkey.pem; root /var/www/default; server_name grpc-backend.company.net; location / { grpc_pass grpc://production-upstream:12345; } } When we thought about cloning raw TCP traffic we also had to include TLS termination for the cloned traffic. With this we are thinking we’ll keep the same desired testing properties because the traffic will be cloned without NGINX knowing it, so we tried with iptables rules using the TEE target only to realize minutes later that cloning TCP packets simply does not work because the protocol is stateful and there is no way for the kernel to handle duplicated responses from a second client that has never been tracked. Using a separate server is key to the correct solution presented below (thanks @mejofi for the brilliant idea). ","date":"2021-05-30","objectID":"/post/grpc-traffic-mirroring-using-nginx/:0:3","tags":["SRE","infrastructure"],"title":"gRPC Traffic Mirroring Using NGINX","uri":"/post/grpc-traffic-mirroring-using-nginx/"},{"categories":["tech"],"content":"The solutionThe trick is to combine the gRPC module with the proxy module to achieve the URI rewrite that is needed. Here is a working gRPC traffic mirroring configuration: server { listen 127.0.0.1:9443 ssl http2; ssl_certificate /etc/letsencrypt/live/company.net/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/company.net/privkey.pem; root /var/www/default; server_name grpc-backend.company.net; location / { grpc_pass grpcs://mirror-upstream:443; } } server { listen 443 ssl http2; ssl_certificate /etc/letsencrypt/live/company.net/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/company.net/privkey.pem; root /var/www/default; server_name grpc-backend.company.net; location / { mirror /grpc-mirror; grpc_pass grpc://production-upstream:12345; } location = /grpc-mirror { internal; proxy_pass https://127.0.0.1:9443$request_uri; } } In the snippets, the definition of upstream servers has been omitted. The configuration is not valid without them! What we have here: the second server now listens on the loopback interface to avoid traffic leaving the NIC the un-encrypted traffic is mirrored to an internal location a proxy_pass directive that will set the original URI in the next upstream, proxying the traffic the loopback server the grpc_pass in the loopback server will re-encrypt and pass the request to the shadow environment, but with the right gRPC path URI Pay attention: the previous configuration will copy all traffic from a production gRPC service into a shadow environment. It is left as an exercise to the reader to find the proper way of shadowing only a portion of the traffic (hint: check the split_clients_module). A further improvement could be to use a UNIX socket for the server that will proxy requests to mirror-upstream, on some OSes it might be faster than TCP proxying. I didn’t test it yet but it looks like a possible combination offered by listen and proxy_pass (the URI rewrite should be possible). ","date":"2021-05-30","objectID":"/post/grpc-traffic-mirroring-using-nginx/:0:4","tags":["SRE","infrastructure"],"title":"gRPC Traffic Mirroring Using NGINX","uri":"/post/grpc-traffic-mirroring-using-nginx/"},{"categories":["tech"],"content":"ConclusionsTraffic mirroring is a key enabler for so many scenarios that it should be a first class citizen in any major webserver, for every protocol. NGINX configuration can be treated as an art, given the infinite possibilities the server offers with its modularity. I hope this post will save you some hours of debugging and will increase your testing capabilities, I decided to write it because I could not find a complete answer online do the question: “can NGXIN mirror gRPC traffic?”. And the answer is “yes”! ","date":"2021-05-30","objectID":"/post/grpc-traffic-mirroring-using-nginx/:0:5","tags":["SRE","infrastructure"],"title":"gRPC Traffic Mirroring Using NGINX","uri":"/post/grpc-traffic-mirroring-using-nginx/"},{"categories":["tech"],"content":"I have been using micro-committing for some time now, during which I have adapted the usage of this technique to my needs, bringing it to a level that makes me more productive than ever in software development. Combining micro-committing with Git, while doing TDD is now my favorite development experience: I like how this workflow helps to deliver changes with speed and confidence. This is not a one-size-fits-all approach, I’m sharing what works great for me; I hope some parts of what follows will help you and your team as well. ","date":"2021-04-03","objectID":"/post/micro-committing-with-git/:0:0","tags":["software-engineering","practices"],"title":"Micro-committing with Git","uri":"/post/micro-committing-with-git/"},{"categories":["tech"],"content":"What is micro-committing?Despite micro-commits are not new, there is not much literature or a formal definition of it. The concept is very simple: when developing or refactoring, use the SCM at every notable step creating a small commit. Sizing properly micro-commits is what takes more practice mastering: I think it’s a very personal thing, one might be comfortable with lots of very tiny commits, some other with slightly larger bundles of changes, the important factor is usability. We want to store in a commit every relevant code that produces an increment towards a goal. Why should you care?I found that using micro-commits makes working iteratively safer: the confidence I gain in adding or changing code once I am backed by a “diffable history”, is priceless. Keeping track of changes at a smaller rate enables to move back and forth into history to do local optimizations. Having these changes recorded with Git might not be useful all the time, but when they’re needed they are going to be very valuable. ","date":"2021-04-03","objectID":"/post/micro-committing-with-git/:0:1","tags":["software-engineering","practices"],"title":"Micro-committing with Git","uri":"/post/micro-committing-with-git/"},{"categories":["tech"],"content":"How to make the best out of itIn theory, the smaller the change bundle recorded in every commit, the better! While keeping a long list of tiny changes locally can have several benefits, it can also turn your review process and CI pipelines into chaos. That’s why we need to be careful in managing micro-commits not only during coding but also when preparing changes to be submitted for peer review. During developmentDuring the development phase we want to store locally as many micro-commits as we need to ensure that we can easily move our code back to a working revision. When the development is completed we don’t want to send the micro-commits to the remote repository, because reviewing that history would be difficult. With these concepts in mind, below the things to remember when mirco-committing during the development of new features. never push to the remote repository until the commit history has been reviewed One thing that I learnt recently thanks to my amazing colleagues is that Git history is precious. We don’t want to pollute the tree with too many commits targeting the same feature. Micro-commits are an aid for local development: they should be squashed, rewritten and edited before sending them into a remote repository (more on this below). add a micro-commit for every development iteration unit: every test assertion added, every successful implementation and refactoring We want that every change with a significant impact on the codebase can be consistently fetched. Any IDE Undo/Redo feature does kind of the same, but a micro-commit plays better with refactoring, for example when changing a signature or renaming a package. We don’t want to store in a micro-commit changes that do not have a “business” meaning, like renaming a variable. At the same time we don’t want to store in a micro-commit several changes that map to multiple features: doing so would prevent us to go back in time with fine-grained control, and thus lose the benefits or incremental history. write a short but meaningful commit message: we must be able to understand what we did Optionally, include in the commit message the name of the component or sub-system under change. When re-reading the history of commits, it will be helpful to know which commits are part of which bundle of changes. create a tag when a cycle of development or refactoring is complete, but delete it before pushing (in summary, never push tags used for referencing micro-commits) This is not strictly required: I like to create a simple, non-annotated tag, with the name of the task when I declare it completed. It helps me to point a commit to a task completion in the Git log. Adding a tag can be replaced by details in the commit message: we need to be able to understand which commit resolved a given task. when you are stuck, throw everything away and go back to last commit! This is where this technique really shines: firing up a single command git reset --hard will allow to re-start from what you chose minutes ago as a standing point, a safe place from which a new iteration can start. If you create tags as in the previous bullet, you can even jump multiple micro-commits back, correcting a wider design mistake or a faulty implementation with a newer idea, just specify the tag name you want to reset to. Preparing for peer reviewIt is important to review the Git history before submitting it to the remote repository: we should group multiple micro-commits belonging to the same task into a single commit. The reasons for doing so: peer-review done commit by commit helps reviewers understanding more than the code logic, enriching code with the rationale and the decisions made by the author reverting a single feature should be as easy as reverting a single commit (or a few of them): reducing the cognitive load embedded in the history helps when things go wrong, even if all tests had passed 😁 A good way to achieve a clean history, ready for being reviewed, is with an interactive rebase to the tar","date":"2021-04-03","objectID":"/post/micro-committing-with-git/:0:2","tags":["software-engineering","practices"],"title":"Micro-committing with Git","uri":"/post/micro-committing-with-git/"},{"categories":["tech"],"content":"Wrap upMicro-committing is very helpful to collaborate effectively on a project, it can be tedious in first instance and apparently the benefits are not tangible, though adopting it will be very valuable in some occasions. Try to practice it stick with mataining a clean Git history, then ask for feedback to the people collaborating with you on the repository. ","date":"2021-04-03","objectID":"/post/micro-committing-with-git/:0:3","tags":["software-engineering","practices"],"title":"Micro-committing with Git","uri":"/post/micro-committing-with-git/"},{"categories":["tech"],"content":"CreditsThanks to all the mentors that helped me be more productive! 🙏 Mario Russo for first exposing me to mirco-committing in a coding dojo, some years ago Giancarlo Di Paolantonio and Paolo Banfi for the refactoring/TDD tips Sean Heelan and Tim Ruhsen for the peer review and Git history tips Victor Michel for the mind-blowing Git usage tricks ","date":"2021-04-03","objectID":"/post/micro-committing-with-git/:0:4","tags":["software-engineering","practices"],"title":"Micro-committing with Git","uri":"/post/micro-committing-with-git/"},{"categories":["tech"],"content":"Yes! Yesterday I received an awesome email stating that I cleared the Certified Kubernetes Administrator exam! 😎 Here I want to report my experience in preparing and taking the exam, hopefully this info can help others Kubernetes practitioners get the certification too. ","date":"2019-11-01","objectID":"/post/cka-exam-experience/:0:0","tags":["cka","kubernetes"],"title":"CKA exam experience and preparation","uri":"/post/cka-exam-experience/"},{"categories":["tech"],"content":"PreparationI consider myself lucky because for the past two years I had the chance to use Kubernetes working at lastminute.com; on top this on-the-job training I went through a lot of studying and practicing because the exam itself has a lot of content. Having some expertise of working with Kubernetes is definitely a huge help, as this specific exam is tailored for cluster administrators. You will need to prove a thorough understanding of the Kubernetes primitives, architecture and operational strategies. Do not fear though! The documentation is extensive and very detailed on (most of) the topics that are needed to use and operate a cluster so here’s my suggestions. Exam structure Understand the structure of the exam: 3 hours, 24 questions, 74% score needed to pass. In my experience the first 10 questions will be the easiest, so try to complete them during the first hour; the last 3-4 questions are going to be very long to read and execute so you will need more time in the end. The exam software will not warn you if the question exercise is completed: read carefully the tasks that will mark the exercise complete. Knowledge Read through the whole documentation carefully, at least twice. Get familiar with the structure of the docs: concepts, tasks, reference: they are organized in such a way that the content is mixed up so know where to look for. The following books are recommended read: - Kubernetes up and running - Managing Kubernetes - Kubernetes in action Practice If you have free credits for a cloud provider use them to spin up a cluster to play with; you will need to know well how to interact with the cluster using kubectl CLI. Try all the commands listed in the kubectl cheat sheet and understand their effect on the cluster. Explore the tutorials and tasks sections of the documentation and run them on the cluster to find caveats and errors in the docs. Run through the amazing Kelsey Hightower Kubernetes The Hard Way at least twice; spin up VMs on your PC using Vagrant and try building a cluster from scratch. Push it even further trying to break the cluster in every possible way and observe the effects of failure: systemd logs, kubectl errors, etc… ","date":"2019-11-01","objectID":"/post/cka-exam-experience/:0:1","tags":["cka","kubernetes"],"title":"CKA exam experience and preparation","uri":"/post/cka-exam-experience/"},{"categories":["tech"],"content":"Taking the examI have to say the exam infrastructure is well done: in a single browser window you will have a left menu with the questions and a central section with a tmux terminal. Before starting the exam a proctor will ask to confirm your identity and will validate the exam conditions are met: clean desk (no headset, no phone, no paper), empty and silent room. During the exam you can have drinks but not food, and you can request a break at any time but the clock will continue to run during the break. Some tips to meet the requirements. Workstation Remove everything from your exam desk except: - valid passport or national ID with picture - a glass of water or a juice that can help you stay hydrated - the PC you will run the exam on, the optional external monitor and external mouse/keyboard Connectivity Ensure you have enough bandwidth for a streaming connection (10Mb should be enough), because you will have to share your desktop(s) and show your face through the webcam for the whole time. Prefer WiFi over cable: before the exam can start you will have to pan your camera around to show the proctor the whole room, probably twice. If you (like me) used the notebook integrated webcam you will have to move the screen left and right and all across the room If you’re at home reboot your router one hour before the exam starts, just to be sure. Manage time There’s a timer in the upper left corner of the exam window: use it to check that you are completing enough questions on time. Questions vary from 2 to 8 percent weight; on average you will have 7.5 minutes for evey question, but the last 3/4 are much longer to read and complete as they involve troubleshooting and actions to complete: you probably have to recover/install parts of a cluster. Save time on the first questions to have more for the last ones; when switching from a question to another, check the score percentage of the question so you can evaluate if to do it or skip it with respect to the remaining time. Use the resources There’s a staggering feature in the Kubernetes docs: search. When you are solving an exercise and need a reference or an example, search for it in the documentation! As using the whole *.kubernetes.io domain is permitted during the exam you can consult examples, copy-paste YAML text and commands and even use snippets from the blog posts! Use these resources as soon as you are confronted with a question for which kubectl commands are not enough, reading carefully a doc page or a blog post can get you out of trouble when something peculiar does not work as expected. Another nice feature of the exam is a notepad you can open to take notes and copy-paste and edit content. I used it to take notes on the questions I was not able to complete fully and get back on them during the last 15 minutes of time. ","date":"2019-11-01","objectID":"/post/cka-exam-experience/:0:2","tags":["cka","kubernetes"],"title":"CKA exam experience and preparation","uri":"/post/cka-exam-experience/"},{"categories":["tech"],"content":"The overall exam experienceAlthough the exams rules are quite strict and I admit I was intimated reading the CKA FAQ, setting up the exam room was easy and being able to run the test at home is a major plus compared to force you to go to a certification center. The testing facility ran smoothly thanks to a Chrome extension that is required to run the exam and the instructions on how to complete the exercise were clear enough. ","date":"2019-11-01","objectID":"/post/cka-exam-experience/:0:3","tags":["cka","kubernetes"],"title":"CKA exam experience and preparation","uri":"/post/cka-exam-experience/"},{"categories":["tech"],"content":"Last but not leastIf you purchased the exam from the Linux Foundation or CNCF Foundation you should have a free retake. If you fail the first time don’t worry! CKA is a difficult exam, very rich of content and all hands-on, definitely one of the most difficult certifications I’ve ever done. Try again after you get back studying and practicing on what you could not solve the first time 💪 ","date":"2019-11-01","objectID":"/post/cka-exam-experience/:0:4","tags":["cka","kubernetes"],"title":"CKA exam experience and preparation","uri":"/post/cka-exam-experience/"},{"categories":["tech","work"],"content":"Shipping fast and keeping most of your users happy","date":"2019-05-05","objectID":"/post/progressive-delivery-with-kubernetes/","tags":["containers","kubernetes","platform","continuous-delivery"],"title":"Progressive Delivery with Kubernetes","uri":"/post/progressive-delivery-with-kubernetes/"},{"categories":["tech","work"],"content":"I’m more and more fond of finding the perfect solution to manage application delivery: dev teams want to be fast but their ops counterpart is not happy to loose control over the growing number of deployments that could cause an outage. We as an industry need to find the right balance to have features delivered in time and keep the service up and running for our users! And that’s where progressive delivery can help! What is progressive delivery? It’s the evolution of continuous integration and continuous delivery practices, taken to the extreme - if this is the first time you hear about it, read this excellent post by James Governon. But as of today what are the tools that embed this practice in deployment pipelines? None that I could find ☹️… hence I started this post to share some of the techniques that you can use to achieve progresive delivery today on Kubernetes! I’d be really glad to have any comments and discuss on the matter. ","date":"2019-05-05","objectID":"/post/progressive-delivery-with-kubernetes/:0:0","tags":["containers","kubernetes","platform","continuous-delivery"],"title":"Progressive Delivery with Kubernetes","uri":"/post/progressive-delivery-with-kubernetes/"},{"categories":["tech","work"],"content":"The goalThe progressive delivery manifesto (if there will ever be one) should explain the rationale why delivering feature in parts is better than all at once: feedback. In this Agile world feedback is everything, and the only feedback that matters is your users'; as Jez Humble puts it “Users don’t know what they want. Users know what they don’t want once you’ve built it for them.” You are not going to build anything useful if you don’t collect your users opinion while building the product, that is why having a system that is able to change quickly and that can collect this feedback is so vital to success. ","date":"2019-05-05","objectID":"/post/progressive-delivery-with-kubernetes/:0:1","tags":["containers","kubernetes","platform","continuous-delivery"],"title":"Progressive Delivery with Kubernetes","uri":"/post/progressive-delivery-with-kubernetes/"},{"categories":["tech","work"],"content":"To mesh or not to meshThe first approach to progressive delivery is via infrastructure components. I cheated a bit in the post intro: there actually is a tool combination that is able to implement a feature close to progressive delivery right now: it’s Istio plus Spinnaker; the network mesh in this scenario is a router for connections originating by clients between multiple versions of the same backend, whose deployed versions are managed by Spinnaker releases. The mesh could be another product (Envoy, Linkerd, Consul Connect…) but it is the necessary component that contains the logic to serve the user a specific version of the application, based on goegraphic location rules, latency or even application rules (layer 7). If you want to avoid the burden of installing and maintaining a mesh network you need to manage custom tooling to have the traffic routed for a subset of users to a specific version, Skipper is a good example but comes with the restriction of not being able to manage percentage of traffic, so the percentage of user served is based only on the number of pods configured from service to service (so not ideal for small sized deployments). The other way I see right now is creating a Kubernetes operator and a CustomResourceDefinition that can interact with the Ingress resource: this is hypothetical and I am not aware of any tool that is doing this but it could be posible to configure the ingresses to serve part of the requests by a specific Service (e.g. v1.2.3 backed by a Deployment with a proper selector). As far as I know the current ingress controler based on nginx does not have such feature, but I just discovered writing this line that Traefik does support this! It would be great to understand if Traefik can manage multiple rules at once and if it can be managed via API so that the traffic is gradually moved from service to service. ","date":"2019-05-05","objectID":"/post/progressive-delivery-with-kubernetes/:0:2","tags":["containers","kubernetes","platform","continuous-delivery"],"title":"Progressive Delivery with Kubernetes","uri":"/post/progressive-delivery-with-kubernetes/"},{"categories":["tech","work"],"content":"Feature flagsOf course if you move to the application things get easier in terms of programmability, problem is they tend to be more difficult to manage at scale. If you use one of the multitude of available feature-flag products (also referred as feature toggles) you are soon going to be able to experiment with progressive delivery capabilities; your application will most likely contain the logic required to show a specific user a feature or another. While this is intriguing, if you have more than 2 product teams this can easily become a nightmare because if each team implements its own solution of feature toggle the company as a whole can really struggle to get what type of experience is serving to its users. Change management, for as light as it can be, should still be accounting for features enabled and disabled that may cause a service disuption, even if for a small percentage of users, and when the logic for serving different versions of your system is scattered around multiple applications, this goes quickly out of control. One approach I’ve seen succeed in using software-defined toggle is adopt a centralized, company-wide solution around an existing product: this simplifies greatly the management around features that are delivered passing through multiple services while being able to keep track of changes consitently. ","date":"2019-05-05","objectID":"/post/progressive-delivery-with-kubernetes/:0:3","tags":["containers","kubernetes","platform","continuous-delivery"],"title":"Progressive Delivery with Kubernetes","uri":"/post/progressive-delivery-with-kubernetes/"},{"categories":["tech","work"],"content":"Delivering itOnce you’ve established an infrastructure for serving users based on some policies you should also have in place automation to be able to push your features out. In case you went for the infrastructure/network path you’ll need a deployment tool that can sit between your CI artifacts and the platform running your services; on the contrary for a software-driven solution you will just need an application build and deployed regularly. For the former I am really struggling to find a product that suites my need, I’ve poked around with Spinnaker, ArgoCD and Tekton Pipelines but none of them seems to have the adequate primitives to address my progressive delivery needs. I’d be happy to hear from the community how this is being addressed: I’d like to have a descriptive way of defining multiple versions of artifacts and configurations paired together (maybe via commit hash?) and have all of them deployed at the same time; I’d also like to update the configurations of a given version while it’s running. Seems fair right? I might need to tweak my service here and there, but I’d like to tune it only for a specific set of features that I know are in version ABC. Now I could not find on the internet a single product that works with Kubernetes able to satisfy this requirement, so please if you happen to have something in mind leave a comment! ","date":"2019-05-05","objectID":"/post/progressive-delivery-with-kubernetes/:0:4","tags":["containers","kubernetes","platform","continuous-delivery"],"title":"Progressive Delivery with Kubernetes","uri":"/post/progressive-delivery-with-kubernetes/"},{"categories":["tech","work"],"content":"ConclusionAs always Kubernetes is a great enabler for delivery techniques based on software, it’s an extensible platform and the multiple uses that can lead to achieve progressive delivery just confirm it. Personally I see a lot of space for progressive delivery in the upcoming future, especially for IoT. Let’s see what’s next! ","date":"2019-05-05","objectID":"/post/progressive-delivery-with-kubernetes/:0:5","tags":["containers","kubernetes","platform","continuous-delivery"],"title":"Progressive Delivery with Kubernetes","uri":"/post/progressive-delivery-with-kubernetes/"},{"categories":["tech","culture"],"content":"A recap of the best Go conference in Italy","date":"2018-10-23","objectID":"/post/golab-2018-wrap-up/","tags":["conferences","golang"],"title":"GoLab 2018: Wrap Up","uri":"/post/golab-2018-wrap-up/"},{"categories":["tech","culture"],"content":"I’m in the train back from GoLab 2018 and I am so happy that I attended this conference! It’s been definitely one of most beautiful con I have attended in Italy, with tremendous speakers from all over the globe like Filippo Valsorda, Eleanor McHugh, Ron Evans and Bill Kennedy among many others; I have to say the organizers were just perfect in everything from the venue setup to the workshop organization, as if the quality of the talks ware not enough. I was so delighted I want to write a wrap-up immediately with my head full of ideas for the upcoming future. ","date":"2018-10-23","objectID":"/post/golab-2018-wrap-up/:0:0","tags":["conferences","golang"],"title":"GoLab 2018: Wrap Up","uri":"/post/golab-2018-wrap-up/"},{"categories":["tech","culture"],"content":"Speakers layoutIt was 2 dense days with 45 minutes talks and 4 tracks, 2 each day: Patterns, Embedded (day 1); Web, DX (day 2). Before the talks a keynote each day (Eleanor McHugh and Bill Kennedy respectively), 30 minutes for lightning talks at the end of day 1, cocktails and networking at the end of day 2. ","date":"2018-10-23","objectID":"/post/golab-2018-wrap-up/:0:1","tags":["conferences","golang"],"title":"GoLab 2018: Wrap Up","uri":"/post/golab-2018-wrap-up/"},{"categories":["tech","culture"],"content":"The communityPoeple in the Gophers community is just awesome, and the environment was welcoming and warm in every part of the con; in the afternoon of day 1 a panel on diversity and inclusion was hoste by Cassandra Salisbury and it was really good in the sense that was not the regular D\u0026I talk with practices and models to adopt to “pretend” that you are inclusive, it was actually a discussion and sharing of stories that enable a good community. Because a good community is inclusive by design, and the Go community is very good. During breaks I had the chance to shake hands and chat with many people I only had virtually met on Twitter before. ","date":"2018-10-23","objectID":"/post/golab-2018-wrap-up/:0:2","tags":["conferences","golang"],"title":"GoLab 2018: Wrap Up","uri":"/post/golab-2018-wrap-up/"},{"categories":["tech","culture"],"content":"Things I learntHere’s some of the thing I got to know thanks to the amazing talks I saw: CERN uses Go too! They rebuild their DNS service, see cernops/golbd Google created a project called Flutter to build native mobile apps for multiple platforms [thanks @edoardo849] the go runtime is powerful but also can be tricky in some occasions, especially with closures; when using them =, make sure to clean up the resources they use and ensure local variable are scoped correctly to avoid data races and bugs. Always run tests with -race option and use channels and mutexes to orchestrate your pipelines [thanks @empijei] finite state machines are not only academic lecture, you can make them solve actual problems like decoding different variants of base64 [thanks @annaopss] there are 2 packages for auto-generating Go structs code from an arbitrary input in JSON and XML format: JSONGen and XMLGen because Go is written in Go, you can parse .go files before they are compiled and create your own language extensions (macro) to be then re-compiled in a final binary, crazy! [thanks Max Ghilardi, see cosmos72/gomacro] fnProject is a serverless platform to run function-as-a-service on Oracle cloud and on your private cloud on Kubernetes, as I covered it in this previous post you can manipulate network packets directly with Go! The Linux kernel has a feature to let programs in userspace filter packets based on custom logic, so Telefonica develop (and open sourced) a Go library to manipulate packets because C++ with libevent hadn’t enough throughput, so they moved from connection-based filtering to packet-based filtering (DPI) doubling throughput with 8 times less CPU! [See telefonica/nfqueue] you can close a buffered channel immediately after sending all the items into it, and you’ll still be able to read from it all the values; only then the close(channel) instruction will be magically executed by the runtime. So this is valid [thanks @goinggodotnet] instrumenting and monitoring applications in Kubernetes can be fun! When building an operator you can interact with the exported metrics directly in the controller, you can enrich your systems with distributed traces thanks to OpenTracing/OpenCensus using gRPC you can auto-generate Swagger documentation from protobuf service definitions, thus auto-generating web-browseable documentation of you service [thanks @pawel_slomka] io.Copy() from the standard library is the most efficient way of sending network data from 2 net.Conn instances; this thanks to interface upgrade in the Copy() implementation, ending in leveraging the kernel packet passing in the most efficient way - without involving the application at all! [thanks @filosottile] And these are just a few (IMO the most important) things I got to know and discover these 2 days… What a ride! Can’t wait for next year to be back in Florence! GoLab 2018 ","date":"2018-10-23","objectID":"/post/golab-2018-wrap-up/:0:3","tags":["conferences","golang"],"title":"GoLab 2018: Wrap Up","uri":"/post/golab-2018-wrap-up/"},{"categories":["tech"],"content":"Cloud-native: sounds attractive right? What does it even mean? Wikipedia has no page on it already so anyone can give its own definition… Here’s mine: A Cloud-native application has only concern on the functionalities that it has to deliver as it is completely decoupled from the infrastructure it runs on So how can software delivery be cloud-native? Isn’t software delivery supposed to “install” software onto some infrastructure? Well if your infrastructure provider is cloud-native, you can transitively deliver software on it in a cloud-native way (counts of cloud-native is over 9000, so stopping here)! Recently RedHat acquired CoreOS, bold move if you ask me. CoreOS since the M\u0026A has been very quite until a week ago when the operator-framework was announced through a blog post; this is a huge step forward for everyone as this new toolkit will empower the average developer with the ability to run operators on Kubernetes and package their applications as extensions to the Kubernetes API. Never heard of Custom Resource Definitions? You’d better get on track as this will be driving the next-gen wave of applications that will run as part of the platform that delivers them, with the ability to automate their management and simplify dramatically their operations which will be tightly integrated with the cluster management itself. And as usual I’m eager to try out this new toy and see what can be done with it: I want to build a cloud-native software delivery application that will enable CI/CD jobs to be running inside the cluster and managed by the same API server! Using a CRD for the “Pipeline” kind I can control the build/test/release flow of my application and moreover monitor the whole thing with the same tools with which I will monitor my application. ","date":"2018-05-05","objectID":"/post/cloud-native-software-delivery/:0:0","tags":["platform","continuous-delivery","software"],"title":"Cloud-native applications: Operator-Framework","uri":"/post/cloud-native-software-delivery/"},{"categories":["tech"],"content":"Creating CD³I decided to start a new project called CD³ (cd kube): it will be a Continuous-* software that will run in Kubernetes and will be dedicated to deliver software _through_ Kubernetes. I found Weaveworks did something similar with Flux and since I don’t want to reinvent the wheel I’ll just try and replicate some functionality of running a continer in an operator. First things first: I installed the operator-sdk and I have the binary in my $GOPATH/bin. Running $GOPATH/bin/operator-sdk new cdkube --api-version=delivery.inge.4pr.es/v1alpha1 --kind=Pipeline I am resulting in an auto-generated project with some scaffold code, as in this commit. Next I add some details which I think are at the core of a pipeline: the repository with code and configurations, the image to build the software and what version/name give to the resulting application artifact. When adding new items to the Spec and Status of CRD I will modify the pkg/apis/delivery/v1alpha1/types.go file, then the guide suggests to run operator-sdk generate k8s to update the code, in fact the zz_generated_deepcopy.go is updated, but I don’t see the deploy/cr.yaml changed as it should so probably there’s a bug… Moving forward I have my operator logic to be defined now: I add some check whether the building pod is succeded and build the operator operator-sdk build inge4pres/cdkube:v0.0.1 docker push inge4pres/cdkube:v0.0.1 My operator is built, pushed to dockerhub and ready to be kicked in a k8s cluster, so I fire up one in GKE gcloud beta container --project \"inge4pres-gcp\" clusters create \"operator-framework-test\" --zone \"europe-west1-b\" --machine-type \"g1-small\" --image-type \"COS\" --disk-size \"25\" gcloud container clusters get-credentials operator-framework-test --zone europe-west1-b --project inge4pres-gcp and when it’s ready I can deploy the auto-generated resources to the cluster. An amazing result is that once the depoloyment is done I can create my CRD with the kubectl command and see my custom-type declared, running kubectl create -f deploy/crd.yaml I have a new object created in k8s ➜ ~ kubectl get pipeline NAME AGE doing-nothing 16s ➜ ~ kubectl describe pipeline Name: doing-nothing Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e API Version: delivery.inge.4pr.es/v1alpha1 Kind: Pipeline Metadata: Cluster Name: Creation Timestamp: 2018-05-05T18:08:31Z Deletion Grace Period Seconds: \u003cnil\u003e Deletion Timestamp: \u003cnil\u003e Initializers: \u003cnil\u003e Resource Version: 1528 Self Link: /apis/delivery.inge.4pr.es/v1alpha1/namespaces/default/pipelines/doing-nothing UID: 5160a990-508f-11e8-8f06-42010a8401f8 Spec: Spec: Build Arguments: building something... now I'm done Build Commands: echo Build Image: busybox Repo: http://github.com/inge4pres/just-a-test Target Name: tesApp Target Version: 0.1 Events: \u003cnone\u003e ➜ ~ When I deploy my operator and create the custom resource applying the manifest for a pipeline, the operator picks it up and starts a container with the name of the pipeline ➜ cdkube git:(master) ✗ kubectl apply -f deploy/cr.yaml ➜ cdkube git:(master) ✗ kubectl get po NAME READY STATUS RESTARTS AGE cdkube-57bcf65584-sbvd6 1/1 Running 0 12s ➜ cdkube git:(master) ✗ kubectl apply -f deploy/cr.yaml pipeline \"doing-nothing\" created ➜ cdkube git:(master) ✗ kubectl get po NAME READY STATUS RESTARTS AGE cdkube-57bcf65584-sbvd6 1/1 Running 0 29s testapp 0/1 ContainerCreating 0 2s ➜ cdkube git:(master) ✗ kubectl logs cdkube-57bcf65584-sbvd6 time=\"2018-05-05T18:42:59Z\" level=info msg=\"Go Version: go1.10.2\" time=\"2018-05-05T18:42:59Z\" level=info msg=\"Go OS/Arch: linux/amd64\" time=\"2018-05-05T18:42:59Z\" level=info msg=\"operator-sdk Version: 0.0.5+git\" time=\"2018-05-05T18:42:59Z\" level=info msg=\"starting pipelines controller\" time=\"2018-05-05T18:43:21Z\" level=info msg=\"build still running, status of builder container: Pending\" ➜ cdkube git:(master) ✗ kubectl get po NAME READY STATUS RESTARTS AGE cdkube-57bcf65584-sbvd6 1/1 Running 0 49s testapp 0/1 Completed 2 22s and if I get the","date":"2018-05-05","objectID":"/post/cloud-native-software-delivery/:0:1","tags":["platform","continuous-delivery","software"],"title":"Cloud-native applications: Operator-Framework","uri":"/post/cloud-native-software-delivery/"},{"categories":["tech"],"content":"Getting started with a new CD tool based on containers","date":"2018-02-25","objectID":"/post/continuous-delivery-with-drone/","tags":["devops","docker","containers","platform"],"title":"Continuous Delivery with Drone","uri":"/post/continuous-delivery-with-drone/"},{"categories":["tech"],"content":"Continuous Delivery should be a solved issue: the practice is well-defined and there is a plethora of tools implementing it with more or less peculiarities, but still many struggle implementing it. The dream of a perfect continuous deployment flow from the developer to the production environment with software quality gates based on automated tests is still alive in me, I tried and tried several times with multiple implementations on multiple platforms and never got to the point where I could say: “I’m done, this works exactly as I wanted”. So I stumbled on drone and decided to give it a go: it’s an open-source project, written in Go and with a SaaS offering via their website. The concept I like is that every step of the build/deployment process runs through a container and this is very close to my idea of a modern CD tool, a platform where I can compose pipelines by chaining containers execution on a shared workspace. Love it already. ","date":"2018-02-25","objectID":"/post/continuous-delivery-with-drone/:0:0","tags":["devops","docker","containers","platform"],"title":"Continuous Delivery with Drone","uri":"/post/continuous-delivery-with-drone/"},{"categories":["tech"],"content":"Installing the stackYou can run the drone server and agent locally on your laptop with docker-compose as detailed here. Only issue is: for integrating with any of the big Git cloud provider (Github and Gitlab) you will need to expose your service to the internet, so I’ll use a local instance of gogs running in a docker container from the official image. All I need is in the docker-compose.yml file: I added just a couple of volumes directive compared to the original one and I am using the docker-for-mac internal hostname to resolve the bridge IP internally as detailed here. This is a lab setup and having a production-ready installation will require database setup and filesystem persistence, but I don’t have this requirement now. After a docker-compose up -d my stack is ready. Installing the CLIAs easy as following this guide. Once logged to the web UI I navigate to the ${DRONE_HOST}/account/token page where I can get a token to configure the CLI. ","date":"2018-02-25","objectID":"/post/continuous-delivery-with-drone/:0:1","tags":["devops","docker","containers","platform"],"title":"Continuous Delivery with Drone","uri":"/post/continuous-delivery-with-drone/"},{"categories":["tech"],"content":"Adding secretsThere is a nice feature in drone: I can manage secrets directly from the command line and they can be scoped globally or be available only to one pipeline step (corresponding to an image). I will need to add Dockerhub username and password to the plugin/docker image to be able to push the image, so I add this 2 secrets with the following drone secret add -repository=inge/goapp -image=plugins/docker -name=docker_username -value=inge4pres drone secret add -repository=inge/goapp -image=plugins/docker -name=docker_password -value=*************** ","date":"2018-02-25","objectID":"/post/continuous-delivery-with-drone/:0:2","tags":["devops","docker","containers","platform"],"title":"Continuous Delivery with Drone","uri":"/post/continuous-delivery-with-drone/"},{"categories":["tech"],"content":"A sample pipelineAs many of the continuous delivery tools available on the market, drone uses a YAML configuration file in the root of the repository, so adding a .drone.yml hidden file is enough to start hooking every commit to the build system. I configured a 3 stages pipeline: test and build artifact publish the artifact deploy the application It’s very simple to get started and the one-container-per-step architecture makes it trivial to glue together multiple steps. There is an implicit concept of shared workspace (configurable) that you can leverage to use Makefile and Dockerfile just as the build was happening on your local environment. So I really recommend trying out drone and reporting some issues if you find any, for the time being I am very excited to have a CD product entirely written in Go - I think I will contribute to the project to have some enhancements available in the free version. Below here some screenshots of the drone web UI and the pipeline resulting from the YAML config file: I will explore more complex workflows like promoted builds and gated builds - and build more in the tool if I need. drone repository view drone pipeline log drone stage error drone running ","date":"2018-02-25","objectID":"/post/continuous-delivery-with-drone/:0:3","tags":["devops","docker","containers","platform"],"title":"Continuous Delivery with Drone","uri":"/post/continuous-delivery-with-drone/"},{"categories":["kubernetes","serverless"],"content":"An overview of the options to run functions as a service on k8s","date":"2018-01-30","objectID":"/post/serverless-on-kubernetes/","tags":["serverless","kubernetes","platform","development"],"title":"Serverless on Kubernetes","uri":"/post/serverless-on-kubernetes/"},{"categories":["kubernetes","serverless"],"content":"Kubernetes is the de facto platform for running modern applications: its broad adoption in 2017 and the velocity of the project made it so and it’s been accepted as the standard for many companies, from small to planet scale. It was impossible that such an extensible platform would be left out the serverless party, so here are the 4 main players offering FaaS to be run via k8s. ","date":"2018-01-30","objectID":"/post/serverless-on-kubernetes/:0:0","tags":["serverless","kubernetes","platform","development"],"title":"Serverless on Kubernetes","uri":"/post/serverless-on-kubernetes/"},{"categories":["kubernetes","serverless"],"content":"A premiseIf you’re new to serverless and FaaS and all the previous buzzwords sound like cacophony to your ears, I really recommend reading this post and watching this talk. You could also notice how I put FaaS and serverless under the same hat here, this is just a personal opinion although some might argue that FaaS is a subset of serverless: historically I approached the serverless world using AWS Lambda, and I really tied the idea of writing functions and let someone else manage the infrastructure to the serverless concept. Also Sam Newman gave a good talk on serverless that I really recommend watching. ","date":"2018-01-30","objectID":"/post/serverless-on-kubernetes/:0:1","tags":["serverless","kubernetes","platform","development"],"title":"Serverless on Kubernetes","uri":"/post/serverless-on-kubernetes/"},{"categories":["kubernetes","serverless"],"content":"Why serverless on k8sIt seems like a natural evolution for distributed systems to be composed by smaller and smaller parts. When moving from SOA to microservices the size of the service was reduced to enable development of more fine-grained functionalities into smaller and more maintainable components; taken to the extreme, you can reduce a microservice to be dedicated to just one task or to be made of just one function, that’s where FaaS fits into. Kubernetes is a great activator for such modularity as it creates a very powerful abstraction over infrastructure, so when developing a function as a separate module of a distributed system you can scale both vertically and horizontally any building block, each one independently from another, or you could even let Kubernetes manage that (think Horizontal Pod Autoscaler). ","date":"2018-01-30","objectID":"/post/serverless-on-kubernetes/:0:2","tags":["serverless","kubernetes","platform","development"],"title":"Serverless on Kubernetes","uri":"/post/serverless-on-kubernetes/"},{"categories":["kubernetes","serverless"],"content":"Four players offering FaaS on k8s OpenFaaS Fission Kubeless Fn Project Now there might be others, but this 4 are the ones I mostly heard of in the last 6 months, so they must be the right ones 😁. Comparison criteriaThis is not a technical benchmark on the capabilities of this 4 frameworks: it’s a “look Ma, I can serverless on k8s” post where I try and highlight the pros and cons of adopting one or the other; the criteria will be installation methodology (client and server), languages support, cluster interoperability and developer experience, voted from 0 to 5 the higher the better. I will use Kubernetes 1.8.6 that is, at the moment of writing, the latest available stable version. The target function to deploy will be a super-serious analytics and business intelligence tool that will read the incoming HTTP request body and save it in a JSON document alongside with a timestamp. The JSON will be stored on a REDIS using a random UUIDv4 as key. All the code that will be deployed as functions is in Github, while for installing the GCP cluster and REDIS I used the following gcloud beta container --project \"${GCP_PROJECT}\" clusters create \"serverless-k8s\" \\ --zone \"europe-west2-c\" --username \"admin\" --cluster-version \"1.8.6-gke.0\" \\ --machine-type \"g1-small\" --image-type \"COS\" --disk-size \"50\" --num-nodes \"3\" gcloud container clusters get-credentials serverless-k8s \\ --zone europe-west2-c --project \"${GCP_PROJECT}\" helm init helm install stable/redis Fn ProjectFeatures function configuration via YAML local development server via fn start and fn run uses DockerHub to store functions as containers web UI with function monitoring Client installation: 4The installation instructions are easy to read and execute, multiple platforms supported out of the box. User is required to set an environment variable with a DockerHub handle export FN_REGISTRY=\u003cDOCKERHUB_USERNAME\u003e Server installation: 3A Helm chart is provided under fn-helm but it’s not immediately linked to the project’s page. The installation requires the user to export an environment variable with the command export FN_API_URL=http://$(kubectl get svc --namespace default fn-release-fn-api -o jsonpath='{.status.loadBalancer.ingress[0].ip}'):80 Language support: 5Built-in support for Java, Ruby, Go, Python. Runs any docker container as a function. Cluster interoperability: 2It requires a LoadBalancer resource, so you won’t be able to run it on minikube out of the box. It has MySQL and REDIS as dependency services and uses a DaemonSet for the API controller, which might impact node’s performance. No monitoring provided for the in-cluster components. Developer experience: 3Very extensive CLI interface. Functions are pipes: they should read Stdin and write to Stdout; some environment variables are injected to the running code to detect request URL and other configurations. I was able to complete my function deployment in roughly 1 hour after digging the docs a while to find out how to add custom configurations to the functions via environment variables. OpenFaaSFeatures sponsored by CNCF function grouping configuration via YAML (stack file) public function repository Web UI with function monitoring runs any docker container as a function Client installation: 2The CLI installation is straight-forward for Linux and Mac users but it’s not immediately available for Windows. I cannot find an easy way to set the cluster address to point the CLI to. Server installation: 1Helm chart provided under faas-netes/helm but it’s failing the first time because of RBAC property not set and not rolling back, so I’m forced to delete and recreate the release. Even when installation is completed I cannot connect to the FaaS gateway as the service NodePort 31112, and the LoadBalancer creation errors out with Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply The Service \"gateway\" is invalid: spec.ports[0].nodePort: Invalid value: 31112: provided po","date":"2018-01-30","objectID":"/post/serverless-on-kubernetes/:0:3","tags":["serverless","kubernetes","platform","development"],"title":"Serverless on Kubernetes","uri":"/post/serverless-on-kubernetes/"},{"categories":["kubernetes","serverless"],"content":"Wrapping upThere are lots of people investing in serverless right now, almost as many as there are for Kubernetes; the integrations between the two will bring a new exciting technology scenario in the next years. To my experience building this post none of the previously listed framework is ready for production usage, to be honest most of them are not even ready for the average developer weekend project usage. You need to know Kubernetes quite a bit to troubleshoot issues happening during deployment and/or execution of your functions and this makes the whole serverless idea crumble, as you’ll be forced to dig into infrastructure details to have your code running. If I’d have to bet on one of the project I tried so far, I’d do so on Kubeless; it’s been definitely the smoothest setup among all and the tight Kubernetes integration makes it a perfect candidate for community-driven growth. If you know any other framework that should be in this post please let me know it in the comments! I am always curious to see what’s around in all things serverless so don’t keep it for yourself! ","date":"2018-01-30","objectID":"/post/serverless-on-kubernetes/:0:4","tags":["serverless","kubernetes","platform","development"],"title":"Serverless on Kubernetes","uri":"/post/serverless-on-kubernetes/"},{"categories":["tech"],"content":"Some interesting insights on concurrency primitives offered by Go","date":"2017-10-28","objectID":"/post/2017-10-28-golang-concurrency-pattern/","tags":["golang","development"],"title":"Golang Concurrency Patterns","uri":"/post/2017-10-28-golang-concurrency-pattern/"},{"categories":["tech"],"content":"In the early days of Go the language was often tailored towards “system programming” due to its C-stlye syntax and ability to write high-performance applications. Few time after, Go adoption was starting to gain traction for distributed systems development and projects like etcd, docker and kubernetes revealed the power of the networking capabilities offered by the internals in the language. Along the way a lot of libraries have been built around the powerful primitives offered by Go but in my opinion there is not enough use literature around the Communicating Sequential Processes implementation available through channels and goroutines, they are not even widely used in the standard library. I’ll detail here some concurrency patterns that I found useful and hopefully they’ll be idiomatic enough to represent a good use case for you. A premiseCSP it’s kind of a similar feature to threading but there are some differences; to know more on CSP I really recommend watching Rob Pike’s excellent talk on the topic. My experiencePersonally it took me a while to find my way out of the issues I ran into when first using concurrency features in Go: they are definitely the most complicated part of using Go, which is on average simpler that any other language I tried. So for me, the biggest problem was to understand what it means to have a goroutine spawned and how to control its execution or get data out of it, so I put together a list of examples on how concurrent programs flow can be controlled with the primitives built in the language. Channel, channels, channels everywhereA channel in Go is a way to pass messages between functions and goroutines, the official definition from A Tour of Go is: Channels are a typed conduit through which you can send and receive values with the channel operator, \u003c- So what are they good for? They are actually not very helpful without goroutines: a goroutine is a lightweigth thread managed by the Go runtime (definition), think like a background process that can be spawned and does not need to be managed directly by you, I like the concept of “run and forget”. The easiest concurrency pattern available is thinking of a goroutine processing some data in the background and returning them through a channel to the main thread executing our code; this can be very powerful and scale well to multiple functions and channels. WaitGroupsWaitGroups are part of the sync package from the Go standard lib: they are a way for waiting the execution of goroutines to end properly and ensure all the work done in the background is completed. WaitGroups are often used with defer to fill in the wait queue when the goroutine exits. Some examplesFor me the most difficult thing to understand when approaching concurrency was how to ensure all of my goroutines completed execution: to do this the easiest way is using WaitGroups as in waitgroup_test.go: wg.Add(1) adds one item in the wait queue and wg.Done() removes one item from it; using wg.Wait() in the main process makes the process wait until the wait group is emptied. If you run the tests with go test -v . -race -run ^TestWaitGroup you can see the execution time when using concurrency or not. Changing the value of ops variable in functions_test.go will make the tests process less or more items. With channels there are more features and gotchas that need to be taken into account: a read from a closed channel returns the type’s zero-value a send to a closed channel will panic a read and a send alone to an unbuffered channel are blocking: they will generate a deadlock if there is not a corresponding send/read operation on the other side of the channel a send on a buffered channel will block when the buffer is full and no other read is happening on the other side That being said, there are a couple of notable usages that I like to include in my concurrency-enabled Go software: the fan-out pattern where an input generates multiple goroutines that perform operations in the background and th","date":"2017-10-28","objectID":"/post/2017-10-28-golang-concurrency-pattern/:0:0","tags":["golang","development"],"title":"Golang Concurrency Patterns","uri":"/post/2017-10-28-golang-concurrency-pattern/"},{"categories":null,"content":"Work experiences7/2020 - now() » Senior Software Engineeroptimyze.cloudWe are a startup building Prodfiler: the lowest-overhead, always-on continuous-profiling tool for all your software stacks. Prodfiler gives a holistic view of CPU consumption across all your fleets, supporting multiple platforms and languages. I contribute to the product and leverage my previous working to bake reliability into our product and architectural decisions. 9/2018 - 6/2020 » Site Reliability Engineerlastminute.comJoined the SRE team within Platform to work more closely on the company cloud platforms both on-premise and in public clouds. Main focus is developing software in Go to automate the platform based on Linux, containers and Kubernetes. 9/2017 - 8/2018 » DevOps Engineerlastminute.comContributing mainly to the development of a CI/CD platform to enable developers to test, build and deploy software with ease and confidence; we are part of the Platform area and we collaborate with SREs and DBAs to provide developers automated services to consume infrastructure. The end-user platform runs on Kubernetes and we leverage Jenkins and Docker containers to shorten the development feedback loop and continuously improve product quality. Here I have the opportunity to sharpen my software engineering skills growing the use of TDD practices and adopting SOLID design principles, developing in Groovy, Java and Go. 11/2016 - 8/2017 » DevOps Tech LeadAccenture DVOWithin a team of 7, my job is to define a medium term strategy on monitoring tools and have it implemented. We aim to reduce the MTTR, improve the incident detection and the application performance analysis; we write plugins and APIs (mainly in Python) to make this tools interact seamlessly between each others and with other teams. 11/2015 - 10/2016 » DevOps EngineerAccenture DVOContributing to the operations for a Digital Video platform running on multiple cloud providers and serving 30 million end users worldwide. We aim at 0% downtime for our clients using high-availability techniques and continuous improvement practices. We develop our own monitoring, analytics and intelligence tool based on Splunk. We provide developers isolated test environments on AWS cloud and manage continuous delivery pipelines with automated testing and deployment. 6/2010 - 10/2015 » Systems EngineerMetelWorking on an e-procurement platform and a proprietary EDI format to help businesses digitalize supply chain management. My job is to manage and maintain the IT infrastructure serving the core-business services both on-premise and on the Amazon AWS cloud platform. Daily activities span from infrastructure event monitoring to middleware and application deployment. In 2012 I started developing backend software with J2EE and in 2014 I introduced Go in the company primarily for systems programming. 1/2014 - 9/2014 » Technical writermiamammausalinux.org8/2009 - 3/2014 » Audio EngineerfreelanceWorking with audio equipment to achieve best audio quality for live events; I was audio designer for some bands based in Milan, working with them in studio recordings and during live sets. Education and certifications10/2019 Certified Kubernetes AdministratorCloud Native Computing Foundation 2/2017 AWS SysOps Administrator - Associate LevelAmazon Web Services 8/2012 Oracle 11g Performance TuningOracle University Rome (trainer: Alessandro Colonna) 7/2009 Bachelor Degree in Industrial EngineeringPolitecnico di Milano 7/2004 High School DegreeScientific Lyceum “Falcone e Borsellino” - Arese (Milan, Italy) Love to work on Continuous Integration and Delivery practices infrastructure design and cost optimization cloud computing and distributed systems design and implementation monitoring and incident response Lang Go, Rust, Java, Python, Groovy, BASH OS RHEL/CentOS, Fedora, Debian/Ubuntu, OS X Cloud AWS, DigitalOcean, GCP, Kubernetes, Cloudflare DB MySQL, Oracle 11gR2, REDIS, Clickhouse Product HTTPd, NGINX, Varnish, Jenkins, GOCD, ELK, Zabbix, S","date":"2017-10-14","objectID":"/resume/:0:0","tags":[],"title":"❤️ your job or die trying","uri":"/resume/"},{"categories":["tech"],"content":"GCB is a build tool to automate the creation of containerized applications","date":"2017-10-01","objectID":"/post/2017-10-01-getting-started-with-google-cloud-builder/","tags":["gcp","cicd","development"],"title":"Getting Started With Google Cloud Builder","uri":"/post/2017-10-01-getting-started-with-google-cloud-builder/"},{"categories":["tech"],"content":"One of the advantages of containerized applications is the standardization, some would say “write it once, runs everywhere” but that’s another motto for another product. Anyway with a new packaging technology the same problems are faced: build reproducibility, or the necessity for people doing Ops to know they are going to deploy the same exact piece of code the Dev team used in their tests. So to address this issue the container image needs to be immutable: once it’s built, it’s not going to be changed, ever. And the same image will be used for testing, QA, beta, preview, presales-demo, whatever environment you need to deploy the app to. Building itDocker has been around for a few years now, it’s mature and stable, but ask anyone using it if they’d allow images built on development workstation to run in production: not gonna happen! The artifact that will serve production traffic will pass through the CI/CD pipelines and pushed to the registry, this is the way of shipping containers. “That’s easy”, you’ll say, “I’ll stick my Dockerfile in the repo and let the build system do the magic!”, but that’s not the whole picture: there are lower layers to pull, tests to be run and only after they succeed you can build the container. So who is going to maintain all of this configurations? Can we store them into the repo too? Yes! With GCB you can write a declarative multi-step workflow that will compile, test and package the code in a container; the container will get immediately pushed to the Google Container Registry too, so it’s ready to be consumed (maybe Kubernetes on GKE?). A simple applicationThere is quite a good number of examples in the cloud builder repository but I’d like to create a fresh one with Golang: a random number generator. The app will serve a random integer via HTTP, and you can see the code is very straightforward. Now I want to build a container to run into GCP with confidence so that every time I make a build I will have a new container image tagged with the version and ready to roll it out. Using GCBAll builds in GCB happen in a container, right now the only engine supported is Docker but more are going to be added. You can leverage Google pre-baked builders or use your own images as builders, in the example I am using Google’s golang-project image to compile and docker to build the final docker image and push it to registry. Note how some environment variables are injected to the container, like PROJECT_ID is your GCP running project as configured via gcloud auth login gcloud config set project your-gcp-project Side note for gophersThe golang-project image does some checks at setup to determine the workspace structure: there need to be a ./src folder or GOPATH must be passed, or the simplest way is to insert a comment next to package main in main.go Running the buildIt’s as easy as executing gcloud container builds submit --config cloudbuild.yaml . in the root of your project. See it in action! As you can see the current folder (. as last parameter) is compressed and shipped to a Cloud Storage random location, then GCB starts the steps listed in the configuration YAML file, running step by step the containers with their arguments. ConclusionGCB is fast and very easy to use but for what I’ve been able to test is bound to GCP right now, so if you are willing to deliver a service from Google Cloud and your application is containerized or “cloud native” you have a lightweight build system ready to go, but if you have a private registry or other integrations to do, GCB might still be a too small niche. I’ll make more tests and try to hack a bit GCB in the future so stay tuned! ","date":"2017-10-01","objectID":"/post/2017-10-01-getting-started-with-google-cloud-builder/:0:0","tags":["gcp","cicd","development"],"title":"Getting Started With Google Cloud Builder","uri":"/post/2017-10-01-getting-started-with-google-cloud-builder/"},{"categories":["social","work"],"content":"My 2 cents on what is happening with DevOps in the Enterprise","date":"2017-05-28","objectID":"/post/2017-05-28-devops-you-re-doing-it-wrong/","tags":["devops","culture","enterprise"],"title":"DevOps: you're doing it wrong","uri":"/post/2017-05-28-devops-you-re-doing-it-wrong/"},{"categories":["social","work"],"content":"Recently I received a mail pointing me to a post about DevOps culture and some anti-patterns and misconception on how to build and grow a DevOps culture in a company. Whoever like me works in the Enterprise (“the one with the big E” - Kelsey Hightower) knows that applying DevOps practices often is limited to the adoption of some tools or the creation of a “DevOps team” responsible of managing some continuous delivery pipeline. I would like to share my personal experience and possibly explain what to show to your boss when he thinks they are doing DevOps right. Luckily for me I had the chance to work 5 years in a small company before DevOps was a thing: we were a small team and were forced to handle development and operations together, scaling our services in feature and size was becoming impossible without the use of some practices that a few years later we discovered was called “DevOps”. When I changed job for an Enterprise company I was happy because I thought I would go and find a better implementation of the same practices, with people trained better than me to handle complex build and deployment systems; I actually found that a deployment system was not there and I was hired to help build one. My delusion was high because there was nothing for me to learn in there, I was stepping back into 3 years of work done automating JVM delivery with Jenkins. At the very same time the higher management wanted desperately to implement a DevOps organization because studies was talking about how it could increase team productivity and reduce the time to market of features. So this decisions were made: a dedicated DevOps team was created to take care of the automation of the build/deployment process (at that time 100% manual) a set of tools was chosen by the management and dictated to be used for monitoring, configuration management, etc… no change to the rest of organization was made: developers department and operations department kept the very same separate structure no change on how the software was developed: waterfall methodology was not removed Then once the automation of the release was almost complete and some of the hot-sounding technologies on the market (Ansible, Packer, Terraform) were used by some team members the company declared that “through DevOps” they could be the best in breed. The DevOps button was pressed! There is enough literature around (see bottom of page) to have a common understanding on what a DevOps culture is, how it originated and the practices that make it powerful for modern application delivery; I think most of Enterprise companies struggle impementing a DevOps model because the higher management do not understand what the DevOps transformation requires: a complete rethinking of people, processes and organization. The technological tools that end up implemented in successful organizations implementing DevOps are only a consequence of such change, not the fuel of the change itself. You cannot switch from SVN to Git, install Jenkins and Gradle and call yourself a DevOps company for doing that. There is no silver bullet, there is no one-size-fits-all solution; DevOps is about empowering people to understand continuous improvement and the value of collaboration. There is so much confusion in the higher management of the Enterprise companies that just changing your job title into “DevOps Engineer” will grant you a salary increase (happened to me). Take your stand for a truly cohesive organization that put “people over processes over tools” and find the courage to change, experiment and fail. This is what DevOps is about. Or you can keep doing it wrong. Suggested readsCloud System Adminstration Volume 2 Effective DevOps The Phoenix Project ","date":"2017-05-28","objectID":"/post/2017-05-28-devops-you-re-doing-it-wrong/:0:0","tags":["devops","culture","enterprise"],"title":"DevOps: you're doing it wrong","uri":"/post/2017-05-28-devops-you-re-doing-it-wrong/"},{"categories":["tech"],"content":"TLS management on AWS","date":"2016-12-03","objectID":"/post/2016-12-03-automate-tls-management-on-aws-with-letsencrypt/","tags":["aws","tls","letsencrypt"],"title":"Automate TLS management on AWS with LetsEncrypt","uri":"/post/2016-12-03-automate-tls-management-on-aws-with-letsencrypt/"},{"categories":["tech"],"content":"Letsencrypt is cool: automated, free TLS certificates for everybody! They are sponsored mainly by internet corps and they started a crowd-funding campaign to avoid the influence of this corps in the future of the project. I recently moved the blog to hugo on AWS and I’m now porting the TLS management scripts I wrote a while ago on AWS: this is a nice exercise to give a proper TLS automation valid for everyone on AWS. I used Terraform, the AWS CLI and the amazing (although still in beta) lego; the idea is to spin up an EC2 instance every month to query Letsencrypt to renew a certificate I already provisioned; the instance will have permissions to update the certificate using AWS Certificate Manager and other services like API Gateway custom domain names and Cloudfront are already bound to use ACM certificate of the domain. I tried and make the Terraform code as abstract and reusable as possible: cloudfront distribution id, domain name and issuer mail are all variable configurable via config file or at command line. Setting up the environmentI prepared the TLS certificate and key running first lego once on my local box, using the DNS challenge against AWS Route53, lego will use Letsencrypt ACME secret and put it into a TXT record for the domain to be validated so that Letsencrypt will know I am the owner of the domain. You can read more on Letsencrypt domain ownership validation here. After the lego account is created, generally in $HOME/.lego, you will find private key and certificate is created for the domain(s); the certificate is a bundle of all the SAN submitted in the request. Then how to automate all of this on AWS? Designing automationThe idea is that every service that needs TLS encryption will be able to fetch the certificate from an AWS service. This was not possible until AWS Certificate Manager was released a while ago: ACM will create or store a certificate to be used in other AWS services via the AWS API. No more magic to spread certificates via S3 or other tricks, you now have an awscli command! So here I chose to use Terraform to bootstrap all the infrastructure to run lego and run Letsencrypt automatically via AWS Autoscaling group. I first imported the certificate created with lego in ACM in us-east-1 region (N. Virginia): this is due to API Gateway limitations to be able to integrate natively with ACM only in us-east-1. I get the certificate ARN for the certificate just uploaded and I run terraform plan -var 'cf_distribution_id=EX4MPL3D15TR0' -var 'certificate_arn=...'. Once completed I can see a new Autoscaling group with scheduled policies, a launch configuration that starting from the base Amazon AMI with a user-data script at each boot will: download and install lego binary sync the TLS account with the S3 bucket created split the certificate from the chain, as they need to be uploaded separately call the acm subcommand of awscli to update the certificate This is amazing, I can now forget about TLS management for all of my websites! ","date":"2016-12-03","objectID":"/post/2016-12-03-automate-tls-management-on-aws-with-letsencrypt/:0:0","tags":["aws","tls","letsencrypt"],"title":"Automate TLS management on AWS with LetsEncrypt","uri":"/post/2016-12-03-automate-tls-management-on-aws-with-letsencrypt/"},{"categories":["social","tech"],"content":"Moving 4pres URL shortener to a more modern architecture","date":"2016-11-17","objectID":"/post/2016-11-17-4pres-goes-serverless/","tags":[],"title":"4pres goes #serverless","uri":"/post/2016-11-17-4pres-goes-serverless/"},{"categories":["social","tech"],"content":"Last month I felt I was a little late for the #serverless party going on all over the internet and I started taking a look at what the pros and cons would be to actually not manage any server myself. Shutting down my VPS hosting my apps I will lose my mail server, my MySQL instances and my Docker registry but: who cares? There are cloud services I can use with hundreds of times more availability and for a fraction of the cost. Why #serverless?We are moving more and more rapidly to a developer friendly world: all cloud providers tend to relief companies from the burden of managing complex cluster architectures and it’s not a coincidence that things like SRE at Google exist. Systadmins are no longer required where they can be substituted with far more performance by declarative syntax clusters (Kubernetes, Docker Data Center) and reliabile, consistent configuration deployment (Ansible, Puppet) in managing high volumes of phisycal or virtual machines. This shift to a developer-centric world forces who embraces it to “trust” the IaaS, PaaS and FaaS providers but in the same time let them focus on core and valuable development processes. That’s why. Take the simplest app in the world: 4pres, a URL shortener. It needs a presentation, computation and data layer as most of apps. Traditionally and depending on your budget and needs, you would spin up one or more VPS and deploy software on them (containers or not, doesn’t matter here). The setup of nginx as reverse proxy to your application already requires skills that most of developers don’t have, but in first hand why should anybody have them when there is a service like AWS API Gateway that lets you deploy one in seconds? Having the possiility to do so, you may want to forget about everything not strictly related to your app, so focusing only on building and maintaining functionalities of your app. How you can migrate your app todayIn terms of cloud provider I have a lot of experience with AWS therefore my first thought is for them when trying to do something like this: they probably already have enough mature services supporting what I need. Don’t take this as a sponsorship: you can do the same with any other provider. Traditional Architecture NGINX reverse proxy Golang application MySQL database NGINX terminates SSL and proxy back to app every request. The Golang app finds out what to do from a combination of HTTP Method and URL Path: GET / renders the landing page template GET /{url} queries the DB and redirect to long url or render 404 template POST / create a short link from form-data url=http://alongurul/ and display the result template Serverless Architecture (AWS) API Gateway expose a request/response mapping endpoint with integration to other services S3 to store and serve the static content Lambda executes Golang functions thanks to the amazing eawsy/aws-go-lambda framework DynamoDB stores data in schema-less fashion (JSON) The API Gateway definition acts as proxy and: GET / serves static page index.html from S3 bucket with ‘Website Hosting’ option enabled GET /{url} runs a Lamdba function 4pres_get that fetches the URL Path parameter from DynamoDB and redirect the client or renders a 404 template GET /s?{urlencodedURL} runs a Lambda function 4pres_post that creates a short URL and tries to store it in DynamoDB, returning the result template or the 500 template. Not that big change in the overall design, but the code for the Golang app only shares a function to shorten the URL between the 2 implementations: that is understandable because we no longer manage HTTP requests attributes and delegate that to API Gateway, we don’t display static content anymore and leave that to S3. At the core we only have 2 things to worry about: store a URL (4pres_post) get URL to redirect the client and then we can focus on extending new features: URL expiration user registration whatever! ConclusionThanks to this development model our craftsmen effort can be 100% dedicated to building featur","date":"2016-11-17","objectID":"/post/2016-11-17-4pres-goes-serverless/:0:0","tags":[],"title":"4pres goes #serverless","uri":"/post/2016-11-17-4pres-goes-serverless/"},{"categories":null,"content":"BioI’m a passionate technology enthusiast, an engineer in love with Computer Science and a part-time musician and writer; I started my career in IT as a sysadmin and became a Go developer in 2013 when approaching distributed systems. I like to study how things work, understand the mechanics of technology products. In my spare time I play drums, bass guitar and acoustic guitar, and sometimes try and write some music; writing small poems is also a passion of mine, as it helps me find a different meaning to things. BackgroundI attended Scientific High School, graduated in Industrial Engineering at Politecnico Milano and started working in Computer Science a few months later. I really feel thankful towards my college education because it introduced me to the real-world side of science; computing and mathematics above all, Open Source, curiosity and love for music are the foundation of my life. I also worked several years as self-taught audio engineer: I really enjoyed helping people find the best possible sound. Get in touchYou can reach me via any of the social media icons in the front page, share with me thoughts and interests. I am keen to discuss anything tech-related, new ideas on software development, cloud computing and CS in general. Please have a look at the blog: tech, music, politics and contemporary culture is what I write about. ","date":"2016-10-12","objectID":"/about/:0:0","tags":[],"title":"You are what you is (F. Zappa)","uri":"/about/"},{"categories":["tech","social"],"content":"In times of experimenting, I am now having a lot of fun with docker, rkt, kubernetes and containers ecosystem in general. But one thing I never forget to play with is content editing and publishing! So here I am, trying to migrate all my blog and website to Hugo :) So instead of a bare VPS I am moving my blog to AWS S3 + Cloudfront CDN. This will be more scalable and far less expensive. And Hugo generates static HTML so no more patching security issues. ","date":"2016-10-10","objectID":"/post/2016-10-10-moving-the-blog-to-hugo/:0:0","tags":null,"title":"Moving the blog to hugo","uri":"/post/2016-10-10-moving-the-blog-to-hugo/"},{"categories":["tech","social"],"content":"How I did itMigrate contentsFirst I found that there is a page on Hugo site that explains how to migrate from any CMS know to human to Hugo. I used the wordpress export plugin to convert the site from Wordpress to markdown and make it work ith Hugo: it did the the job pretty well but still I had to convert some of the posts. HTTPSThen the hard part: HTTPS! I chose to use letsencrypt to automate the TLS certificate handling. The powerful lego is way more multi-platform than the official certbot from EFF written in Python, and has built-in support for DNS challenge on Route53! Generating a new certificate of renewing is a couple of commands away. ","date":"2016-10-10","objectID":"/post/2016-10-10-moving-the-blog-to-hugo/:0:1","tags":null,"title":"Moving the blog to hugo","uri":"/post/2016-10-10-moving-the-blog-to-hugo/"},{"categories":["social","tech"],"content":"Amazon Web Services Elastic File System has been to my knowledge the service to have the longest beta testing period: reason for this may be that not as many client as expected tested it and AWS received too few feedback on it or that there were issues not to release GA. I don’t want to speculate on which one is correct but now that it has been officially released I decided to give it a try and of course compare it to a self-managed solution on the same platform. If you followed AWS evolution you may agree that EFS has been introduced to fill the gap between EBS storage and S3: before EFS was live there was no “easy” way of having a distributed file system in AWS, you could only set up your own using a combination of EC2 instances mounting Elastic Block Storage volumes and S3. Now with EFS you can have a AWS-managed distributed file system to be used in your cloud environment or even across the internet (will try that on a public subnet) with all the benefits of offloading the high-availability and replication burden to Amazon, and at a reasonable price. Will performance be enough compared to a self-managed solution? PlaygroundI use terraform to create an infrastructure template to run the tests, you can see it here. Once terraform apply has finished, you’ll end up with: An EFS with General Purpose performance mode An EFS mount target for 1 Availability Zone 1 EC2 instance named “client” to mount remote file systems 2 EC2 instances named “server_X” each one with a 10 GB General Purpose EBS, they will serve a self-managed distributed, replicated file system This is the terraform output and the steps to run on the 2 server nodes to have a running GlusterFS replicated volume; to configure the NFS export on server1, I used this guide. Apply complete! Resources: 15 added, 0 changed, 0 destroyed. The state of your infrastructure has been saved to the path below. This state is required to modify and destroy your infrastructure, so keep it safe. To inspect the complete state use the `terraform show` command. State path: terraform.tfstate Outputs: client-public-ip = ec2-54-89-124-238.compute-1.amazonaws.com efs-mount-target-a = us-east-1b.fs-a6418fef.efs.us-east-1.amazonaws.com server-a-ip = 172.30.10.122 server-b-ip = 172.30.11.31 ssh ec2-user@ec2-54-89-124-238.compute-1.amazonaws.com # ssh to first server, need agent forwarding setup ssh 172.30.10.122 sudo service glusterd restart # probe the node 2 sudo gluster peer probe 172.30.11.31 sudo gluster peer status ** Number of Peers: 1 Hostname: 172.30.11.31 Uuid: 237bc59a-20b6-4b30-b133-abab34e36720 State: Peer in Cluster (Connected) ** sudo gluster volume create efs-bench replica 2 transport tcp \\ 172.30.10.122:/export/gluster/brick 172.30.11.31:/export/gluster/brick force # force required to use the root disk as export brick ** volume create: efs-bench: success: please start the volume to access data sudo gluster volume start efs-bench ** volume start: efs-bench: success On the client I mount the EFS target with NFS4.1, the GlusterFS volume from the server in the same subnet via the GlusterFS native client_ _and the NFS export on the client’s designated mount points. I use the server on the same subnet as the client is, because the EFS target exposes a mount point in the same subnet and latency is a key factor in remote file system. sudo mount -t nfs4 -o vers=4.1 \\ us-east-1b.fs-a6418fef.efs.us-east-1.amazonaws.com:/ /mnt/efs sudo mount -t glusterfs 172.30.10.122:/efs-bench /mnt/gluster sudo mount -t nfs 172.30.10.122:/export/nfs /mnt/nfs -o user=ec2-user Benchmark TestsI used fio installed on the client box with a command suggested by this BinaryLane post and run it against the mount point for EFS, GlusterFS and NFS v4.0 with the following command fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test \\ --filename=test --bs=4k --iodepth=64 --size=1G --readwrite=randrw --rwmixread=75 changing the target directory each time I tun the test; each test is run isolated","date":"2016-09-18","objectID":"/post/2016-09-18-a-benchmark-of-aws-efs/:0:0","tags":["aws","benchmark","cloud","EFS","filesystem"],"title":"A benchmark of AWS EFS","uri":"/post/2016-09-18-a-benchmark-of-aws-efs/"},{"categories":["tech"],"content":"If you’re an AWS administrator you know that managing web console security is pretty tough unless you know what you want and you know what you’re doing. So if what you want is let each AWS user manage their own MFA device configuration without you and force them to have MFA active to use the web console, here is your solution. TL;DR Create one or more groups with your web users Create a new policy using this JSON Attach the policy to the group(s) How does it work? The policy has this logic: Allow basic operations on IAM without having MFA set up Allow setup and management of MFA for own user in IAM (create, delete, resync device) Deny every action on every resource when MFA is not setup Allow user to access IAM without MFA – this is necessary to sub-segment the previous rule The magic lies in the use of ARN policy variables which is a poorly documented feature of IAM. Notice how in some case the statement makes use of ${aws:username} to confine the action executed on the only user receiving the policy grants. This IAM policy blocks every serice usage when MFA is not setup, and in conjunction with default IAM behavior will deny access on every action if not explicitly given. You should combine this “base” policy with other group/service oriented policies to confine web users on certain functionalities. For example if you want a set of users self-managing their own MFA and access the EC2 service only after having setup MFA, you should execute the following after having setup the IAMUsersMFAManagement policy. aws iam create-group --group-name ec2webgroup aws iam create-user --user-name ec2webuser aws iam add-user-to-group --group-name ec2webgroup --user-name ec2webuser aws iam attach-group-policy --policy-arn arn:aws:iam::AWSACCOUNTID:policy/IAMUsersMFAManagement --group-name ec2webgroup aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name ec2webgroup ``Reference: AWS IAM variables documentation ","date":"2016-04-06","objectID":"/post/2016-04-06-aws-iam-policy-to-let-users-manage-their-own-mfa/:0:0","tags":["aws","cloud","iam","mfa","policy"],"title":"AWS IAM policy to let users manage their own MFA","uri":"/post/2016-04-06-aws-iam-policy-to-let-users-manage-their-own-mfa/"},{"categories":["social","tech"],"content":"As a coding challenge I was asked to provide a generic list implementation using a language of my choice and using only primitive types, avoiding the use of high level built-ins. I chose Go because I want to learn it and I know it can be useful to create an abstract, generic implementation. The challenge request to implement at least 4 methods on the generic type: Filter() – returns a subset of the List satisfying an operation Map() – returns the List objects’ map Reverse() – reverse the ordering of the List objects FoldLeft() – join the objects from left to right using a join character As a bonus question I was asked to code unit tests for the aforementioned methods and give an explanation on how the implementation guarantees concurrent access on resources. So here is my implementation: the type List has only one attribute, an array of type interface{} type List struct { data []interface{} } Every type will be convertible to the interface{} type, but as Golang has strong types the conversion is not implicit and must be declared. Reverse() will create a new array of interface{} to hold the reversed list func (m *List) Reverse() *List { var ret []interface{} for i := 1; i \u003c= len(m.data); i++ { ret = append(ret, m.data[len(m.data)-i]) } return \u0026List{ data: ret, } } Map() returns the List elements’ array, so it can be accessed as a whole func (m *List) Map() []interface{} { return m.data } This two function types will help define a custom operation to be used in Filter() and FoldLeft(): functions are types in Go and this enable a great level of abstraction. type filterFn func(interface{}) interface{} type foldFn func([]interface{}) interface{} Filter() will use a filter function, without the need to define it (!), and return a portion of the List data array. func (m *List) Filter(filter filterFn) *List { var ret []interface{} for d := range m.data { if data := filter(m.data[d]); data != nil { ret = append(ret, data) } } return \u0026List{ data: ret, } } FoldLeft() will use a fold function, again not yet defined, the return a single element made of the entire list. func (m *List) FoldLeft(fold foldFn) *List { var ret []interface{} ret = append(ret, fold(m.data)) return \u0026List{ data: ret, } } You can find all the code here, any comment is welcome on how to improve the abstraction or efficiency of the implementation. The opportunity to dig into the language ability to abstract is a very helpful way to better understand the language itself, so this coding challenge was a great opportunity to learn a little bit more Go! ","date":"2016-01-24","objectID":"/post/2016-01-24-implement-a-generic-data-list-structure/:0:0","tags":["aws","challenge","coding","design","golang","software"],"title":"Implement a generic data list structure","uri":"/post/2016-01-24-implement-a-generic-data-list-structure/"},{"categories":["culture","tech"],"content":"A friend asked me if I was able to get back working a Windows 98 PC he had in his house; I have never done it so I said “sure I can!” just to have the opportunity to learn something new, and of course do a friend a favour. My idea was to copy the whole PC and get it running on a virtual machine thus doing what I later discovered is called a “P2V” (Physical to Virtual); the result of which would have been a portable VM which I could then install in my friend laptop to have his old PC back. The idea was indeed good and I am writing this blog just after finishing the job in the hope I will save someone the headache I’ve had in the last 4 days to get this done. My tools for this job are : an ATA/SATA/IDE to USB adapter (this exactly) to mount the old drive winImage: a fantastic freeware by Gilles Vollant (runs on Windows only) Windows 98 ISO (get one here) VMWare Workstation: to run winImage on a pre-installed Windows VM, configure and test the new Windows 98 VM Why VMWare? It is by my knowledge the most reliable and portable hypervisor, with a vast documentation and huge community support; I also had it already installed in my Fedora box running a virtual Windows 10. Don’t worry if you don’t have VMWare: VirtualBox or Virtual-PC will do the same… So let’s get it started! Verify the drive is working connecting it to the USB adapter: connect the USB to your PC first, connect the power adapter to te disk then finally plug the power cable; if a green light is on and you hear the disk spinning, move ahead. If the disk has no vitals, consider calling a data recovery expert. If you’re on Windows the disk should be visibile in your drives: press the Windows logo and E key to open explorer and the drive should be visible and browseable. Otherwise check the Disk Utility to verify it has been recognized and why is not mounted. On Linux, use fdisk -l to know the device and mount -t filesystemtype /dev/deviceid /your/mount/point to mount on a folder specifying the filesystem type. To know the file system type use file -sL /dev/deviceid using the device identifier. Now that the disk is operational I recommend to take a backup with a raw image: this will help if any trouble happens with the disk during the P2V process. On windows you can use ShadowCopy or Ghost or Acronis Image, it all depends on your budget. On Linux I will use dd if=/dev/deviceid of=/my/backup/destination/disk.img Now I feel more confident because any action can be reverted with the original disk image backup: the next step is to virtualize the disk and all of its content with WinImage. From the menu select “Convert physical to virtual”, choose the input drive, a destination folder and the type of virtual disk (.vmdk if you want to use VMWare); WinImage will ask if you want to make a backup of your drive and if you are paranoic like me, answer yes and save another backup. Depending on the disk size the convertion process may take about 30 minutes long, so relax and wait; once WinImage is done you will notice it because the disk will lower down the noise. Open VMWare (or any other hypervisor you like) and create a new virtual machine: for Windows 98 I choose 1 vCPU with 512MB RAM; attach the virtual drive created with WinImage to the guest and start it up. If you’re lucky enough you should end up with a Windows boot screen and the operating system loading. Now the fun part! As we took a raw image of the old disk the drivers the hypervisor will use won’t be recognized! So in my case I had to click through a lot of driver reinstall, but they were all already present. A couple of reboots and the Windows 98 system is back up and running. Don’t be discouraged by what seems an infinite loop of driver reinstall! Continue installing the missing drivers and you willl succeed! This is definitely the most important thing to do: never give up!  ","date":"2016-01-24","objectID":"/post/2016-01-24-virtualize-an-old-windows-pc/:0:0","tags":["geek","guide","oldies","virtualization","windows"],"title":"Virtualize an old Windows PC","uri":"/post/2016-01-24-virtualize-an-old-windows-pc/"},{"categories":["social","tech"],"content":"[TL;DR] I wrote a Pub/Sub message queue in Go, branch “master” is stable but missing some interesting feature like distributed memory synchronization between nodes in a cluster and encryption. Code at https://github.com/inge4pres/gmq Being a cloud system engineer, my work is to design and implement distributed systems: one of the key principles in designing such architectures is decoupling, which means ensuring the many parts composing the system are able to share informations and complete a sequence of operations without being tied together. You can read more about cloud architectures and decoupling here. One of the most common scenario in a cloud application is a series of asynchronous operations executed by many nodes on different layers: for example a front end server tier receiving files and a backend server tier doing analysis on them; a good practice is to have a message queue between the two serving as an orchestration component. Each web server node will post a message in the queue for every files received, each backend node will consume a message from the queue to complete his operations on the files. In this way the two tiers are independent one from each other: in case of backend failure or over-capacity, the web servers will keep receiving files and storing message in the queue. If the two operations where done synchronously, the backend failure would stop the whole system to work. A lot of off-the-shelf message queue software is already available, but I felt like writing my own would give me a good point of view on system programming with Go, so I wrote it, and the result is pretty awesome too. In a few days I was able to have a configurable message queue storing messages in memory, on filesystem or database (MySQL); communication is based on JSON via TCP, and the server can be configured to support a maximum number of queues, a maximum message length and queue capacity: combination of the configured parameters will have performance effects on the single node. The roadmap of “develoment” branch is: adding cluster mode adding memory synchronization in cluster mode adding encryption: TLS over TCP adding client authentication As you may have guessed from the above list, security of GMQ is not implemented at the moment, be careful! Feel free to try it out and give suggestions! Cheers 😀 ","date":"2015-08-02","objectID":"/post/2015-08-02-golang-message-queue-a-simple-tcp-message-bus/:0:0","tags":["cloud","development","golang","software"],"title":"Golang Message Queue: a simple TCP message bus","uri":"/post/2015-08-02-golang-message-queue-a-simple-tcp-message-bus/"},{"categories":["social","tech"],"content":"It is true: I fell in love with Go, not because I love Google and his products, but because it really fits my ideology of simplicity and power in a programming language. I started experimenting with the language and thank to his web-oriented approach I quickly came up with one of the simplest single task web application I could write: a URL shortener. What is a URL shortener? It’s a service that will give you a short link for a long URL. Why should I use it? A short URL is easier to remember and to copy and paste, it lets you write more on social media where characters are limited (Twitter). Why writing one when there are plenty of them already? Other shortener have an expiration date on short links, 4pr.es doesn’t! Take a look at the code and you’ll see it is very simple: it takes a URL as text input filed of a form short, err := createUrl(req.FormValue(\"url\")) generates a random string of 6 charachters, checking that the string is not in the database ... for urlPresent(coder.Shrt) { coder.Shrt = shorten(coder.Length) } ... func shorten(c uint) string { rand.Seed(time.Now().UnixNano()) b := make([]rune, c) for i := range b { b[i] = letters[rand.Intn(len(letters))] } return string(b) } and keep the URL – string association in the database for further redirection; once the random short URL is visited the client gets redirected to the origianl long URL! ... err := getUrl(params[\"short\"], w, req) ... func getUrl(short string, w http.ResponseWriter, req *http.Request) error { var redir string err := db.QueryRow(\"SELECT url FROM urls WHERE short = ?\", short).Scan(\u0026redir) if err != nil { return err } http.Redirect(w, req, redir, 301) return nil } So please try it and if you have any suggestion to increase performance or security please let me know! Cheers   ","date":"2015-06-11","objectID":"/post/2015-06-11-my-first-golang-web-project/:0:0","tags":["GO","golang","shortener","software","URL"],"title":"My first Golang web project is online","uri":"/post/2015-06-11-my-first-golang-web-project/"},{"categories":["tech"],"content":"During the last years I’ve been experimenting with GlusterFS and his functionalities as distributed object store; a lot has changed in the software, overall since Red Hat acquired it. I have been using it and find it useful for many projects but not for others: what I love is the community oriented approach with a very responsive team and support for any kind of users (meaning from the 2 nodes web server to a RAID10 Infiniband cluster for high end storage). My personal story with Gluster starts with a porting of a on-premise architecture in the cloud: moving an existing application to the cloud, instead of redesigning it from scratch, involves a lot of engineering to adapt the current system settings to a scalable infrastructure. Gluster comes handy when talking about scaling: the latest milestone has a very simple and efficient way of reconfiguring the underlying hardware, adding and removing nodes in the storage pool is as simple as inputting a couple of commands from any of the peers in the cluster. If you’re unfamiliar with Gluster concepts (storage pool, peers, etc…) I suggest you RTFM on Gluster’s website; in this post I will detail a few points you won’t find on documentation and you should definetely know before starting to evaluate Gluster adoption. Gluster is not a replacement for disaster recovery and backups If you think that data redundancy mechanisms built in Gluster (replication and georeplication) are substitutes for backups you’re doing it wrong: in Gluster there is no way of recovering data present only in failed drives or unavailable portions of the pool. There is no SPOF free implementation that will avoid you regular backups, unless you can tollerate loss of data. Gluster has limited configuration options Gluster has been developed to “take common hardware and turn it into scalable high performance storage solution”. Gluster is great when availability and durability are performance indicators because it has been thinked for horizontal scalability, but scaling vertically in system resources will not have the desired outcome. There are phisycal thresholds in a node’s configuration that make huge hardware resources useless (eg, limit to the number of CPU threads for the transaltor). Do consider the application scenario If you are uncertain about Gluster capabilities, try it out yourself installing the software on at least two virtual machines and test if your application works well with the native FUSE module. As storage layer for I/O intensive applications Gluster is useful when average file size is bigger than the minimum size of the read cache (4MB). Currently the Gluster community is discussing how (relatively) small files should be handled in the next major release of milestone 3 (release 3.7) scheduled for the end of April 2015, but for now if your application scenario has lots of small files written frequently, Gluster may not be the right chioce. If you find Gluster is not suitable for your application, consider analizying a different solution like DRBD: it may not be as cutting edge as Gluster or Ceph but may be the right solution for the job. ","date":"2015-03-11","objectID":"/post/2015-03-11-glusterfs-is-it-suitable-for-me/:0:0","tags":["cloud","gluster","storage"],"title":"GlusterFS: is it suitable for me?","uri":"/post/2015-03-11-glusterfs-is-it-suitable-for-me/"},{"categories":["tech"],"content":"One of the very basic need of any startup is setting up a LAN in the workspace and configuring the Internet most used service: DNS. Relying on a public DNS may give you full functionality towards WAN connectivity, but when you need to address some hosts inside your LAN it can be handy to use names instead of IPs (especially with IPv6). Here’s a straight forward guide to get you started with your private DNS in a few minutes. OS filesystem’s path and package management utility may vary with the flavour of your distro, here I use CentOS. Requirements a router with DHCP ad WAN connectivity already setup 2 CentOS 6.x servers (physical or virtual with due availability concerns), enable to network with each others a desktop PC Some general info I use this data as example, change them to your needs startup.me the domain name you want to use in your LAN * `10.20.30.0/24` the subnet of your LAN * `10.20.30.40` the static IP address assigned to CentOS server 1, with hostname `centos1.startup.me` * `10.20.30.50` the static IP address assigned to CentOS server 2, with hostname `centos2.startup.me` **Configuring the primary DNS server ** On server centos1 [root@centos1 ~]# yum update \u0026\u0026 yum -y install bind bind-libs bind-utils The BIND daemon is now installed; the base dir for the service is /var/named and the configuration file is /etc/named.conf ; modify the configuration file with your favourite editor [root@centos1 ~]# vim /etc/named.conf In the options section adjust the settings to your LAN configurations, changing the example values options { listen-on port 53 { 10.20.30.40 }; # inet address of centos1 listen-on-v6 port 53 { ::1; }; # comment this out to use IPv4 only directory \"/var/named\"; recursion yes; allow-recursion { 10.20.30.0/24; }; # recursion only in LAN, change this with your subnet allow-transfer { localhost; 10.20.30.50; }; # enable zone transfers only to secondary DNS sever forwarders { 208.67.222.222; 208.67.220.220; }; # OpenDNS used here, Google 8.8.8.8, 8.8.4.4 can be used dump-file \"/var/named/data/cache_dump.db\"; statistics-file \"/var/named/data/named_stats.txt\"; memstatistics-file \"/var/named/data/named_mem_stats.txt\"; allow-query { 10.20.30.0/24; }; # accept queries only from LAN, change this with your subnet }; Now comment the include lines following the options section and comment out all the rest. The end of the file should be (modify the domain) # zone \".\" IN { # type hint; # file \"named.ca\"; #}; #include \"/etc/named.rfc1912.zones\"; #include \"/etc/named.root.key\"; zone \"startup.me\" IN { type master; file \"/var/named/startup.me.zone\"; allow-update { none; }; }; The latter lines create the definition of the domain and specify that DNS records should be looked up in the /var/named/startup.me.zone file which we are about to write. If you’re unfamiliar with DNS records and basic concepts, have a look at this. Now create and edit a new file [root@centos1 ~]# vim /var/named/startup.me.zone Insert this $TTL 8H @ IN SOA centos1.startup.me. root.startup.me. ( 1 ; serial 1D ; refresh 1H ; retry 1W ; expire 1H ) ; minimum TTL ; Name servers IN NS centos1.startup.me. IN NS centos2.startup.me. ; Resolvers @ IN A 10.50.30.10 ; default IP for your domain root startup.me centos1 IN A 10.20.30.40 ; the primary DNS server centos1 centos2 IN A 10.20.30.50 ; the secondary DNS server centos2 www IN A 10.20.30.20 ; a web server in your LAN ftp IN A 10.20.30.30 ; an FTP server in your LAN git IN A 10.20.30.5 ; a GIT server in your LAN Be careful when copying the above snippet into the config file: indentation must be respected! You have configured the primary DNS with some DNS A records; the provided settings are not for a heavy load DNS server, nor they should be used in networks wih frequent IP address change: the time-to-live settings are high, therefore any IP mapping change should be followed by a clients’ resolver cache flush, which may be inconvenient for most users. Now it’s time to enable the service; verify files permis","date":"2014-12-19","objectID":"/post/2014-12-19-set-up-a-private-masterslave-dns-using-bind/:0:0","tags":["BIND","CentOS","DNS","Linux"],"title":"Set up a private master/slave DNS using BIND","uri":"/post/2014-12-19-set-up-a-private-masterslave-dns-using-bind/"},{"categories":["tech"],"content":"In an attempt to make someone happier I wrote a script to notify a server admin when his child has gone through a year of uptime 🙂 Platform suggestions accepted, enjoy! happy_bday_server script ","date":"2014-12-11","objectID":"/post/2014-12-11-happy-birthday-server/:0:0","tags":["admin","bash","server"],"title":"Happy birthday server!","uri":"/post/2014-12-11-happy-birthday-server/"},{"categories":["tech"],"content":"The last weekend my colleagues and I had a nice time moving an existing application from a bare-metal infrastructure to AWS. I would like to share some of the focal points involved in such process, in case you’d go through it and would like to know: don’t expect everything to work as usual: you are changing the underlying hardware, moving to a virtualized environment. You can test every single part of the application but infrastructural side effects may occur in a second time relying on the provider: consider well which functionalities should be delegated to the cloud provider (AWS, in this case, offers a lot) or should be managed internally; for example S3 is not a distributed filesystem, and in some cases an RDS instance won’t have the same performance as database installed on an EC2 instance test application compliance, not hardware failure: instead of focusing on stress tests, you should focus first on functionality tests to ensure every part of te application is behaving as expected; hardware failure are easily handled in the cloud, that’s the primary purpose of IaaS. What is not handled by the cloud is that the application’s features will work on it! use a checklist: this may seem obvious, but having a clear and well written to-do list with a time table and activities’ details will help you analyze if anything is missing or needs to be done in advance Aside of this technical considerations, having the support of your coworkers and managers it is what really makes the difference: it keeps you focused in every step and at the same time helps if any problem comes up. That’s why my boss decided to take several videos with his phone and produced a “movie”, hope you like it 😀 ","date":"2014-10-15","objectID":"/post/2014-10-15-a-smooth-migration-to-the-cloud/:0:0","tags":null,"title":"A smooth migration to the cloud","uri":"/post/2014-10-15-a-smooth-migration-to-the-cloud/"},{"categories":["tech"],"content":"Here’s a thing I came up with: you’re administering a Linux system with 100 users circa and you’re moving to a new server, you can save crontabs per user with this: mkdir crontabz \u0026\u0026 cd crontabz; for user in `cat /etc/cron.allow`; do crontab -l -u $user \u003e cron_$user; done you will end up with a list of files cron_xxx, each one has the users’ cron. Hopefully your cron version will use the /etc/cron.allow file to control user based access to crontab. Now, once users in the new system are all in place you can copy the directory crontabz and then restore their cron with: cd crontabz; for file in `ls -m1`; do echo `basename $file`|sed -s 's/cron_//' \u003e\u003e temp ; done ; \\  for user in `cat temp`; do cat cron_$user | crontab -u $user -; done; Optionally you can include an “rm -f temp” at the end to delete the file used to store user names. I put the script on github here, Cheers ","date":"2013-11-11","objectID":"/post/2013-11-11-backup-and-restore-crontabs/:0:0","tags":["bash","cron","scripting","shell"],"title":"Backup and restore crontabs","uri":"/post/2013-11-11-backup-and-restore-crontabs/"},{"categories":["tech"],"content":"It may come in mind to any IT system engineer to know what is the status of the network, server by server, instance by instance; it happened to me when I was given the responsibility to manage my company’s infrastructure and I was wondering which tool could have helped to do the job. I chose Zabbix to monitor my infrastructure because: despite it’s a bit difficult to install (you need a PHP enabled web server, a database and a C compiler), you will benefit a very user-friendly web interface with lots of functionalities native agents for major OS release are already complied: FreeBSD, Linux, Windows, etc… Compiling to other OS just requires a “configure \u0026\u0026 make \u0026\u0026 make install” it offers many monitoring methods via a unique interface: you can group SNMP, JMX, HTTP monitoring in one shot it has multi-step HTTP/HTTPS monitoring, simulating different browsers and clients you can build nice infographics bundling all kind of monitored datas you can manage users and roles to give access to the web interface at your company’s employees you can build custom monitoring scripts to your needs Well let’s see some action now: I would like to post a short tutorial on how to build a custom script to monitor resources used by a Glassfish application server. You can use this methodology for other application servers or services. Requirements: you have installed Zabbix server, deployed an agent to a host, set up the necessary networking stuff On the host to be monitored, you will have a directory where the agent configuration file is located (usually /usr/local/etc or C:\\Zabbix) Step 1: enable custom parameters parsing Edit the file zabbix_agent.conf or zabbix_agentd.conf (depending if you’re usgin the daemon or not) and uncommment/add the following line: Include=/usr/local/etc/zabbix_agentd.conf.d/ or Include=/usr/local/etc/zabbix_agent.conf.d/ Step 2: write the script Create a file, name it as you please and insert the script you want to be executed by the agent: I needed a script that would inetract with Glassfish, so I used th following: # Flexible parameter to grab global variables. On the frontend side, use keys like glassfish.status[server.jvm.heapsize-current]. # Key syntax is glassfish.status[monitoring-key]. UserParameter=glassfish.status[*],/opt/glassfish/bin/asadmin get –user admin –passwordfile /opt/glassfish/bin/.pwd -m $1 | cut -d “=” -f 2 | tr -d ‘ ‘ | bc The script syntax is always UsrParameter=name.of.script[*] followed by the code to be executed. This one uses the Glassfish utility “asadmin” and a couple of shell commands to trim the string output and translate it into an integer value. You can see the arguments array can be retrieved using $ and index of argument. In this example you will call the script with one argument only (the monitoring data you want from Glassfish). Step 3: start harvesting datas! Once finished editing the script, go back to the Zabbix monitoring console and add an Item to the host you are monitoring. You will add the key as shown in the picture below:     Then go back to the dashboard and verify that the script just created is returning datas as expected. In the section Monitoring-\u003eLatest Data check if the item is giving the expected values. In this exemple I chose to monitor the current heap size used by the server. One cool thing is: once the script is done you can call it with all the parameters Glassfish has, and then combine datas in an infograph like the following:    Here I put together two Glassfish parameters (JVM upper bound and current heap used) and a system parameter (free memory). To get a list of all parameters you can monitor via Glassfish asadmin command see Glassfish documentation here. Cheers, inge4pres  ","date":"2013-10-07","objectID":"/post/2013-10-07-zabbix-a-powerful-yet-simple-monitoring-software/:0:0","tags":["cloud","infrastructure","monitoring","software","zabbix"],"title":"Zabbix: a powerful yet simple monitoring software","uri":"/post/2013-10-07-zabbix-a-powerful-yet-simple-monitoring-software/"},{"categories":["tech"],"content":"Ieri in ufficio mi è capitata una disgrazia, una rara occasione in cui non c’è capacità o competenza che possano aiutarti a risolvere il caso, serve solo fortuna: nel pomeriggio rientro dalla sigaretta e mi sento dire “non abbiamo più rete”. L’azienda dove lavoro si occupa di fornire servizi IT, senza la connessione internet siamo un’auto senza motore, una penna senza inchiostro, inutili. La linea di backup? All’inizio incolpo subito il nostro ISP (Fastweb), che a discapito di una fibra 100Mb ci ha giocato in passato scherzi del genere; quando la verifica evidenzia che non è un loro problema, il panico. Attiviamo la linea di backup: una WiFi 7Mb Telecom, penosa per servire il traffico che ci serve. Con il mio fedele collega Dario iniziamo a cercare la fonte del guasto. Andiamo alla sala macchine e tentiamo di capire dove sia il problema: un cavo collegato male? No. Saltata la corrente su uno degli interruttori di rete? No. L’impresa delle pulizie ha distrutto lo switch?!? No. Il router è fritto!! L’unico apparato di rete per cui non abbiamo sostituti, l’unica macchina fondamentale di cui non si può fare a meno è guasta: e non c’è porta secondaria, configurazione o altro che ci salvi! Vegno subito preso d’assalto dal DG, che tra l’altro mi ricorda che l’indomani è prevista una riunione in sede con dei clienti. Rabbrividisco al pensiero. Il tutto rimane in stand-by fino alla telefonata del mio capo, a casa malato, a cui devo comunicare la triste notizia. Lo chiamo e la cosa non lo rende affatto felice, e mentre sto per darmi per vinto lancio uno sguardo alla borsa sotto la mia scrivania: c’è un router lì dentro, il vecchio Dlink che usavo per simulare le reti dei clienti! Sarà anche una schifezza ma è sempre un router! Mi precipito al pc attacco il giocattolo e lo configuro pari al vecchio (con le sue minori funzioni) per servire la nostra LAN, vado in sala macchine, attacco i due cavi e…miracolo! Il problema è risolto! Ed è bastata un’occhiata al momento giusto dove non avresti guardato mai! Che dire…CULO! ","date":"2013-02-28","objectID":"/post/2013-02-28-quando-si-dice-problem-solving/:0:0","tags":null,"title":"Quando si dice problem solving","uri":"/post/2013-02-28-quando-si-dice-problem-solving/"},{"categories":["music","social"],"content":"Ieri mi sono trovato con parte della mia famiglia nella loro sala a suonare un po’ il pianoforte, mio fratello Gianmaria e io ci siamo esibiti riproducendo la semplice (ma efficace) musica che accompagna la pubblicità dell’iPad Mini. Il motivetto è in Do maggiore così come viene proposto suonando i due iPad nella pubblicità, di seguito il video. Enjoy!  ","date":"2012-11-26","objectID":"/post/2012-11-26-ipad-mini-la-musica-dello-spot-tv/:0:0","tags":["ipad mini music spot tv"],"title":"iPad Mini: la musica dello spot TV","uri":"/post/2012-11-26-ipad-mini-la-musica-dello-spot-tv/"},{"categories":["culture","social"],"content":"In Italia abbiamo molti problemi, ma l’ultimo dei nostri problemi era l’istruzione che vantava uno dei primi posti al mondo fino a pochi anni fa. Ora invece il declino strutturale del nostro Paese ha colpito anche questo settore che era di valore. Parlo di mancanza di fondi per università e ricerca, di formazione non orientata al lavoro e tutto ciò che ne consegue: l’aumento della disoccupazione giovanile e della sfiducia nel futuro è preoccupante perché è nella stesso ordine di grandezza di stati molto indietro rispetto alla media europea. Per fortuna l’Italia è nota per le sue particolarità e anche in questo caso voglio esaltare un esempio di eccellenza di cui sono venuto a conoscenza per puro caso: pochi giorni fa infatti ho preso i mezzi per andare a seguire un lavoro fuori ufficio (evento più che raro) e durante il viaggio di ritorno ho incontrato la sorella di un’amica d’infanzia. Maddalena è parecchio cresciuta dall’ultima volta che l’avevo vista, si è laureata in psicologia clinica da un anno ed è in procinto di finire il praticantato che le permetterà di fare l’esame di stato e raggiungere l’abilitazione alla professione. Senza stare a discutere di questo ulteriore scempio di tempo (12 mesi in cui, per prassi, non si percepisce stipendio) imposto dalla legge italiana, voglio raccontare di quale coincidenza abbia voluto che proprio il giorno precedente mia sorella mi avesse dichiarato “mi piacerebbe fare psicologia all’università”. Parlando mi ha spiegato che il corso universitario è diverso da come ce lo si aspetta: al primo anno ci sono molti esami scientifici, che sono un vero e proprio scoglio per le matricole, con conseguente scrematura degli iscritti; il “divertimento” per gli appassionati inizierà dal terzo anno e culminerà con la tesi. Maddalena è stata tanto gentile da darmi il permesso di mettere a disposizione di chi volesse informarsi sulla facoltà di psicologia una presentazione che lei stessa ha scritto per degli alunni del liceo: la trovate allegato in fondo al post. Il contenuto è sua proprietà intellettuale. Questi sono gli esempi da cui tutti dovrebbero prendere ispirazione: dedicare del tempo a divulgare informazioni su un corso universitario aiuta molto chi deve fare una scelta importante come quella dell’università. Grazie Maddalena Presentazione_psicologia ","date":"2012-10-05","objectID":"/post/2012-10-05-universita-facolta-di-psicologia/:0:0","tags":null,"title":"Università: facoltà di psicologia","uri":"/post/2012-10-05-universita-facolta-di-psicologia/"},{"categories":["culture","politics","social"],"content":"Stamattina ho lavorato all’Arena Civica di Milano come tecnico del suono. La manifestazione che ho seguito era la cerimonia di chiusura del Ramadan islamico, l’equivalente della Pasqua cristiana. La cerimonia di per sè è stata breve, circa un’ora, ed è stato affascinante assistere a una manifestazione religiosa a cui non avevo mai assistito prima. Il programma è stato il seguente: raduno dei fedeli nel campo dell’Arena presentazione dei celebranti del rito preghiera sermone sfollaggio Innanzitutto ciò che mi ha colpito è stata la mole di persone che erano presenti: circa 15000 tra uomini donne e bambini; da notare che la religione islamica non conta solo fedeli dal medio oriente, gran parte infatti erano di origine africana e indonesiana. La seconda cosa che mi ha pietrificato è stata che all’inizio dei canti, un inserviente della comunità musulmana mi è venuto a chiedere di avere dei microfoni per la platea. Normalmente alle funzioni religiose è chi celebra che diffonde l’audio verso la folla, mentre loro mi chiedevano addirittura di zittire durante i canti il loro “parroco” perchè non aveva una bella voce! Stupito, e un po’ impaurito di commettere un errore, ho comunque eseguito l’ordine e il risultato è stato apprezzatto perfino dal parroco stesso. Il momento più solenne e sicuramente il più spettacolare è stato quello della shallah: è una preghiera liturgica, che è intonata dal loro “vescovo” e a cui tutta la folla risponde in coro, rigorosamente in arabo. E qui ho avuto la terza sorpresa: qualunque musulmano prega in arabo, cioè non hanno una traduzione delle loro preghiere nelle altre lingue. Affascinante il fatto che nigeriani, bangra, e marocchini stessero intonando le stesse litanie in un’unica lingua: qualcosa di impensabile per altre religioni ampiamente diffuse nel mondo. I video della preghiera principale qui di seguito: Quindi al termine della cerimonia è iniziato il deflusso e io ho cominciato a preparare il mio lavoro per smontare l’impianto. Mi aspettavo di trovare l’Arena vuota in una manciata di minuti, invece un buon migliaio di persone è rimasto per aiutare gli organizzatori a pulire, raccogliere i gazebi, accompagnare i bambini smarriti verso i propri genitori, con uno spirito di fratellanza che raramente ho osservato in passato. Un paio di ragazzi si sono anche offerti per aiutarmi nel mio lavoro… Questa è stata la cosa più bella della giornata: la partecipazione dei fedeli oltre la cerimonia, oltre la preghiera cantata e nel concreto delle azioni per la comunità. A chi non avesse mai visto una cerimonia del genere, consiglio di assisterci. A chi non avesse mai pensato a partecipare attivamente e concretamente nella vita dei propri simili, consiglio di rifletterci. ","date":"2012-08-20","objectID":"/post/2012-08-20-ho-capito-il-significato-della-parola-islam/:0:0","tags":null,"title":"Il significato della parola Islam","uri":"/post/2012-08-20-ho-capito-il-significato-della-parola-islam/"},{"categories":["culture","music"],"content":"Una cosa che adoro di questo millennio è l’abbondanza di dati: nella storia l’uomo ha sempre cercato di tramandare ai posteri la propria vita e io credo che i dati raccolti e archiviati dall’umanità nel secolo scorso siano in valore di molte volte superiori a ogni calendario Maya o testo sacro. Trovo fantastica la sensazione che si prova guardando un pezzo comico di Valter Chiari di 50 anni fa e immedesimandomi nei miei genitori per esempio, che hanno assistito alle stesse scene. Certo gli usi e costumi sono cambiati, la comicità, la musica e l’arte in genere non possono avere le stesse rappresentazioni del passato perchè non rifletterebbero la realtà. Proprio pensando a questo mi sono trovato qualche giorno fà a riflettere su una canzone di quel periodo: “Il ballo del Mattone” di Rita Pavone (1963). Chiunque può essere certo di conoscere questo grande successo del passato, se gliene intoni il ritornello. Qui sotto la canzone, fondamentale ascoltarla prima di continuare a leggere l’articolo.  https:////www.youtube.com/embed/zPDTWuJqkAg Non c’è che dire, un capolavoro, il cui successo popolare è giustificato dallo stesso movente di tutte le canzoni moderne: l’amore. Questa è una canzone d’amore e lo si capisce prima dal testo e poi dalla musica. Dal testo ovviamente si capisce che seppur Rita si diverta a ballare con altri ragazzi, ci sia solo un uomo nel suo cuore, con cui lei balla il ballo del mattone. Inoltre la musica, che inizia come un classico twist, ha un cambiamento di ritmo e tonalità proprio quando Rita spiega come si balla il ballo del mattone: “…lentamente, guancia a guancia, io ti dico che ti amo, tu mi dici che son bella, dondolando, dondolando sulla setssa mattonella”. Questo pezzo è quello che mi ha fulminato dopo un’attenta riflessione che voglio condividere: musicalmente è facile notare come dal movimento e allegria del twist si passi a tonalità più romantiche e morbide, come quelle di un lento. Ma è nel testo che bisogna ricercare il vero significato: la mattonella, secondo me è un materasso, che del mattone può avere la forma. E cosa fanno i due innamorati su quel mattone, lentamente, guancia a guancia, mentre si dicono che si amano? L’amore. Una metafora fantastica, accompagnata musicalmente e trasmessa senza un filo di volgarità. Ecco cosa ascoltavano i nostri genitori quando avevano la nostra età, forse anche dieci anni di meno. I tempi sono cambiati, oggi una canzone del genere non avrebbe senso musicalmente e nemmeno verbalmente: non è più obbligatorio nascondere temi emotivi o sessuali dietro ad analogie così raffinate. Tutto molto più semplice e diretto, i messaggi verbali e musicali sono diventati usa e getta, come i prodotti che consumiamo. Oggi si può comunicare lo stesso episodio con quest’altra canzone. Godetevela.  ","date":"2012-07-30","objectID":"/post/2012-07-30-musica-e-linguaggio-uninvoluzione-al-passo-coi-tempi/:0:0","tags":null,"title":"Musica e linguaggio: un'involuzione al passo coi tempi","uri":"/post/2012-07-30-musica-e-linguaggio-uninvoluzione-al-passo-coi-tempi/"},{"categories":["music","social"],"content":"Ed eccolo!! Il mashup che stavate aspettando dopo la cena con più argomenti di economia della mia vita! Impressionante… http://inge.4pr.es/files/2014/07/tranx-mashup.ogg ","date":"2012-07-10","objectID":"/post/2012-07-10-discorsi-economici-finiti-in-un-mashup/:0:0","tags":null,"title":"Discorsi economici finiti in un mashup!","uri":"/post/2012-07-10-discorsi-economici-finiti-in-un-mashup/"},{"categories":["culture"],"content":"Mio amato ferro da stiro, è da 4 mesi che ci conosciamo e il nostro rapporto è intenso e caloroso tanto quanto la tua piastra in acciaio Inox. Sei un compagno di molte sere in cui, poichè non ho proprio niente da fare, ti accendo e passiamo insieme momenti fantastici mentre lisci le pieghe dei miei vestiti… Devo dire che era da molto che non avevo un rapporto così intenso e infatti, parafrasando la fantastica canzone di Marco Ferradini …chi è troppo amato amore non dà… io te lo devo proprio dire: quando le camicie hanno iniziato a invadere il nostro tempo insieme, è cominciata una caduta libera dell’affetto che provavo per te. Un’inesorabile odio verso te è venuto a galla, tu che sei strumento dal contenuto tecnologico elevatissimo, baluardo di una rivoluzione industriale da cui sono passati oltre cento anni. So che queste parole potranno ferire i tuoi circuiti ma anche io ho una dignità e non voglio più essere succube di questa relazione: certo avremo ancora momenti per incontrarci, tra una una maglietta e un asciugamano, e perchè no anche per delle lenzuola, ma non sarà mai come prima. Non voglio che tu soffra, non permetterei mai che tu arrugginissi all’ombra di un cassetto umido, ma non posso continuare così. Mi hai dato momenti di grande soddisfazione in passato, delle soddisfazioni che solo i fornelli possono dire di aver provocato (ebben sì ti ho tradito molte volte) ma è giunto il momento che io lasci spazio a chi ti sa capire, a chi ti può dare quello di cui hai bisogno: una mano ferma e una passata precisa, tanta acqua osmotizzata per la tua salute e un posto in primo piano come io non ho mai saputo darti… C’è chi ti usa per fare un mestiere, chi ha le capacità e l’ambizione di volerti utilizzare 6-8 ore al giorno e sentirsi appagato da ciò e io devo portare rispetto per questo. Devo riconoscere che esiste una tecnica, che in fondo stirare è come suonare uno strumento e io proprio non riesco a sentire l’energia necessaria ad esprimere le tue capacità. Con affetto, inge4pres goodbye ferro ","date":"2012-07-09","objectID":"/post/2012-07-09-ferro-da-stiro-che-passione/:0:0","tags":null,"title":"Ferro da stiro, mon amour\u0026#8230;","uri":"/post/2012-07-09-ferro-da-stiro-che-passione/"},{"categories":["culture","music"],"content":"Ho appena avuto la fortuna di fare un incontro, un incontro speciale di quelli che ti restano impressi per sempre. Non credo nel destino o fato o casualità che dir si voglia, seppure una serie di circostanze che hanno portato a questo incontro potrebbero farmi cambiare idea. La storia è questa: da pochi mesi ho deciso di inziare con mio fratello Gianmaria la ristrutturazione di uno studio musicale. Lo studio è stato costruito da mio padre 25 anni fa ed era in disuso. Iniziando a fare pulizia sono state ritrovate due casse (termine tecnico diffusori) impolverate, che si pensava fosse giusto buttare. Quando mio padre le ha viste ha detto che saremmo stati pazzi a volerle buttare in quanto sono un capolavoro di tecnologia degli anni ’70 e che la cosa più saggia sarebbe stata farle riparare per usarle nel futuro studio. Mio padre si prese anche la briga di suggerire il riparatore a cui avremmo dovuto portarle. Ripensandoci ora, sono davvero felice che mi abbia dato quel consiglio; il sig. Chiesa (Riky, come vuol essere chiamato) è davvero una persona che è in grado di farti respirare la sua passione per i diffusori ad ogni parola. Sono andato nel suo laboratorio dopo il lavoro convinto che avrei potuto passare il resto della giornata seguendo i miei programmi, ne sono uscito un’ora più tardi dimenticando tutto quello che avevo da fare: Riky è un appassionato e vive la sua passione ogni giorno nel suo laboratorio, facendo quello che ama fare. Appena entrato mi ha dato una stretta di mano e ha guardato subito i diffusori: ha iniziato a raccontarmi la loro storia nei dettagli che solo un appassionato può conoscere e ricordare, le loro caratteristiche tecniche, i punti di forza e le pecche. Sono rimasto ammaliato dalla sua voce e dalla sua padronanza dell’argomento e ho inziato a fargli domande tecniche e non. Non sono rimasto sorpreso quando, rispondendo ad ogni domanda con precisione, cercava con gesti ed esempi di farmi capire quanto più potesse riguardo l’argomento: chi vive le proprie passioni ha sempre voglia di condividere la sua conoscenza, perchè vuole trasmettere quel senso di attaccamento all’interlocutore che ha davanti. Questo episodio mi ha fatto pensare a come io vivo la mia passione per la musica, a come cerco di trasmetterla e condividerla con chi mi sta intorno. Mi ha fatto riflettere su questo blog e il senso che ha comunicare e vivere la propria passione; sicuramente mi ha cambiato la giornata in meglio!   Il suo sito: http://www.rikychiesa.com/ ","date":"2012-05-23","objectID":"/post/2012-05-23-vivere-le-proprie-passioni-con-passione/:0:0","tags":null,"title":"Vivere le proprie passioni, con passione!","uri":"/post/2012-05-23-vivere-le-proprie-passioni-con-passione/"},{"categories":["culture","politics"],"content":"Come dice Oscar Giannino, giornalista e conduttore radiofonico, nonchè grande conoscitore dei meccanismi dell’economia italiana, lo stato italiano invece di salvaguardare i propri cittadini, li deruba, li sfrutta, si prende gioco di loro; per utilizzare una frase del sempre impeccabile editorialista bisogna cercare di difendersi dallo “Stato LADRO!!!”. La situazione dell’economia italiana non è delle migliori, anzi, proprio oggi l’ISTAT ha pubblicato un report che indica come il reddito pro capite è uguale a quello del 1992 e fatti estremi stanno accadendo sempre più numerosi giorno dopo giorno. Fatta questa premessa, il mio interesse era portare alla luce un fatto che dimostra quanto lo Stato sia veramente indecente nei riguardi dei cittadini onesti, ed invece che stare dalla loro parte, fomenta insistentemente un clima di rancore e odio che poi porta in certi casi ad azioni che si vorrebbe evitare. Il sottoscritto nel 2009 riceve una multa di 86 Euro per ingresso in zona Ecopass non seguita all’acquisto del ticket da 5 euro che lo avrebbe salvato dal già esagerato importo della contravvenzione. Per ingenuità infatti, non avendo visto la telecamera, e avendo attraversato una via di 60 metri nei pressi di Piazza Conciliazione, non mi ero accorto di essere entrato nella “zona proibita” per 5 secondi di una giornata, per poi uscirne immediatamente quando la strada confluiva nella piazza di questa zone centrale di Milano. Bene, dopo aver pagato la multa (86 Euro) dopo 67 giorni (ebbene si, 7 giorni di ritardo dopo i 60 previsti, anche qui per ingenuità), sempre il sottoscritto si è visto recapitare in data odierna (MERCOLEDI 22 MAGGIO 2012!!!!! Ripeto… MERCOLEDI 22 MAGGIO 2012!!!), dopo quasi 3 anni, una cartella esattoriale da parte di Equitalia che indica come debba pagare 105 Euro (avete capito bene… CENTOCINQUE EURO!!!) entro 60 giorni, comprensivi di interessi di mora (importo destinato ad aumentare qualora non paghi entro i 2 mesi prestabiliti), per via del ritardo (di 7 giorni!!!!!) nel pagamento della suddetta multa!! Ora io dico…. se mi fossi fatto prestare i soldi l’ultimo giorno disponibile per pagare, dallo strozzino peggiore che possa esistere sulla faccia del pianeta, e glieli avessi restituiti dopo 7 giorni, probabilmente, sommando quanto devo pagare adesso, avrei potuto fare una bella cena di pesce in un ristorante nel centro di Milano e alla fine aver pagato gli stessi soldi di adesso… A voi le riflessioni. A me l’amaro in bocca… Andiamo avanti cosi… in questo paese sempre più di merda… ","date":"2012-05-22","objectID":"/post/2012-05-22-lennseima-dimostrazione-dello-stato-ladro/:0:0","tags":["ecopass","Equitalia","multa"],"title":"L'ennesima dimostrazione dello stato ladro!","uri":"/post/2012-05-22-lennseima-dimostrazione-dello-stato-ladro/"},{"categories":["culture","tech"],"content":"Ho sempre vissuto in città di provincia (MI) dove è abitudine frequentare una cerchia ristretta di persone. Questo mi ha permesso di stringere legami molto forti con chi considero i miei amici, ma allo stesso tempo non mi ha favorito nello sviluppo di una sensibilità verso la gestione delle informazioni personali nella vita reale; la mia lingua lunga ha solo fatto piovere sul bagnato… Il paese è piccolo e la gente mormora… recita il proverbio: ho constatato che è vero in molte occasioni. Non tutti i mali vengono per nuocere: nella mia vita digitale ho saputo imparare dagli errori commessi nela vita parallela e ho sempre dato un occhio di riguardo alla privacy online e tutto ciò che consegue allo scambio di informazioni tramite internet. Anche internet, infatti, con le sue milioni di miliardi di connessioni digitali può essere considerato un paesino di provincia, dove tutti possono venire a sapere tutto. Consideriamo un esempio: Alice e Roberto (sempre chiamati in causa in questo genere di esempi), hanno una relazione che vogliono mantenere segreta. I due non si incontrano mai in pubblico, ma volentieri si scambiano messaggi passionali attraverso il telefonino e il computer. Un giorno Alice sta chattando con Roberto via Facebook quando per sbaglio il mouse le cade e viene cliccato questo link. Alice capita sulla pagina che descrive le regole con cui Facebook registra e archivia le informazioni a lei correlate e incuriosita legge interamente il contenuto. Scopre essenzialmente che: Facebook registra qualsiasi attività dell’utente svolta all’interno del sito (visite di profili, like, post cancellati, chat) queste informazioni sono usate a fini statistici e sono condivise con altre aziende (pubblcitarie, di applicazioni, etc…) è possibile richiedere una copia dei propri dati riempiendo un modulo e ricevenrli su un CD/DVD L’indomani Roberto torna da un viaggio di lavoro e chiede ad Alice di incontrarsi; la avvisa inoltre che il suo telefonino gli è stato rubato in aereoporto. In quel momento Alice è fulminata da un pensiero: e se il ladro del cellulare fosse a conoscenza del modulo per ricevere i dati da Facebook e usasse il telefono di Roberto per farne richiesta? A quel punto avrebbe a disposizione tutte le conversazioni tra i due e potrebbe approfittarne per ricattarli! Non vorrei essere al posto di Roberto quando si incontreranno… Generalizzando dall’esempio, ogni compagnia che opera online ha il potere di acquisire dei dati: dalla sola mail e password in uso anni fa, al web 2.0 di oggi basato sui contenuti generati dagli utenti. Ogni compagnia si deve dotare quindi di politiche di detenzione e divulgazione di queste informazioni; vorrei che questo concetto fosse più ampiamente diffuso tra chi fa uso di questi servizi. Non è raro infatti considerare più sicuro e riservato scambiarsi informazioni via internet che a parole; se domando “Ci sentiamo per organizzare la festa a sorpesa di X?” mi sento rispondere “Creo un gruppo su Facebook” e rimango perplesso: come si può pensare che sia meno rintracciabile o più segreta una serie di caratteri leggibili e che viaggiano andata e ritorno per l’oceano Atlantico, rispetto alle parole sussurrate direttamente all’orecchio di chi deve sentirle? ","date":"2012-05-21","objectID":"/post/2012-05-21-privacy-e-riservatezza-il-digital-divide/:0:0","tags":null,"title":"Privacy e riservatezza: il digital divide","uri":"/post/2012-05-21-privacy-e-riservatezza-il-digital-divide/"},{"categories":["blog"],"content":"Il blog è tornato in vita dopo 24 ore dalla sua distruzione totale a causa di un mio errore di configurazione. Ho anche perso i primi tre post perchè le copie di backup non funzionavano… :'( Beh tutti sbagliano: alla PIXAR hanno distrutto il lavoro di 4 mesi su Toy Story 2 per un comando sbagliato lanciato sui server di storage…guarda il video! ","date":"2012-05-18","objectID":"/post/2012-05-18-blog-re-start/:0:0","tags":null,"title":"Blog RE-start","uri":"/post/2012-05-18-blog-re-start/"}]