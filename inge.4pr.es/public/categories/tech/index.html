<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    tech - inge4pres
  </title><meta name="generator" content="Hugo 0.98.0" /><link
    rel="stylesheet"
    href="https://inge.4pr.es/css/styles.css"
    integrity=""
  />
  
  <script>
    
    if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
      document.documentElement.classList.add("dark");
    } else {
      document.documentElement.classList.remove("dark");
    }
  </script>
  
</head>

  <body
    class="flex flex-col min-h-screen dark:bg-gray-900 dark:text-gray-100 transition-colors duration-500"
  ><header class="w-full px-4 pt-4 max-w-5xl mx-auto">
  <nav class="flex items-center justify-between flex-wrap">
    <div class="flex gap-2 items-center">
      
      <a href="mailto:fgualazzi@gmail.com" aria-label="EMail">
        <svg
          xmlns="http://www.w3.org/2000/svg"
          width="20"
          height="20"
          viewBox="0 0 24 24"
          stroke-width="1.5"
          stroke="currentColor"
          fill="none"
          stroke-linecap="round"
          stroke-linejoin="round"
        >
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <circle cx="12" cy="12" r="4" />
          <path d="M16 12v1.5a2.5 2.5 0 0 0 5 0v-1.5a9 9 0 1 0 -5.5 8.28" />
        </svg>
      </a>
      
      <a href="https://inge.4pr.es/" class="flex items-center font-bold">
        inge4pres
      </a>
    </div>

    <ul id="nav-menu" class="flex w-auto mt-0 space-x-2">
      
      <li>
        <a href="https://inge.4pr.es/about/" class="hover:text-blue-800 dark:hover:text-blue-300">You are what you is (F. Zappa)</a>
      </li>
      
      
      <li>
        <a href="https://inge.4pr.es/categories/blog/" class="hover:text-blue-800 dark:hover:text-blue-300">blog</a>
      </li>
      
    </ul>
  </nav>
</header>
<main class="flex-1 mx-4 md:mx-12 lg:mx-24 mt-8 sm:mt-16"> 
<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/grpc-traffic-mirroring-with-ingress-nginx-on-k8s/">gRPC Traffic Mirroring With Ingress-Nginx on K8s</a></h1>
  <p>In a <a href="https://inge.4pr.es/post/grpc-traffic-mirroring-using-nginx/">previous post</a> we saw an NGINX configuration to allow gRPC traffic mirroring.</p>
<p>Is the same technique applicable on Kubernetes?
Yes! Using the <a href="https://github.com/kubernetes/ingress-nginx">ingress-nginx</a> ingress controller!</p>
<h3 id="traffic-mirroring">Traffic mirroring</h3>
<p>Use the following configurations snippets in the <a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/">ingress-nginx configMap</a>
and in the Ingress manifest to mirror <strong>all traffic</strong> to a separate gRPC server.</p>
<h4 id="configmap">ConfigMap</h4>
<p>Replace <code>grpc-backend.company.net</code> and <code>grpc-mirror.company.net</code> with the original and mirror endpoint, respectively.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>    <span style="color:#ff79c6">http-snippet</span>: |<span style="color:#f1fa8c">
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      server {
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        listen 127.0.0.1:9443 ssl http2;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        server_name grpc-backend.company.net;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        grpc_ssl_protocols TLSv1.3;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        ssl_certificate_by_lua_block {
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">          certificate.call()
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        }
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        location / {
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">          grpc_pass grpcs://grpc-mirror.company.net:443;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        }
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      }</span>      
</span></span></code></pre></div><h4 id="ingress">Ingress</h4>
<p>Add the following annotations in the Ingress manifest defining the endpoint of your gRPC service.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>    <span style="color:#ff79c6">nginx.ingress.kubernetes.io/backend-protocol</span>: GRPC
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">nginx.ingress.kubernetes.io/configuration-snippet</span>: |<span style="color:#f1fa8c">
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      </span>      mirror /mirror;
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">nginx.ingress.kubernetes.io/server-snippet</span>: |<span style="color:#f1fa8c">
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      location = /mirror {
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        internal;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        proxy_set_header X-Mirrored-From $http_host;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        proxy_pass https://127.0.0.1:9443$request_uri;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      }</span>      
</span></span></code></pre></div><h4 id="nginx-configuration-details">NGINX configuration details</h4>
<p>Compared to the <a href="grpc-traffic-mirroring-using-nginx.md#the-solution">non-K8s version</a> we have some differences:</p>
<p>In the main <code>nginx.conf</code> file, applied through the configMap, we have a weird section.</p>
<pre><code>ssl_certificate_by_lua_block {
  certificate.call()
}
</code></pre>
<p>This is a something I discovered while looking in the ingress-nginx source: it&rsquo;s a helper used to load the right TLS certificate
which is impossible to do otherwise, because TLS certificates are stored in Kubernetes secrets, instead of normal files.
This replaces the TLS certificate loading directives.</p>
<p>The rest is unchanged.</p>
<p>The Ingress resource manifest contains the annotation to configure a gRPC backend <code>nginx.ingress.kubernetes.io/backend-protocol: GRPC</code>
and has 2 important settings.</p>
<p>The first snippet</p>
<pre><code>nginx.ingress.kubernetes.io/configuration-snippet: |
  mirror /mirror;
</code></pre>
<p>adds the mirroring directive to the virtual server location, to copy the gRPC traffic to the <code>/mirror</code> internal location.</p>
<p>The second snippet</p>
<pre><code>nginx.ingress.kubernetes.io/server-snippet: |
  location = /mirror {
    internal;
    proxy_set_header X-Mirrored-From $http_host;
    proxy_pass https://127.0.0.1:9443$request_uri;
 }
</code></pre>
<p>creates the internal location that will proxy the traffic to the additional server created in the configMap above.</p>
<p>That&rsquo;s it for copying <em>all</em> traffic from an ingress to a separate server!
But what if we&rsquo;d like to only mirror <em>a portion</em> of the traffic?</p>
<p>At the end of a previous post I left as a homework for the readers to discover how to copy only a percentage of traffic.
Read on to see how to achieve it.</p>
<h3 id="bonus-mirror-a-part-of-traffic">Bonus: mirror a part of traffic</h3>
<p>NGINX has a <a href="https://nginx.org/en/docs/http/ngx_http_split_clients_module.html"><code>split_clients</code> module</a> that is capable
of setting a variable based on the <em>distribution of an input</em>. The variable can be used in virtual servers to apply
conditional configurations.</p>
<p>The syntax to configure the module is</p>
<pre><code>split_clients &lt;input string&gt; &lt;variable&gt; {
  5% something;
  10% nothing;
  * &quot;&quot;;
}
</code></pre>
<p>with the value of <code>&lt;variable&gt;</code> being set based on the hash of <code>&lt;input string&gt;</code>: this can be anything that NGINX assigns
when processing a request.</p>
<p>The important detail to understand of the above configurations, is how to choose the input string: the percentage defines
the portion of hash values that will yield in <code>&lt;variable&gt;</code> the value to its right.</p>
<p>Let&rsquo;s have a look at the configs.</p>
<p>NGINX configMap:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>    <span style="color:#ff79c6">http-snippet</span>: |<span style="color:#f1fa8c">
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      split_clients &#34;${remote_addr}mirror${request_uri}&#34; $mirror_backend {
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        10% 1;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        * &#34;&#34;;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      }
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      server {
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        listen 127.0.0.1:9443 ssl http2;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        server_name original.domain.com;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        grpc_ssl_protocols TLSv1.3;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        ssl_certificate_by_lua_block {
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">          certificate.call()
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        }
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        location / {
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">          grpc_pass grpcs://destination.domain.com:443;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        }
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      }</span>      
</span></span></code></pre></div><p>Ingress manifest:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>    <span style="color:#ff79c6">nginx.ingress.kubernetes.io/backend-protocol</span>: GRPC
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">nginx.ingress.kubernetes.io/configuration-snippet</span>: |<span style="color:#f1fa8c">
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      </span>      mirror /mirror;
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">nginx.ingress.kubernetes.io/server-snippet</span>: |<span style="color:#f1fa8c">
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      location = /mirror {
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        internal;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        if ($mirror_backend = &#34;&#34;) { 
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">          return 200; 
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        }
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        proxy_set_header X-Mirrored-From $http_host;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">        proxy_pass https://127.0.0.1:9443$request_uri;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">      }</span>      
</span></span></code></pre></div><p>The additions to the previous config: with <code>split_clients</code> in the main nginx.conf file we set a variable <code>$mirror_backend</code>
with a non-empty string when the hash of <code>&quot;${remote_addr}mirror${request_uri}&quot;</code> falls in the <em>first</em> 10% of all possible
hash values.
The <code>if</code> added in the Ingress manifest will only proxy traffic when the <code>$mirror_backend</code> variable is not empty.</p>
<p>The hash value can be from 0 to 4294967295 (NGINX uses <a href="https://en.wikipedia.org/wiki/MurmurHash#MurmurHash2">MurMurHash2</a>,
returning a 32-bit integer); the percentages written in the configuration create segments of the whole hash space in a
contiguous manner, starting from 0.</p>
<p>In the example above where we defined <code>5%</code>, <code>10%</code> and <code>*</code>, you will have 3 ranges of possible values for <code>&lt;variable&gt;</code>:</p>
<ul>
<li><code>5%</code> -&gt; hash values from 0 to 214748364</li>
<li><code>10%</code> -&gt; hash values from 214748365 to 429496728</li>
<li><code>*</code> -&gt; all remaining hash values from 429496729 to 4294967295</li>
</ul>
<p>Therefore, the probability of getting each value is not <em>exactly</em> the same of the percentage configured,
because the distribution of hashes tightly depends on the input.</p>
<p>For example, if you have most of your traffic from a few <code>$remote_address</code>es, you don&rsquo;t want to set it as input alone.
The more you have a sparse distribution of values in the input variable, the better the filter will work.</p>
<p>This is why in the example configuration, I added <code>$request_uri</code> to the input, concatenating with a constant string
<code>mirror</code>: this highly increases the entropy of the hashes, making the percentage more reliable.</p>
<p>An important property of hashing the input is that it&rsquo;s deterministic, so if you want to mirror <em>exactly</em> for a subset
of requests, you can do it: define the percentages to include only the portion of hashes that you want to be copied.</p>
<p>For example, if you want to mirror only traffic for a certain URI, as in:</p>
<pre><code>$request_uri        = `/bank.Service/askForTransactions`
murmurhash2         = 1227040391
range percentage    = 28.57%
</code></pre>
<p>then the <code>split_clients</code> config would be</p>
<pre><code>split_clients $request_uri $mirror_backend {
  28.5699% &quot;&quot;;
  28.57% 1;
  * &quot;&quot;;
}
</code></pre>
<h3 id="drawbacks-and-limitations">Drawbacks and limitations</h3>
<p>Copying traffic using this technique is simple and effective, but it has a cost: we have a number of TCP connections
that are dedicated to serving the cloned traffic, even if going through the loopback interface.</p>
<p>You will notice from the ingress-nginx controller metrics that enabling the virtual server via the configMap does not
create more connections immediately, but as soon as you configure an Ingress to mirror using the new server, there will
be an increase in the average open connections.</p>
<p>This is expected, because of the non-native way we are doing mirroring for gRPC traffic.</p>
<p>The same applies to memory and CPU usage: handling more connections, and decrypting and re-encrypting every gRPC call
will come with a resource overhead.</p>
<p>One more thing to note: this technique <em>might</em> be working with gRPC streams, but I was only able to test it with unary
RPCs.</p>
<h3 id="credit">Credit</h3>
<p>Thanks again to Joni (<a href="https://twitter.com/mejofi">@mejofi</a>) for helping me find the original gRPC traffic mirroring configuration.</p>
<p>The partial mirroring addition is taken from this nice blog post by Alex Dzyoba
<a href="https://alex.dzyoba.com/blog/nginx-mirror/">https://alex.dzyoba.com/blog/nginx-mirror/</a></p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/grpc-traffic-mirroring-using-nginx/">gRPC Traffic Mirroring Using NGINX</a></h1>
  <p>Recently at work with the <a href="https://optimyze.cloud">Optimyze</a> team we faced the necessity of copying traffic from our current customer-facing environment to a new environment.
We have assumptions and ideas about architectural changes that cannot be validated only with synthetic tests and require cloning traffic to a separate, internal testing environment.</p>
<p>There is no better test than the one performed with real-world data: when you hear speaking about <em>testing in production</em>, a deployment of a new feature to &ldquo;see what happens&rdquo; is not what I have in mind.
I think of techniques that allow testing <em>with</em> production such as traffic mirroring (or <em>shadowing</em>), canary releases, A/B testing and feature flags.</p>
<p>These techniques will allow gathering data directly from your customers, in the form of raw input data (web/API requests) or feedback recorded from customers&rsquo; interaction with a new product version.</p>
<p>In the case of traffic mirroring we don&rsquo;t introduce any changes to a production system, we simply copy the traffic (entirely or a portion of it) into a different system, for internal analysis.
When applying this technique, security considerations apply: you do not want to leak customers data into a non-production system,
so ensure you apply the same security policies to the shadow infrastructure receiving a copy of production traffic.</p>
<h3 id="tldr">TL;DR</h3>
<p><strong>If you&rsquo;re in a rush and don&rsquo;t care about the details, the solution is <a href="#the-solution">here</a>.</strong></p>
<h3 id="the-problem">The problem</h3>
<p>We ♥ NGINX!
We knew it has the ability to serve as <a href="https://www.nginx.com/blog/nginx-1-13-10-grpc/">reverse proxy for gRPC services</a>,
and it has <a href="https://nginx.org/en/docs/http/ngx_http_mirror_module.html">built-in support for mirroring</a>.</p>
<p>What we didn&rsquo;t know is that mirroring gRPC traffic has a caveat: URI rewrite is not possible within the <code>grpc_pass</code> directive.
This means that traffic copied over to a new location (as described in the mirroring module docs) will have an URI set with the mirror location, making the service unable to map it with any RPC when received.</p>
<h3 id="debugging-nginx-capabilities">Debugging NGINX capabilities</h3>
<p>NGINX documentation is not very rich of examples or references to complex configurations, it is often left to the user
to build the desired outcome using the various building blocks offered by the modules.</p>
<p>In the case of traffic mirroring, the documentation states:</p>
<pre tabindex="0"><code>Sets the URI to which an original request will be mirrored. 
Several mirrors can be specified on the same configuration level.
</code></pre><p>In first instance we thought we could simply copy directly the traffic together with <code>grpc_pass</code> in a naive configuration like the following (<strong>not working as intended!</strong>):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-nginx" data-lang="nginx"><span style="display:flex;"><span><span style="color:#ff79c6">server</span> {
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">listen</span> <span style="color:#bd93f9">443</span> <span style="color:#f1fa8c">ssl</span> <span style="color:#f1fa8c">http2</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">server_name</span> <span style="color:#f1fa8c">grpc-backend.company.net</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">ssl_certificate</span> <span style="color:#f1fa8c">/etc/letsencrypt/live/company.net/fullchain.pem</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">ssl_certificate_key</span> <span style="color:#f1fa8c">/etc/letsencrypt/live/company.net/privkey.pem</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">root</span> <span style="color:#f1fa8c">/var/www/default</span>;
</span></span><span style="display:flex;"><span>	
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">location</span> <span style="color:#f1fa8c">/</span> {
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">mirror</span> <span style="color:#f1fa8c">/grpc-mirror</span>;
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">grpc_pass</span> <span style="color:#f1fa8c">grpc://production-upstream:12345</span>;
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">location</span> = <span style="color:#f1fa8c">/grpc-mirror</span> {
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">internal</span>;
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">grpc_pass</span> <span style="color:#f1fa8c">grpcs://mirror-upstream:443</span>;
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Note 2 things here:</p>
<ul>
<li>we are using NGINX to do TLS termination and we have a production service listening at <code>production-upstream:12345</code> for unencrypted traffic</li>
<li>we are re-encrypting traffic when copying over to the mirror because we want to emulate the full end-to-end traffic that a client would generate, and that includes TLS termination</li>
</ul>
<p>This may not apply to you but I prefer to sponsor a secure-by-default configuration, you can get free and automated TLS certificates with <a href="https://letsencrypt.org/">LetsEncrypt</a>.</p>
<p>We noticed the shadow environment was not processing data, so it was time to enable some debugging and see what was happening.</p>
<p>We add some logging to help us troubleshoot what is going on.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-nginx" data-lang="nginx"><span style="display:flex;"><span>	<span style="color:#ff79c6">location</span> <span style="color:#f1fa8c">/</span> {
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">mirror</span> <span style="color:#f1fa8c">/grpc-mirror</span>;
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">grpc_pass</span> <span style="color:#f1fa8c">grpc://production-upstream:12345</span>;
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">location</span> = <span style="color:#f1fa8c">/grpc-mirror</span> {
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">internal</span>;
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">grpc_pass</span> <span style="color:#f1fa8c">grpcs://mirror-upstream:443</span>;
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">error_log</span> <span style="color:#f1fa8c">debug</span>;
</span></span></code></pre></div><p>We can now find these messages in the logs:</p>
<pre tabindex="0"><code>2021/05/27 20:15:51 [debug] 15765#15765: *1 http upstream request: &#34;/grpc-mirror?&#34;
2021/05/27 20:15:51 [debug] 15765#15765: *1 grpc header: &#34;grpc-message: malformed method name: &#34;/grpc-mirror&#34;&#34;
</code></pre><p>As documented, the <code>mirror-upstream</code> server receives a request with URI <code>/grpc-mirror</code>, making it useless on the receiving end and creating a path mismatch.</p>
<p>At this point we thought native gRPC mirroring was not possible with NGINX and we tried to copy the traffic via the kernel network stack.</p>
<p>So we introduced a new server on a different port, willing to direct there the traffic <em>outside</em> of NGINX.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-nginx" data-lang="nginx"><span style="display:flex;"><span><span style="color:#ff79c6">server</span> {
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">listen</span> <span style="color:#bd93f9">9443</span> <span style="color:#f1fa8c">ssl</span> <span style="color:#f1fa8c">http2</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">ssl_certificate</span> <span style="color:#f1fa8c">/etc/letsencrypt/live/company.net/fullchain.pem</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">ssl_certificate_key</span> <span style="color:#f1fa8c">/etc/letsencrypt/live/company.net/privkey.pem</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">root</span> <span style="color:#f1fa8c">/var/www/default</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">server_name</span> <span style="color:#f1fa8c">grpc-backend.company.net</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">location</span> <span style="color:#f1fa8c">/</span> {
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">grpc_pass</span> <span style="color:#f1fa8c">grpcs://mirror-upstream:443</span>;
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">server</span> {
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">listen</span> <span style="color:#bd93f9">443</span> <span style="color:#f1fa8c">ssl</span> <span style="color:#f1fa8c">http2</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">ssl_certificate</span> <span style="color:#f1fa8c">/etc/letsencrypt/live/company.net/fullchain.pem</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">ssl_certificate_key</span> <span style="color:#f1fa8c">/etc/letsencrypt/live/company.net/privkey.pem</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">root</span> <span style="color:#f1fa8c">/var/www/default</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">server_name</span> <span style="color:#f1fa8c">grpc-backend.company.net</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">location</span> <span style="color:#f1fa8c">/</span> {
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">grpc_pass</span> <span style="color:#f1fa8c">grpc://production-upstream:12345</span>;
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>When we thought about cloning raw TCP traffic we also had to include TLS termination for the cloned traffic.</p>
<p>With this we are thinking we&rsquo;ll keep the same desired testing properties because the traffic will be cloned without NGINX knowing it,
so we tried with iptables rules using the <code>TEE</code> target only to realize minutes later that cloning TCP packets simply does not work because the protocol
is stateful and there is no way for the kernel to handle duplicated responses from a second client that has never been tracked.</p>
<p>Using a separate server is key to the correct solution presented below (thanks <a href="https://twitter.com/mejofi">@mejofi</a> for the brilliant idea).</p>
<h3 id="the-solution">The solution</h3>
<p>The trick is to combine the gRPC module with the proxy module to achieve the URI rewrite that is needed.</p>
<p>Here is a working gRPC traffic mirroring configuration:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-nginx" data-lang="nginx"><span style="display:flex;"><span><span style="color:#ff79c6">server</span> {
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">listen</span> 127.0.0.1:<span style="color:#bd93f9">9443</span> <span style="color:#f1fa8c">ssl</span> <span style="color:#f1fa8c">http2</span>;
</span></span><span style="display:flex;"><span>	
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">ssl_certificate</span> <span style="color:#f1fa8c">/etc/letsencrypt/live/company.net/fullchain.pem</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">ssl_certificate_key</span> <span style="color:#f1fa8c">/etc/letsencrypt/live/company.net/privkey.pem</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">root</span> <span style="color:#f1fa8c">/var/www/default</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">server_name</span> <span style="color:#f1fa8c">grpc-backend.company.net</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">location</span> <span style="color:#f1fa8c">/</span> {
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">grpc_pass</span> <span style="color:#f1fa8c">grpcs://mirror-upstream:443</span>;
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">server</span> {
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">listen</span> <span style="color:#bd93f9">443</span> <span style="color:#f1fa8c">ssl</span> <span style="color:#f1fa8c">http2</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">ssl_certificate</span> <span style="color:#f1fa8c">/etc/letsencrypt/live/company.net/fullchain.pem</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">ssl_certificate_key</span> <span style="color:#f1fa8c">/etc/letsencrypt/live/company.net/privkey.pem</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">root</span> <span style="color:#f1fa8c">/var/www/default</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">server_name</span> <span style="color:#f1fa8c">grpc-backend.company.net</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">location</span> <span style="color:#f1fa8c">/</span> {
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">mirror</span> <span style="color:#f1fa8c">/grpc-mirror</span>;
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">grpc_pass</span> <span style="color:#f1fa8c">grpc://production-upstream:12345</span>;
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>	
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">location</span> = <span style="color:#f1fa8c">/grpc-mirror</span> {
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">internal</span>;
</span></span><span style="display:flex;"><span>		<span style="color:#ff79c6">proxy_pass</span> <span style="color:#f1fa8c">https://127.0.0.1:9443</span><span style="color:#8be9fd;font-style:italic">$request_uri</span>;
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><strong>In the snippets, the definition of upstream servers has been omitted.</strong> The configuration is not valid without them!</p>
<p>What we have here:</p>
<ul>
<li>the second server now listens on the loopback interface to avoid traffic leaving the NIC</li>
<li>the un-encrypted traffic is mirrored to an <code>internal</code> location</li>
<li>a <code>proxy_pass</code> directive that will set the original URI in the next upstream, proxying the traffic the loopback server</li>
<li>the <code>grpc_pass</code> in the loopback server will re-encrypt and pass the request to the shadow environment, but with the right gRPC path URI</li>
</ul>
<p><strong>Pay attention</strong>: the previous configuration will copy <em>all traffic</em> from a production gRPC service into a shadow environment.</p>
<p>It is left as an exercise to the reader to find the proper way of shadowing only a portion of the traffic (hint: check the
<a href="https://nginx.org/en/docs/http/ngx_http_split_clients_module.html">split_clients_module</a>).</p>
<p>A further improvement could be to use a UNIX socket for the server that will proxy requests to <code>mirror-upstream</code>,
on some OSes it might be faster than TCP proxying. I didn&rsquo;t test it yet but it looks like a possible combination offered
by <code>listen</code> and <code>proxy_pass</code> (the URI rewrite <em>should</em> be possible).</p>
<h3 id="conclusions">Conclusions</h3>
<p>Traffic mirroring is a key enabler for so many scenarios that it should be a first class citizen in any major webserver, for every protocol.</p>
<p>NGINX configuration can be treated as an art, given the infinite possibilities the server offers with its modularity.</p>
<p>I hope this post will save you some hours of debugging and will increase your testing capabilities,
I decided to write it because I could not find a complete answer online do the question: &ldquo;can NGXIN mirror gRPC traffic?&rdquo;.
And the answer is &ldquo;yes&rdquo;!</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/micro-committing-with-git/">Micro-committing with Git</a></h1>
  <p>I have been using micro-committing for some time now, during which I have adapted the usage of this technique to my needs,
bringing it to a level that makes me more productive than ever in software development.</p>
<p>Combining micro-committing with Git, while doing <a href="https://it.wikipedia.org/wiki/Test_driven_development">TDD</a>
is now my favorite development experience: I like how this workflow helps to deliver changes with speed and confidence.</p>
<p>This is not a one-size-fits-all approach, I&rsquo;m sharing what works great <em>for me</em>; I hope some parts of what follows
will help you and your team as well.</p>
<h3 id="what-is-micro-committing">What is micro-committing?</h3>
<p>Despite micro-commits are not new, there is not much literature or a formal definition of it.
The concept is very simple: when developing or refactoring, use the SCM <em>at every notable step</em> creating a small commit.</p>
<p>Sizing properly micro-commits is what takes more practice mastering: I think it&rsquo;s a very personal thing, one might be comfortable
with lots of very tiny commits, some other with slightly larger bundles of changes, the important factor is usability.</p>
<p>We want to store in a commit every relevant code that produces an increment towards a goal.</p>
<h5 id="why-should-you-care">Why should you care?</h5>
<p>I found that using micro-commits makes working iteratively safer: the confidence I gain in adding or changing code once
I am backed by a <em>&ldquo;diffable history&rdquo;</em>, is priceless.</p>
<p>Keeping track of changes at a smaller rate enables to move back and forth into history to do local optimizations.
Having these changes recorded with Git might not be useful <em>all the time</em>, but when they&rsquo;re needed they are going
to be very valuable.</p>
<h3 id="how-to-make-the-best-out-of-it">How to make the best out of it</h3>
<p>In theory, the smaller the change bundle recorded in every commit, the better!
While keeping a long list of tiny changes locally can have several benefits, it can also turn your review process and CI
pipelines into chaos.</p>
<p>That&rsquo;s why we need to be careful in managing micro-commits not only during coding but also when preparing changes to
be submitted for peer review.</p>
<h4 id="during-development">During development</h4>
<p>During the development phase we want to store locally as many micro-commits as we need to ensure that we can easily
move our code back to a working revision. When the development is completed we don&rsquo;t want to send the micro-commits
to the remote repository, because reviewing that history would be difficult.</p>
<p>With these concepts in mind, below the things to remember when mirco-committing during the development of new features.</p>
<ul>
<li><strong>never</strong> push to the remote repository until the commit history has been reviewed</li>
</ul>
<p>One thing that I learnt recently thanks to my <a href="#credits">amazing colleagues</a> is that Git history is precious.
We don&rsquo;t want to pollute the tree with too many commits targeting the same feature.
Micro-commits are an aid for <em>local</em> development: they should be squashed, rewritten and edited before sending them
into a remote repository (more on this below).</p>
<ul>
<li>add a micro-commit for every development iteration unit: every test assertion added, every successful implementation and
refactoring</li>
</ul>
<p>We want that every change with a significant impact on the codebase can be consistently fetched.
Any IDE Undo/Redo feature does kind of the same, but a micro-commit plays better with refactoring, for example when
changing a signature or renaming a package.</p>
<p>We don&rsquo;t want to store in a micro-commit changes that do not have a &ldquo;business&rdquo; meaning, like renaming a variable.
At the same time we don&rsquo;t want to store in a micro-commit several changes that map to multiple features: doing so would
prevent us to go back in time with fine-grained control, and thus lose the benefits or incremental history.</p>
<ul>
<li>write a short but meaningful commit message: we must be able to understand what we did</li>
</ul>
<p>Optionally, include in the commit message the name of the component or sub-system under change.
When re-reading the history of commits, it will be helpful to know which commits are part of which
bundle of changes.</p>
<ul>
<li>create a tag when a cycle of development or refactoring is complete, but delete it before pushing
(in summary, never push tags used for referencing micro-commits)</li>
</ul>
<p>This is not strictly required: I like to create a simple, non-annotated tag, with the name of the task when I declare
it completed. It helps me to point a commit to a task completion in the Git log.
Adding a tag can be replaced by details in the commit message: we need to be able to understand which commit resolved
a given task.</p>
<ul>
<li>when you are stuck, throw everything away and go back to last commit!</li>
</ul>
<p>This is where this technique really shines: firing up a single command</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git reset --hard
</span></span></code></pre></div><p>will allow to re-start from what <em>you</em> chose minutes ago as a standing point, a safe place from which a new iteration can start.
If you create tags as in the previous bullet, you can even jump multiple micro-commits back, correcting a wider design
mistake or a faulty implementation with a newer idea, just specify the tag name you want to <code>reset</code> to.</p>
<h4 id="preparing-for-peer-review">Preparing for peer review</h4>
<p>It is important to review the Git history before submitting it to the remote repository: we should group multiple
micro-commits belonging to the same task into a single commit.</p>
<p>The reasons for doing so:</p>
<ol>
<li>peer-review done commit by commit helps reviewers understanding more than the code logic, enriching code with
the <em>rationale</em> and the decisions made by the author</li>
<li>reverting a single feature should be as easy as reverting a single commit (or a few of them): reducing the
cognitive load embedded in the history helps when things go wrong, even if all tests had passed 😁</li>
</ol>
<p>A good way to achieve a clean history, ready for being reviewed, is with an <a href="https://git-scm.com/docs/git-rebase"><strong>interactive rebase</strong></a>
to the target branch.
This is the most straightforward way to create a meaningful list of commits that can tell a story about the development
being shipped.</p>
<p>When rebasing interactively, you are presented the list of commits you are going to add on top of the target branch in an
editor. From there it&rsquo;s possible to merge (<code>squash</code>, <code>fixup</code>), remove (<code>drop</code>) and even change the order of commits!</p>
<p>I use these 3 steps to craft the intended Git history:</p>
<ol>
<li>remove tags used as references to micro-commits</li>
</ol>
<p>In most CI systems tied to the Git tree, tags can trigger special workflows like creating artifacts for releases:
we don&rsquo;t want this.</p>
<p>If tags have been created to keep track of multiple features, I&rsquo;ll remove them to avoid accidentally pushing them
and triggering such workflows.</p>
<ol>
<li><code>squash</code> or <code>fixup</code> corrections and clean-up commits to the uppermost relevant commit</li>
</ol>
<p>It is good to remove commits that were useful during local development, but do not have a tangible impact for the
reviewers: i.e. linting corrections, typos in documentation, etc&hellip; These commits should always be conflated with a previous one.</p>
<p>It&rsquo;s also possible to move commits up and down during an interactive rebase, but note that it can lead to difficulties in
setting up the desired history: you might end up in conflicts that will counter-effect the benefits of micro-committing,
with time lost on editing during the rebase.</p>
<ol>
<li>set the commit that is the union of multiple fixup/squash commits with a <code>reword</code></li>
</ol>
<p>In this way, Git will open the editor to revrite the commit message.
A commit that will be in the history of a repository needs to be explicit about what&rsquo;s being committed.
It has to contain a good high-level description of what it holds in all its parts.</p>
<p>Remember the famous jewelery slogan <em>&ldquo;a diamond is forever&rdquo;</em>? A valid statement is also <strong>&ldquo;a commit is forever&rdquo;</strong>
(at least it should be!) 😉.</p>
<h4 id="a-practical-example">A practical example</h4>
<p>I&rsquo;ll use the history of the repository in which I&rsquo;m writing this page 😎.
If I run <code>git log</code> now I have</p>
<pre tabindex="0"><code class="language-git" data-lang="git">commit 91dc03e493fc9ff9426c7250de9d6dd168d58b2b (HEAD -&gt; master, tag: ready-for-conclusion)
Author: inge4pres &lt;xxx@gmail.com&gt;
Date:   Sat May 1 18:18:10 2021 +0200

    refinement before example

commit aaafc4030dae80baa1a6c15905111a69484b9e6d
Author: inge4pres &lt;xxx@gmail.com&gt;
Date:   Tue Apr 6 22:40:58 2021 +0200

    refined central section

commit 1ef9ec1e6b70a61dd1772df3db69cb219dc4609e
Author: inge4pres &lt;xxx@gmail.com&gt;
Date:   Sat Apr 3 19:28:17 2021 +0200

    Resume: update page

commit 869e15fd7c85dcaef1e3e1865f2dcb62a5c06966
Author: inge4pres &lt;xxx@gmail.com&gt;
Date:   Sat Apr 3 17:23:37 2021 +0200

    fixed typos and intro section

commit aa25b348a6a0aa61cc6482beacbe4b96ec96e79f
Author: inge4pres &lt;xxx@gmail.com&gt;
Date:   Sat Apr 3 17:08:21 2021 +0200

    second section: peer review

commit f368028b3dcb38aed86893139ee9230d32fc7f2a
Author: inge4pres &lt;xxx@gmail.com&gt;
Date:   Fri Apr 2 21:25:16 2021 +0200

    first draft: completed intro
</code></pre><p>where we can note the following:</p>
<ul>
<li>there&rsquo;s a tag to be removed (<code>ready-for-conclusion</code>)</li>
<li>commit <code>1ef9ec1e6b70a61dd1772df3db69cb219dc4609e</code> is unrelated to the post I&rsquo;m writing so I&rsquo;ll move it</li>
<li>commits <code>869e15fd7c85dcaef1e3e1865f2dcb62a5c06966</code> and <code>91dc03e493fc9ff9426c7250de9d6dd168d58b2b</code> can be squashed</li>
<li>the article can probably be split in 2: the initial draft (the 3 commits at the bottom, the first in chronological order)
and the final part (the first 2 commits in the list, the last in chronological order)</li>
<li>it took me 1 month to find the time to finish a blog post I had previously started! 😂</li>
</ul>
<p>Removing the tag: <code>git tag -d ready-for-conclusion</code>.</p>
<p>It&rsquo;s time to run a <code>git rebase -i origin/master</code> to rebase interactively my local branch on top of the remote branch.
The output in the editor is the following:</p>
<pre tabindex="0"><code class="language-git" data-lang="git">pick f368028 first draft: completed intro
pick aa25b34 second section: peer review
pick 869e15f fixed typos and intro section
pick 1ef9ec1 Resume: update page
pick aaafc40 refined central section
pick 91dc03e refinement before example

# Rebase 01ff245..91dc03e onto 01ff245 (7 commands)
</code></pre><p>With the considerations done earlier, I will change the history like so:</p>
<pre tabindex="0"><code class="language-git" data-lang="git">pick 1ef9ec1 Resume: update page
pick f368028 first draft: completed intro
squash aa25b34 second section: peer review
fixup 869e15f fixed typos and intro section
reword aaafc40 refined central section
fixup 91dc03e refinement before example
</code></pre><p>The first editor session will be to opened to combine the commits <code>f368028</code>, <code>aa25b34</code> and <code>869e15f</code>: I will merge the commit messages,
and I will add the slug of the post.</p>
<p>The second editor session will be to rewrite the last 2 commits combined, adding details about the post content.</p>
<p>Now the history looks much better, and it can be pushed:</p>
<pre tabindex="0"><code class="language-git" data-lang="git">commit be50a37f5dc63b587a1d105869449a6f67741a6a
Author: inge4pres &lt;xxx@gmail.com&gt;
Date:   Sat May 1 18:35:35 2021 +0200

    microcommitting: best practices and tips

commit 4f351b07ecbf45d1f11268cc98f13506c7fdb81d
Author: inge4pres &lt;xxx@gmail.com&gt;
Date:   Fri Apr 2 21:25:16 2021 +0200

    microcommitting: draft intro and peer review

commit 5d96163f8bf4196a57f01b59de434395fc52a221
Author: inge4pres &lt;xxx@gmail.com&gt;
Date:   Sat Apr 3 19:28:17 2021 +0200

    Resume: update page
</code></pre><h3 id="wrap-up">Wrap up</h3>
<p>Micro-committing is very helpful to collaborate effectively on a project, it can be tedious in first instance and apparently the benefits are not tangible,
though adopting it will be very valuable in some occasions.</p>
<p>Try to practice it stick with mataining a clean Git history, then ask for feedback to the people collaborating with you on the repository.</p>
<h3 id="credits">Credits</h3>
<p>Thanks to all the mentors that helped me be more productive! 🙏</p>
<ul>
<li><a href="https://twitter.com/rmarioo">Mario Russo</a> for first exposing me to mirco-committing in a coding dojo, some years ago</li>
<li><a href="https://twitter.com/giankidipaol">Giancarlo Di Paolantonio</a> and <a href="https://twitter.com/granagli4">Paolo Banfi</a> for the refactoring/TDD tips</li>
<li><a href="https://twitter.com/seanhn">Sean Heelan</a> and <a href="https://twitter.com/ruehsen">Tim Ruhsen</a> for the peer review and Git history tips</li>
<li><a href="https://twitter.com/vic_mic_">Victor Michel</a> for the mind-blowing Git usage tricks</li>
</ul>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/cka-exam-experience/">CKA exam experience and preparation</a></h1>
  <p>Yes! Yesterday I received an awesome email stating that I cleared the <a href="https://www.cncf.io/certification/cka/">Certified Kubernetes Administrator</a> exam! 😎
Here I want to report my experience in preparing and taking the exam, hopefully this info can help others Kubernetes practitioners get the certification too.</p>
<h3 id="preparation">Preparation</h3>
<p>I consider myself lucky because for the past two years I had the chance to use Kubernetes working at <a href="https://lmgroup.lastminute.com/">lastminute.com</a>; on top this on-the-job training I went through a lot of studying and practicing because the exam itself has a lot of content.
Having some expertise of working with Kubernetes is definitely a huge help, as this specific exam is tailored for cluster administrators.
You will need to prove a thorough understanding of the Kubernetes primitives, architecture and operational strategies.</p>
<p>Do not fear though! The <a href="https://kubernetes.io/docs/home/">documentation</a> is extensive and very detailed on (most of) the topics that are needed to use and operate a cluster so here&rsquo;s my suggestions.</p>
<ul>
<li>
<h5 id="exam-structure">Exam structure</h5>
</li>
</ul>
<p>Understand the structure of the exam: 3 hours, 24 questions, 74% score needed to pass.
In my experience the first 10 questions will be the easiest, so try to complete them during the first hour; the last 3-4 questions are going to be very long to read and execute so you will need more time in the end.
The exam software will not warn you if the question exercise is completed: <em>read carefully</em> the tasks that will mark the exercise complete.</p>
<ul>
<li>
<h5 id="knowledge">Knowledge</h5>
</li>
</ul>
<p>Read through the whole documentation carefully, at least twice.
Get familiar with the structure of the docs: concepts, tasks, reference: they are organized in such a way that the content is mixed up so know where to look for.
The following books are recommended read:
- <a href="https://www.oreilly.com/library/view/kubernetes-up-and/9781492046523/">Kubernetes up and running</a>
- <a href="https://www.oreilly.com/library/view/managing-kubernetes/9781492033905/">Managing Kubernetes</a>
- <a href="https://www.oreilly.com/library/view/kubernetes-in-action/9781617293726/">Kubernetes in action</a></p>
<ul>
<li>
<h5 id="practice">Practice</h5>
</li>
</ul>
<p>If you have free credits for a cloud provider use them to spin up a cluster to play with; you will need to know well how to interact with the cluster using <code>kubectl</code> CLI.
Try all the commands listed in the <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">kubectl cheat sheet</a> and understand their effect on the cluster.
Explore the tutorials and tasks sections of the documentation and run them on the cluster to find caveats and errors in the docs.</p>
<p>Run through the amazing Kelsey Hightower <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes The Hard Way</a> at least twice; spin up VMs on your PC using Vagrant and try building a cluster from scratch.
Push it even further trying to break the cluster in every possible way and observe the effects of failure: systemd logs, kubectl errors, etc&hellip;</p>
<h3 id="taking-the-exam">Taking the exam</h3>
<p>I have to say the exam infrastructure is well done: in a single browser window you will have a left menu with the questions and a central section with a tmux terminal.
Before starting the exam a proctor will ask to confirm your identity and will validate the exam conditions are met: clean desk (no headset, no phone, no paper), empty and silent room.
During the exam you can have drinks but not food, and you can request a break at any time but the clock will continue to run during the break.
Some tips to meet the requirements.</p>
<ul>
<li>
<h5 id="workstation">Workstation</h5>
</li>
</ul>
<p>Remove everything from your exam desk except:</p>
<pre><code>- valid passport or national ID with picture
- a glass of water or a juice that can help you stay hydrated 
- the PC you will run the exam on, the optional external monitor and external mouse/keyboard
</code></pre>
<ul>
<li>
<h5 id="connectivity">Connectivity</h5>
</li>
</ul>
<p>Ensure you have enough bandwidth for a streaming connection (10Mb should be enough), because you will have to share your desktop(s) and show your face through the webcam for the whole time.
Prefer WiFi over cable: before the exam can start you will have to pan your camera around to show the proctor the whole room, probably twice. If you (like me) used the notebook integrated webcam you will have to move the screen left and right and all across the room
If you&rsquo;re at home reboot your router one hour before the exam starts, just to be sure.</p>
<ul>
<li>
<h5 id="manage-time">Manage time</h5>
</li>
</ul>
<p>There&rsquo;s a timer in the upper left corner of the exam window: use it to check that you are completing enough questions on time.
Questions vary from 2 to 8 percent weight; on average you will have 7.5 minutes for evey question, but the last 3/4 are much longer to read and complete as they involve troubleshooting and actions to complete: you probably have to recover/install parts of a cluster.
Save time on the first questions to have more for the last ones; when switching from a question to another, check the score percentage of the question so you can evaluate if to do it or skip it with respect to the remaining time.</p>
<ul>
<li>
<h5 id="use-the-resources">Use the resources</h5>
</li>
</ul>
<p>There&rsquo;s a staggering feature in the Kubernetes docs: <em>search</em>. When you are solving an exercise and need a reference or an example, search for it in the documentation!
As using the whole <code>*.kubernetes.io</code> domain is permitted during the exam you can consult examples, copy-paste YAML text and commands and even use snippets from the blog posts!
Use these resources as soon as you are confronted with a question for which <code>kubectl</code> commands are not enough, reading carefully a doc page or a blog post can get you out of trouble when something peculiar does not work as expected.</p>
<p>Another nice feature of the exam is a notepad you can open to take notes and copy-paste and edit content. I used it to take notes on the questions I was not able to complete fully and get back on them during the last 15 minutes of time.</p>
<h3 id="the-overall-exam-experience">The overall exam experience</h3>
<p>Although the exams rules are quite strict and I admit I was intimated reading the <a href="https://www.cncf.io/certification/cka/faq/">CKA FAQ</a>, setting up the exam room was easy and being able to run the test at home is a major plus compared to force you to go to a certification center.
The testing facility ran smoothly thanks to a Chrome extension that is required to run the exam and the instructions on how to complete the exercise were clear enough.</p>
<h3 id="last-but-not-least">Last but not least</h3>
<p>If you purchased the exam from the Linux Foundation or CNCF Foundation you should have a free retake.
If you fail the first time don&rsquo;t worry! CKA is a difficult exam, very rich of content and all hands-on, definitely one of the most difficult certifications I&rsquo;ve ever done.
Try again after you get back studying and practicing on what you could not solve the first time 💪</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/progressive-delivery-with-kubernetes/">Progressive Delivery with Kubernetes</a></h1>
  <p>I&rsquo;m more and more fond of finding the perfect solution to manage application delivery: dev teams want to be fast but their ops counterpart is not happy to loose control over the growing number of deployments that could cause an outage. We as an industry need to find the right balance to have features delivered in time and keep the service up and running for our users! And that&rsquo;s where progressive delivery can help!</p>
<p>What is progressive delivery? It&rsquo;s the evolution of continuous integration and continuous delivery practices, taken to the extreme - if this is the first time you hear about it, read <a href="https://redmonk.com/jgovernor/2018/08/06/towards-progressive-delivery/">this excellent post by James Governon</a>. But as of today what are the tools that embed this practice in deployment pipelines? None that I could find ☹️&hellip; hence I started this post to share some of the techniques that you can use to achieve progresive delivery today on Kubernetes! I&rsquo;d be really glad to have any comments and discuss on the matter.</p>
<h3 id="the-goal">The goal</h3>
<p>The progressive delivery manifesto (if there will ever be one) should explain the rationale why delivering feature in parts is better than all at once: feedback.
In this Agile world feedback is everything, and the only feedback that matters is your users&rsquo;; as Jez Humble puts it</p>
<blockquote>
<p>“Users don’t know what they want. Users know what they don’t want once you’ve built it for them.”</p>
</blockquote>
<p>You are not going to build anything useful if you don&rsquo;t collect your users opinion <em>while</em> building the product, that is why having a system that is able to change quickly and that can collect this feedback is so vital to success.</p>
<h3 id="to-mesh-or-not-to-mesh">To mesh or not to mesh</h3>
<p>The first approach to progressive delivery is via infrastructure components.
I cheated a bit in the post intro: there actually is a tool combination that is able to implement a feature close to progressive delivery right now: it&rsquo;s <a href="https://istio.io/">Istio</a> plus <a href="https://www.spinnaker.io/">Spinnaker</a>; the network mesh in this scenario is a router for connections originating by clients between multiple versions of the same backend, whose deployed versions are managed by Spinnaker releases. The mesh could be another product (Envoy, Linkerd, Consul Connect&hellip;) but it is the necessary component that contains the logic to serve the user a specific version of the application, based on goegraphic location rules, latency or even application rules (layer 7).</p>
<p>If you want to avoid the burden of installing and maintaining a mesh network you need to manage custom tooling to have the traffic routed for a subset of users to a specific version, <a href="https://github.com/bookingcom/shipper">Skipper</a> is a good example but comes with the restriction of not being able to manage percentage of traffic, so the percentage of user served is based only on the number of pods configured from service to service (so not ideal for small sized deployments).</p>
<p>The other way I see right now is creating a <a href="/2018/05/05/cloud-native-software-delivery/">Kubernetes operator and a CustomResourceDefinition</a> that can interact with the Ingress resource: this is hypothetical and I am not aware of any tool that is doing this but it could be posible to configure the ingresses to serve part of the requests by a specific Service (e.g. <code>v1.2.3</code> backed by a Deployment with a proper <code>selector</code>). As far as I know the current ingress controler based on nginx does not have such feature, but I just discovered writing this line that <a href="https://docs.traefik.io/user-guide/kubernetes/#traffic-splitting">Traefik does support this</a>! It would be great to understand if Traefik can manage multiple rules at once and if it can be managed via API so that the traffic is gradually moved from service to service.</p>
<h3 id="feature-flags">Feature flags</h3>
<p>Of course if you move to the application things get easier in terms of programmability, problem is they tend to be more difficult to manage at scale. If you use one of the <a href="http://featureflags.io/feature-flags/">multitude of available</a> feature-flag products (also referred as <a href="https://www.martinfowler.com/articles/feature-toggles.html">feature toggles</a>) you are soon going to be able to experiment with progressive delivery capabilities; your application will most likely contain the logic required to show a specific user a feature or another. While this is intriguing, if you have more than 2 product teams this can easily become a nightmare because if each team implements its own solution of feature toggle the company as a whole can really struggle to get what type of experience is serving to its users. Change management, for as light as it can be, should still be accounting for features enabled and disabled that may cause a service disuption, even if for a small percentage of users, and when the logic for serving different versions of your system is scattered around multiple applications, this goes quickly out of control.</p>
<p>One approach I&rsquo;ve seen succeed in using software-defined toggle is adopt a centralized, company-wide solution around an existing product: this simplifies greatly the management around features that are delivered passing through multiple services while being able to keep track of changes consitently.</p>
<h3 id="delivering-it">Delivering it</h3>
<p>Once you&rsquo;ve established an infrastructure for serving users based on some policies you should also have in place automation to be able to push your features out. In case you went for the infrastructure/network path you&rsquo;ll need a deployment tool that can sit between your CI artifacts and the platform running your services; on the contrary for a software-driven solution you will just need an application build and deployed regularly.
For the former I am really struggling to find a product that suites my need, I&rsquo;ve poked around with Spinnaker, ArgoCD and Tekton Pipelines but none of them seems to have the adequate primitives to address my progressive delivery needs.
I&rsquo;d be happy to hear from the community how this is being addressed: I&rsquo;d like to have a descriptive way of defining multiple versions of artifacts and configurations paired together (maybe via commit hash?) and have all of them deployed at the same time; I&rsquo;d also like to update the configurations of a given version while it&rsquo;s running.</p>
<p>Seems fair right? I might need to tweak my service here and there, but I&rsquo;d like to tune it only for a specific set of features that I know are in version ABC. Now I could not find on the internet a single product that works with Kubernetes able to satisfy this requirement, so please if you happen to have something in mind leave a comment!</p>
<h3 id="conclusion">Conclusion</h3>
<p>As always Kubernetes is a great enabler for delivery techniques based on software, it&rsquo;s an extensible platform and the multiple uses that can lead to achieve progressive delivery just confirm it. Personally I see a lot of space for progressive delivery in the upcoming future, especially for IoT. Let&rsquo;s see what&rsquo;s next!</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/golab-2018-wrap-up/">GoLab 2018: Wrap Up</a></h1>
  <p>I&rsquo;m in the train back from <a href="https://www.golab.io/">GoLab 2018</a> and I am so happy that I attended this conference!
It&rsquo;s been definitely one of most beautiful con I have attended in Italy, with tremendous speakers from all over the globe like Filippo Valsorda, Eleanor McHugh, Ron Evans and Bill Kennedy among many others; I have to say the organizers were just perfect in everything from the venue setup to the workshop organization, as if the quality of the talks ware not enough.
I was so delighted I want to write a wrap-up immediately with my head full of ideas for the upcoming future.</p>
<h3 id="speakers-layout">Speakers layout</h3>
<p>It was 2 dense days with 45 minutes talks and 4 tracks, 2 each day: Patterns, Embedded (day 1); Web, DX (day 2). Before the talks a keynote each day (Eleanor McHugh and Bill Kennedy respectively), 30 minutes for lightning talks at the end of day 1, cocktails and networking at the end of day 2.</p>
<h3 id="the-community">The community</h3>
<p>Poeple in the Gophers community is just awesome, and the environment was welcoming and warm in every part of the con; in the afternoon of day 1 a panel on diversity and inclusion was hoste by Cassandra Salisbury and it was really good in the sense that was not the regular D&amp;I talk with practices and models to adopt to &ldquo;pretend&rdquo;
that you are inclusive, it was actually a discussion and sharing of stories that enable a good community. Because a good community is inclusive by design, and the Go community is very good. During breaks I had the chance to shake hands and chat with many people I only had virtually met on Twitter before.</p>
<h3 id="things-i-learnt">Things I learnt</h3>
<p>Here&rsquo;s some of the thing I got to know thanks to the amazing talks I saw:</p>
<ul>
<li><a href="https://it.wikipedia.org/wiki/CERN">CERN</a> uses Go too! They rebuild their DNS service, see <a href="https://github.com/cernops/golbd">cernops/golbd</a></li>
<li>Google created a project called <a href="https://github.com/flutter/flutter/">Flutter</a> to build native mobile apps for multiple platforms [thanks <a href="https://twitter.com/edoardo849">@edoardo849</a>]</li>
<li>the go runtime is powerful but also can be tricky in some occasions, especially with closures; when using them =, make sure to clean up the resources they use and ensure local variable are scoped correctly to avoid data races and bugs. Always run tests with <code>-race</code> option and use channels and mutexes to orchestrate your pipelines [thanks <a href="https://twitter.com/empijei">@empijei</a>]</li>
<li>finite state machines are not only academic lecture, you can make them solve actual problems like decoding different variants of base64 [thanks <a href="https://twitter.com/annaopss">@annaopss</a>]</li>
<li>there are 2 packages for auto-generating Go structs code from an arbitrary input in JSON and XML format: <a href="https://github.com/bemasher/JSONGen">JSONGen</a> and <a href="https://github.com/dutchcoders/XMLGen">XMLGen</a></li>
<li>because Go is written in Go, you can parse <code>.go</code> files before they are compiled and create your own language extensions (macro) to be then re-compiled in a final binary, crazy! [thanks Max Ghilardi, see <a href="http://github.com/cosmos72/gomacro">cosmos72/gomacro</a>]</li>
<li><a href="https://github.com/fnproject">fnProject</a> is a serverless platform to run function-as-a-service on Oracle cloud and on your private cloud on Kubernetes, as I covered it in this <a href="https://inge.4pr.es/2018/01/30/serverless-on-kubernetes/">previous post</a></li>
<li>you can manipulate network packets directly with Go! The Linux kernel has a feature to let programs in userspace filter packets based on custom logic, so Telefonica develop (and open sourced) a Go library to manipulate packets because C++ with <code>libevent</code> hadn&rsquo;t enough throughput, so they moved from connection-based filtering to packet-based filtering (DPI) doubling throughput with 8 times less CPU! [See <a href="https://github.com/telefonica/nfqueue">telefonica/nfqueue</a>]</li>
<li>you can close a buffered channel immediately after sending all the items into it, and you&rsquo;ll still be able to read from it all the values; only then the <code>close(channel)</code> instruction will be magically executed by the runtime. So <a href="https://play.golang.org/p/3YYQo2WK37R">this is valid</a> [thanks <a href="https://twitter.com/goinggodotnet">@goinggodotnet</a>]</li>
<li>instrumenting and monitoring applications in Kubernetes can be fun! When building an operator you can interact with the exported metrics directly in the controller, you can enrich your systems with distributed traces thanks to OpenTracing/OpenCensus</li>
<li>using <a href="https://grpc.io/">gRPC</a> you can auto-generate Swagger documentation from protobuf service definitions, thus auto-generating web-browseable documentation of you service [thanks <a href="https://twitter.com/pawel_slomka">@pawel_slomka</a>]</li>
<li><code>io.Copy()</code> from the standard library is the most efficient way of sending network data from 2 <code>net.Conn</code> instances; this thanks to <a href="http://avtok.com/2014/11/05/interface-upgrades.html">interface upgrade</a> in the Copy() implementation, ending in leveraging the kernel packet passing in the most efficient way - without involving the application at all! [thanks <a href="https://twitter.com/filosottile">@filosottile</a>]</li>
</ul>
<p>And these are just a few (IMO the most important) things I got to know and discover these 2 days&hellip; What a ride! Can&rsquo;t wait for next year to be back in Florence!</p>
<p><!-- raw HTML omitted -->GoLab 2018<!-- raw HTML omitted --> <!-- raw HTML omitted --><!-- raw HTML omitted --></p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/cloud-native-software-delivery/">Cloud-native applications: Operator-Framework</a></h1>
  <p><strong>Cloud-native</strong>: sounds attractive right? What does it even mean? <a href="https://en.wikipedia.org/wiki/Cloud-native">Wikipedia</a> has no page on it already so anyone can give its own definition&hellip; Here&rsquo;s mine:</p>
<p><em>A Cloud-native application has only concern on the functionalities that it has to deliver as it is completely decoupled from the infrastructure it runs on</em></p>
<p>So how can software delivery be cloud-native? Isn&rsquo;t software delivery supposed to &ldquo;install&rdquo; software onto some infrastructure? Well if your infrastructure provider <em>is</em> cloud-native, you can transitively deliver software on it in a cloud-native way (counts of cloud-native is over 9000, so stopping here)!</p>
<p>Recently RedHat acquired CoreOS, bold move if you ask me. CoreOS since the M&amp;A has been very quite until a week ago when the <a href="https://github.com/operator-framework">operator-framework</a> was announced through a <a href="https://coreos.com/blog/introducing-operator-framework">blog post</a>; this is a huge step forward for everyone as this new toolkit will empower the average developer with the ability to run operators on Kubernetes and package their applications as extensions to the Kubernetes API.</p>
<p>Never heard of <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#customresourcedefinitions">Custom Resource Definitions</a>? You&rsquo;d better get on track as this will be driving the next-gen wave of applications that will run <em>as part</em> of the platform that delivers them, with the ability to automate their management and simplify dramatically their operations which will be tightly integrated with the cluster management itself.</p>
<p>And as usual I&rsquo;m eager to try out this new toy and see what can be done with it: I want to build a cloud-native software delivery application that will enable CI/CD jobs to be running inside the cluster and managed by the same API server! Using a CRD for the &ldquo;Pipeline&rdquo; kind I can control the build/test/release flow of my application and moreover monitor the whole thing with the same tools with which I will monitor my application.</p>
<h3 id="creating-cd">Creating CD³</h3>
<p>I decided to start a new project called <a href="https://github.com/inge4pres/cdkube">CD³ (cd kube)</a>: it will be a Continuous-* software that will run in Kubernetes and will be dedicated to deliver software <em>through</em> Kubernetes. I found Weaveworks did something similar with <a href="https://github.com/weaveworks/flux/">Flux</a> and since I don&rsquo;t want to reinvent the wheel I&rsquo;ll just try and replicate some functionality of running a continer in an operator.</p>
<p>First things first: I installed the <a href="https://github.com/operator-framework/operator-sdk">operator-sdk</a> and I have the binary in my <code>$GOPATH/bin</code>.
Running</p>
<pre tabindex="0"><code>$GOPATH/bin/operator-sdk new cdkube --api-version=delivery.inge.4pr.es/v1alpha1 --kind=Pipeline
</code></pre><p>I am resulting in an auto-generated project with some scaffold code, as in <a href="https://github.com/inge4pres/cdkube/commit/bd7a1cf8790cf02169057f759e40272993673181">this commit</a>.
Next I add some details which I think are at the core of a pipeline: the repository with code and configurations, the image to build the software and what version/name give to the resulting application artifact. When adding new items to the Spec and Status of CRD I will modify the <a href="https://github.com/inge4pres/cdkube/blob/master/pkg/apis/delivery/v1alpha1/types.go">pkg/apis/delivery/v1alpha1/types.go</a> file, then the guide suggests to run <code>operator-sdk generate k8s</code> to update the code, in fact the <code>zz_generated_deepcopy.go</code> is updated, but I don&rsquo;t see the <a href="https://github.com/inge4pres/cdkube/commit/5a17429d206475bcef85077d1f9aeb6fdda50357#diff-5cd8fbaa4e3fbb473c3e57c6fabca2f9">deploy/cr.yaml</a> changed as it should so probably there&rsquo;s a bug&hellip; Moving forward I have my operator logic to be defined now: I add some check whether the building pod is succeded and build the operator</p>
<pre tabindex="0"><code>operator-sdk build inge4pres/cdkube:v0.0.1
docker push inge4pres/cdkube:v0.0.1
</code></pre><p>My operator is built, pushed to dockerhub and ready to be kicked in a k8s cluster, so I fire up one in GKE</p>
<pre tabindex="0"><code>gcloud beta container --project &#34;inge4pres-gcp&#34; clusters create &#34;operator-framework-test&#34; --zone &#34;europe-west1-b&#34; --machine-type &#34;g1-small&#34; --image-type &#34;COS&#34; --disk-size &#34;25&#34;

gcloud container clusters get-credentials operator-framework-test --zone europe-west1-b --project inge4pres-gcp
</code></pre><p>and when it&rsquo;s ready I can deploy the auto-generated resources to the cluster.
An amazing result is that once the depoloyment is done I can create my CRD with the <code>kubectl</code> command and see my custom-type declared, running <code>kubectl create -f deploy/crd.yaml</code> I have a new object created in k8s</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>➜  ~ kubectl get pipeline
</span></span><span style="display:flex;"><span>NAME            AGE
</span></span><span style="display:flex;"><span>doing-nothing   16s
</span></span><span style="display:flex;"><span>➜  ~ kubectl describe pipeline
</span></span><span style="display:flex;"><span>Name:         doing-nothing
</span></span><span style="display:flex;"><span>Namespace:    default
</span></span><span style="display:flex;"><span>Labels:       &lt;none&gt;
</span></span><span style="display:flex;"><span>Annotations:  &lt;none&gt;
</span></span><span style="display:flex;"><span>API Version:  delivery.inge.4pr.es/v1alpha1
</span></span><span style="display:flex;"><span>Kind:         Pipeline
</span></span><span style="display:flex;"><span>Metadata:
</span></span><span style="display:flex;"><span>  Cluster Name:
</span></span><span style="display:flex;"><span>  Creation Timestamp:             2018-05-05T18:08:31Z
</span></span><span style="display:flex;"><span>  Deletion Grace Period Seconds:  &lt;nil&gt;
</span></span><span style="display:flex;"><span>  Deletion Timestamp:             &lt;nil&gt;
</span></span><span style="display:flex;"><span>  Initializers:                   &lt;nil&gt;
</span></span><span style="display:flex;"><span>  Resource Version:               <span style="color:#bd93f9">1528</span>
</span></span><span style="display:flex;"><span>  Self Link:                      /apis/delivery.inge.4pr.es/v1alpha1/namespaces/default/pipelines/doing-nothing
</span></span><span style="display:flex;"><span>  UID:                            5160a990-508f-11e8-8f06-42010a8401f8
</span></span><span style="display:flex;"><span>Spec:
</span></span><span style="display:flex;"><span>Spec:
</span></span><span style="display:flex;"><span>  Build Arguments:
</span></span><span style="display:flex;"><span>    building something...
</span></span><span style="display:flex;"><span>    now I&#39;m <span style="color:#ff79c6">done</span>
</span></span><span style="display:flex;"><span>  Build Commands:
</span></span><span style="display:flex;"><span>    <span style="color:#8be9fd;font-style:italic">echo</span>
</span></span><span style="display:flex;"><span>  Build Image:     busybox
</span></span><span style="display:flex;"><span>  Repo:            http://github.com/inge4pres/just-a-test
</span></span><span style="display:flex;"><span>  Target Name:     tesApp
</span></span><span style="display:flex;"><span>  Target Version:  0.1
</span></span><span style="display:flex;"><span>Events:            &lt;none&gt;
</span></span><span style="display:flex;"><span>➜  ~</span></span></code></pre></div>
<p>When I deploy my operator and create the custom resource applying the manifest for a pipeline, the operator picks it up and starts a container with the name of the pipeline</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>➜  cdkube git:<span style="color:#ff79c6">(</span>master<span style="color:#ff79c6">)</span> ✗ kubectl apply -f deploy/cr.yaml
</span></span><span style="display:flex;"><span>➜  cdkube git:<span style="color:#ff79c6">(</span>master<span style="color:#ff79c6">)</span> ✗ kubectl get po
</span></span><span style="display:flex;"><span>NAME                      READY     STATUS    RESTARTS   AGE
</span></span><span style="display:flex;"><span>cdkube-57bcf65584-sbvd6   1/1       Running   <span style="color:#bd93f9">0</span>          12s
</span></span><span style="display:flex;"><span>➜  cdkube git:<span style="color:#ff79c6">(</span>master<span style="color:#ff79c6">)</span> ✗ kubectl apply -f deploy/cr.yaml
</span></span><span style="display:flex;"><span>pipeline <span style="color:#f1fa8c">&#34;doing-nothing&#34;</span> created
</span></span><span style="display:flex;"><span>➜  cdkube git:<span style="color:#ff79c6">(</span>master<span style="color:#ff79c6">)</span> ✗ kubectl get po
</span></span><span style="display:flex;"><span>NAME                      READY     STATUS              RESTARTS   AGE
</span></span><span style="display:flex;"><span>cdkube-57bcf65584-sbvd6   1/1       Running             <span style="color:#bd93f9">0</span>          29s
</span></span><span style="display:flex;"><span>testapp                   0/1       ContainerCreating   <span style="color:#bd93f9">0</span>          2s
</span></span><span style="display:flex;"><span>➜  cdkube git:<span style="color:#ff79c6">(</span>master<span style="color:#ff79c6">)</span> ✗ kubectl logs cdkube-57bcf65584-sbvd6
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">time</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;2018-05-05T18:42:59Z&#34;</span> <span style="color:#8be9fd;font-style:italic">level</span><span style="color:#ff79c6">=</span>info <span style="color:#8be9fd;font-style:italic">msg</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Go Version: go1.10.2&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">time</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;2018-05-05T18:42:59Z&#34;</span> <span style="color:#8be9fd;font-style:italic">level</span><span style="color:#ff79c6">=</span>info <span style="color:#8be9fd;font-style:italic">msg</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Go OS/Arch: linux/amd64&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">time</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;2018-05-05T18:42:59Z&#34;</span> <span style="color:#8be9fd;font-style:italic">level</span><span style="color:#ff79c6">=</span>info <span style="color:#8be9fd;font-style:italic">msg</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;operator-sdk Version: 0.0.5+git&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">time</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;2018-05-05T18:42:59Z&#34;</span> <span style="color:#8be9fd;font-style:italic">level</span><span style="color:#ff79c6">=</span>info <span style="color:#8be9fd;font-style:italic">msg</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;starting pipelines controller&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">time</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;2018-05-05T18:43:21Z&#34;</span> <span style="color:#8be9fd;font-style:italic">level</span><span style="color:#ff79c6">=</span>info <span style="color:#8be9fd;font-style:italic">msg</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;build still running, status of builder container: Pending&#34;</span>
</span></span><span style="display:flex;"><span>➜  cdkube git:<span style="color:#ff79c6">(</span>master<span style="color:#ff79c6">)</span> ✗ kubectl get po
</span></span><span style="display:flex;"><span>NAME                      READY     STATUS      RESTARTS   AGE
</span></span><span style="display:flex;"><span>cdkube-57bcf65584-sbvd6   1/1       Running     <span style="color:#bd93f9">0</span>          49s
</span></span><span style="display:flex;"><span>testapp                   0/1       Completed   <span style="color:#bd93f9">2</span>          22s</span></span></code></pre></div>
<p>and if I get the <code>testapp</code> logs I can see the pipeline execution</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>➜  cdkube git:<span style="color:#ff79c6">(</span>master<span style="color:#ff79c6">)</span> ✗ kubectl logs testapp
</span></span><span style="display:flex;"><span>building something... now I&#39;m <span style="color:#ff79c6">done</span></span></span></code></pre></div>
<p>Bonus: if the custom resource is deleted with <code>kubectl delete -f deploy/cr.yaml</code> the pod <code>testapp</code> gets terminated automatically! I don&rsquo;t know if this magic is done by Kubernetes or by the Operator Framework, but sure I love it as it will enable my resources to be managed with the k8s API just like any other.</p>
<p>I still need to tune the logic of handling the container as I didn&rsquo;t expect the restarts to happen, but hey this looks amazing! In a couple of hours I have an operator that can spawn containers when instructed to do so by custom manifests representing a pipeline!</p>
<p>Stay tuned 😎</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/continuous-delivery-with-drone/">Continuous Delivery with Drone</a></h1>
  <p>Continuous Delivery should be a solved issue: the practice is well-defined and there is a plethora of tools implementing it with more or less peculiarities, but still many struggle implementing it. The dream of a perfect continuous deployment flow from the developer to the production environment with software quality gates based on automated tests is still alive in me, I tried and tried several times with multiple implementations on multiple platforms and never got to the point where I could say: &ldquo;I&rsquo;m done, this works exactly as I wanted&rdquo;.</p>
<p>So I stumbled on <a href="https://drone.io">drone</a> and decided to give it a go: it&rsquo;s an open-source project, written in Go and with a SaaS offering via their website. The concept I like is that every step of the build/deployment process runs through a container and this is very close to my idea of a modern CD tool, a platform where I can compose pipelines by chaining containers execution on a shared workspace. Love it already.</p>
<h3 id="installing-the-stack">Installing the stack</h3>
<p>You can run the drone server and agent locally on your laptop with <code>docker-compose</code> as detailed <a href="http://docs.drone.io/installation/">here</a>. Only issue is: for integrating with any of the big Git cloud provider (Github and Gitlab) you will need to expose your service to the internet, so I&rsquo;ll use a local instance of <a href="https://gogs.io/">gogs</a> running in a docker container from the <a href="https://github.com/gogits/gogs/tree/master/docker">official image</a>. All I need is in the <code>docker-compose.yml</code> file: I added just a couple of <code>volumes</code> directive compared to the <a href="http://docs.drone.io/install-for-gogs/">original one</a> and I am using the docker-for-mac internal hostname to resolve the bridge IP internally as detailed <a href="https://docs.docker.com/docker-for-mac/networking/#use-cases-and-workarounds">here</a>. This is a lab setup and having a production-ready installation will require database setup and filesystem persistence, but I don&rsquo;t have this requirement now. After a <code>docker-compose up -d</code> my stack is ready.</p>
<h4 id="installing-the-cli">Installing the CLI</h4>
<p>As easy as following <a href="http://docs.drone.io/cli-installation/">this guide</a>. Once logged to the web UI I navigate to the <code>${DRONE_HOST}/account/token</code> page where I can get a token to configure the CLI.</p>
<h3 id="adding-secrets">Adding secrets</h3>
<p>There is a nice feature in drone: I can <a href="http://docs.drone.io/manage-secrets/">manage secrets</a> directly from the command line and they can be scoped globally or be available only to one pipeline step (corresponding to an image). I will need to add Dockerhub username and password to the <code>plugin/docker</code> image to be able to push the image, so I add this 2 secrets with the following</p>
<pre tabindex="0"><code>drone secret add -repository=inge/goapp -image=plugins/docker -name=docker_username -value=inge4pres
drone secret add -repository=inge/goapp -image=plugins/docker -name=docker_password -value=***************
</code></pre><h3 id="a-sample-pipeline">A sample pipeline</h3>
<p>As many of the continuous delivery tools available on the market, drone uses a YAML configuration file in the root of the repository, so adding a <code>.drone.yml</code> hidden file is enough to start hooking every commit to the build system. I configured a <a href="https://github.com/inge4pres/blog/blob/master/continuous-delivery-with-drone/test-app/.drone.yml">3 stages pipeline</a>:</p>
<ul>
<li>test and build artifact</li>
<li>publish the artifact</li>
<li>deploy the application</li>
</ul>
<p>It&rsquo;s very simple to get started and the one-container-per-step architecture makes it trivial to glue together multiple steps. There is an implicit concept of <a href="http://docs.drone.io/workspace/">shared workspace (configurable)</a> that you can leverage to use Makefile and Dockerfile just as the build was happening on your local environment.</p>
<p>So I really recommend trying out drone and reporting some issues if you find any, for the time being I am very excited to have a CD product entirely written in Go - I think I will contribute to the project to have some enhancements available in the free version.</p>
<p>Below here some screenshots of the drone web UI and the pipeline resulting from the YAML config file: I will explore more complex workflows like <a href="http://docs.drone.io/promoting-builds/">promoted builds</a> and <a href="http://docs.drone.io/gated-builds/">gated builds</a> - and build more in the tool if I need.</p>
<figure><img src="https://raw.githubusercontent.com/inge4pres/blog/master/continuous-delivery-with-drone/screenshots/drone-local.jpeg"/><figcaption>
            <h4>drone repository view</h4>
        </figcaption>
</figure>

<figure><img src="https://raw.githubusercontent.com/inge4pres/blog/master/continuous-delivery-with-drone/screenshots/drone-detail.jpeg"/><figcaption>
            <h4>drone pipeline log</h4>
        </figcaption>
</figure>

<figure><img src="https://raw.githubusercontent.com/inge4pres/blog/master/continuous-delivery-with-drone/screenshots/drone-error.jpeg"/><figcaption>
            <h4>drone stage error</h4>
        </figcaption>
</figure>

<figure><img src="https://raw.githubusercontent.com/inge4pres/blog/master/continuous-delivery-with-drone/screenshots/drone-running.jpeg"/><figcaption>
            <h4>drone running</h4>
        </figcaption>
</figure>


</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2017-10-28-golang-concurrency-pattern/">Golang Concurrency Patterns</a></h1>
  <p>In the early days of Go the language was often tailored towards &ldquo;system programming&rdquo; due to its C-stlye syntax and ability to write high-performance applications. Few time after, Go adoption was starting to gain traction for distributed systems development and projects like etcd, docker and kubernetes revealed the power of the networking capabilities offered by the internals in the language. Along the way a lot of libraries have been built around the powerful primitives offered by Go but in my opinion there is not enough use literature around the <em><a href="https://en.wikipedia.org/wiki/Communicating_sequential_processes">Communicating Sequential Processes</a></em> implementation available through channels and goroutines, they are not even widely used in the standard library. I&rsquo;ll detail here some concurrency patterns that I found useful and hopefully they&rsquo;ll be idiomatic enough to represent a good use case for you.</p>
<h5 id="a-premise">A premise</h5>
<p>CSP it&rsquo;s kind of a similar feature to threading but there are some differences; to know more on CSP I really recommend watching Rob Pike&rsquo;s <a href="https://vimeo.com/49718712">excellent talk on the topic</a>.</p>
<h4 id="my-experience">My experience</h4>
<p>Personally it took me a while to find my way out of the issues I ran into when first using concurrency features in Go: they are definitely the most complicated part of using Go, which is on average simpler that any other language I tried. So for me, the biggest problem was to understand what it means to have a goroutine spawned and how to control its execution or get data out of it, so I put together a list of examples on how concurrent programs flow can be controlled with the primitives built in the language.</p>
<h4 id="channel-channels-channels-everywhere">Channel, channels, channels everywhere</h4>
<p>A channel in Go is a way to pass messages between functions and goroutines, the official definition from <a href="https://tour.golang.org/concurrency/2">A Tour of Go</a> is:</p>
<p><em>Channels are a typed conduit through which you can send and receive values with the channel operator, <code>&lt;-</code></em></p>
<p>So what are they good for? They are actually not very helpful without goroutines: a goroutine is a lightweigth thread managed by the Go runtime (<a href="https://tour.golang.org/concurrency/1">definition</a>), think like a background process that can be spawned and does not need to be managed directly by you, I like the concept of <em>&ldquo;run and forget&rdquo;</em>.
The easiest concurrency pattern available is thinking of a goroutine processing some data in the background and returning them through a channel to the main thread executing our code; this can be very powerful and scale well to multiple functions and channels.</p>
<h4 id="waitgroups">WaitGroups</h4>
<p><a href="https://golang.org/pkg/sync/#WaitGroup">WaitGroups</a> are part of the <code>sync</code> package from the Go standard lib: they are a way for waiting the execution of goroutines to end properly and ensure all the work done in the background is completed. WaitGroups are often used with <code>defer</code> to fill in the wait queue when the goroutine exits.</p>
<h4 id="some-examples">Some examples</h4>
<p>For me the most difficult thing to understand when approaching concurrency was how to ensure all of my goroutines completed execution: to do this the easiest way is using WaitGroups as in <a href="https://github.com/inge4pres/blog/blob/master/golang-concurrency-patterns/waitgroup_test.go">waitgroup_test.go</a>: <code>wg.Add(1)</code> adds one item in the wait queue and <code>wg.Done()</code> removes one item from it; using <code>wg.Wait()</code> in the main process makes the process wait until the wait group is emptied.
If you run the tests with</p>
<pre tabindex="0"><code>go test -v . -race -run ^TestWaitGroup
</code></pre><p>you can see the execution time when using concurrency or not. Changing the value of <code>ops</code> variable in <a href="https://github.com/inge4pres/blog/blob/master/golang-concurrency-patterns/functions_test.go#L3">functions_test.go</a> will make the tests process less or more items.</p>
<p>With channels there are more features and gotchas that need to be taken into account:</p>
<ul>
<li>a read from a closed channel returns the type&rsquo;s zero-value</li>
<li>a send to a closed channel will <code>panic</code></li>
<li>a read and a send alone to an unbuffered channel are blocking: they will generate a deadlock if there is not a corresponding send/read operation on the other side of the channel</li>
<li>a send on a buffered channel will block when the buffer is full and no other read is happening on the other side</li>
</ul>
<p>That being said, there are a couple of notable usages that I like to include in my concurrency-enabled Go software: the <a href="https://github.com/inge4pres/blog/blob/master/golang-concurrency-patterns/channels_test.go#L22">fan-out pattern</a> where an input generates multiple goroutines that perform operations in the background and the output of the concurrent goroutines is fetched by a channel in the main thread. Another pattern is <a href="https://github.com/inge4pres/blog/blob/master/golang-concurrency-patterns/channels_test.go#L36">fan-in</a>: multiple functions can return values to a channel as long as the type is consistent. Run tests with</p>
<pre tabindex="0"><code>go test -v . -race -run ^TestChannelBuffered
</code></pre><p>to see fan-out/fan-in patterns in action.</p>
<p>Another interesting feature is powered by the <code>select</code> statement: you can read from multiple channels in the same function and define behavior for any given channel message, it is another sample of fan-in pattern. Using <code>select</code> will block until one of the send/receive operation is available, the operation gets chosen randomly if multiple are available at the same time. <code>select</code> has a similar syntax to <code>switch</code> so <code>case</code> and <code>default</code> are the scenario selector. Running <a href="https://github.com/inge4pres/blog/blob/master/golang-concurrency-patterns/channels_test.go#L55">multiple channels</a> lets you manage multiple types in a single point: run the test with</p>
<pre tabindex="0"><code>go test -v . -race -run ^TestMultipleChannelsSelect$
</code></pre><p>to check the execution of the multiple goroutines.</p>
<h4 id="conclusions">Conclusions</h4>
<p>My experience with Go concurrency primitives is still forming, I hope I can read and experiment more on the topic as it&rsquo;s one of Go&rsquo;s most powerful and at the same time less documented features! I&rsquo;d really love to hear feedback from the read so if you get up to this point, take a step forward and leave a comment below, I&rsquo;d really love to discuss.</p>
<h4 id="references">References</h4>
<ul>
<li><a href="https://gobyexample.com/worker-pools">Worker pools</a></li>
<li><a href="https://golangbot.com/channels/">Channels</a></li>
<li><a href="https://www.youtube.com/watch?v=yKQOunhhf4A">Concurrency made easy - Dave Cheney</a></li>
<li><a href="https://golang.org/doc/codewalk/sharemem/">Share memory by communicating - Codewalk</a></li>
<li><a href="http://www.jtolds.com/writing/2016/03/go-channels-are-bad-and-you-should-feel-bad/">Go channels are bad and you should feel bad</a></li>
</ul>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2017-10-01-getting-started-with-google-cloud-builder/">Getting Started With Google Cloud Builder</a></h1>
  <p>One of the advantages of containerized applications is the standardization, some would say &ldquo;write it once, runs everywhere&rdquo; but that&rsquo;s another motto for another product. Anyway with a new packaging technology the same problems are faced: build reproducibility, or the necessity for people doing Ops to know they are going to deploy the same exact piece of code the Dev team used in their tests. So to address this issue the container image needs to be immutable: once it&rsquo;s built, it&rsquo;s not going to be changed, ever. And the same image will be used for testing, QA, beta, preview, presales-demo, whatever environment you need to deploy the app to.</p>
<h4 id="building-it">Building it</h4>
<p>Docker has been around for a few years now, it&rsquo;s mature and stable, but ask anyone using it if they&rsquo;d allow images built on development workstation to run in production: not gonna happen! The artifact that will serve production traffic will pass through the CI/CD pipelines and pushed to the registry, this is the way of shipping containers. &ldquo;That&rsquo;s easy&rdquo;, you&rsquo;ll say, &ldquo;I&rsquo;ll stick my Dockerfile in the repo and let the build system do the magic!&rdquo;, but that&rsquo;s not the whole picture: there are lower layers to pull, tests to be run and <em>only after they succeed</em> you can build the container. So who is going to maintain all of this configurations? Can we store them into the repo too? Yes! With GCB you can write a declarative multi-step workflow that will compile, test and package the code in a container; the container will get immediately pushed to the Google Container Registry too, so it&rsquo;s ready to be consumed (maybe Kubernetes on GKE?).</p>
<h4 id="a-simple-application">A simple application</h4>
<p>There is quite a good number of examples in the <a href="https://github.com/GoogleCloudPlatform/cloud-builders" title="GCB on Github">cloud builder repository</a> but I&rsquo;d like to create a fresh one with Golang: a random number generator. The app will serve a random integer via HTTP, and you can see the code is <a href="https://github.com/inge4pres/blog/blob/master/getting-started-with-google-cloud-builder/main.go">very straightforward</a>.
Now I want to build a container to run into GCP with confidence so that every time I make a build I will have a new container image tagged with the version and ready to roll it out.</p>
<h4 id="using-gcb">Using GCB</h4>
<p>All builds in GCB happen in a container, right now the only engine supported is Docker but more are going to be added. You can leverage Google pre-baked builders or use your own images as builders, in the example I am using Google&rsquo;s <code>golang-project</code> image to compile and <code>docker</code> to build the final docker image and push it to registry. Note how some environment variables are injected to the container, like <code>PROJECT_ID</code> is your GCP running project as configured via</p>
<pre tabindex="0"><code>gcloud auth login
gcloud config set project your-gcp-project
</code></pre><h5 id="_side-note-for-gophers_"><em>Side note for gophers</em></h5>
<p>The <code>golang-project</code> image does some checks at setup to determine the workspace structure: there need to be a <code>./src</code> folder or <code>GOPATH</code> must be passed, or the simplest way is to <a href="https://github.com/inge4pres/blog/blob/master/getting-started-with-google-cloud-builder/main.go#L1">insert a comment next to <code>package main</code> in <code>main.go</code></a></p>
<h4 id="running-the-build">Running the build</h4>
<p>It&rsquo;s as easy as executing</p>
<pre tabindex="0"><code>gcloud container builds submit --config cloudbuild.yaml .
</code></pre><p>in the root of your project.</p>
<p>See it in action!</p>
<!-- raw HTML omitted -->
<p>As you can see the current folder (<code>.</code> as last parameter) is compressed and shipped to a Cloud Storage random location, then GCB starts the steps listed in the configuration YAML file, running step by step the containers with their arguments.</p>
<h4 id="conclusion">Conclusion</h4>
<p>GCB is fast and very easy to use but for what I&rsquo;ve been able to test is bound to GCP right now, so if you are willing to deliver a service from Google Cloud and your application is containerized or <em>&ldquo;cloud native&rdquo;</em> you have a lightweight build system ready to go, but if you have a private registry or other integrations to do, GCB might still be a too small niche.</p>
<p>I&rsquo;ll make more tests and try to hack a bit GCB in the future so stay tuned!</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2016-12-03-automate-tls-management-on-aws-with-letsencrypt/">Automate TLS management on AWS with LetsEncrypt</a></h1>
  <p>Letsencrypt is cool: automated, free TLS certificates for everybody! They are sponsored mainly by internet corps and they started a crowd-funding campaign to avoid the influence of this corps in the future of the project. I recently moved the blog to hugo on AWS and I&rsquo;m now porting the TLS management scripts I wrote a while ago on AWS: this is a nice exercise to give a proper TLS automation valid for everyone on AWS.</p>
<p>I used <a href="https://terraform.io" title="terraform">Terraform</a>, the AWS CLI and the amazing (although still in beta) <a href="https://github.com/xenolf/lego" title="lego">lego</a>; the idea is to spin up an EC2 instance every month to query Letsencrypt to renew a  certificate I already provisioned; the instance will have permissions to update the certificate using <a href="https://aws.amazon.com/certificate-manager/" title="aws acm">AWS Certificate Manager</a> and other services like API Gateway custom domain names and Cloudfront are already bound to use ACM certificate of the domain. I tried and make the Terraform code as abstract and reusable as possible: cloudfront distribution id, domain name and issuer mail are all variable configurable via config file or at command line.</p>
<h4 id="setting-up-the-environment">Setting up the environment</h4>
<p>I prepared the TLS certificate and key running first lego once on my local box, using the DNS challenge against AWS Route53, lego will use Letsencrypt ACME secret and put it into a TXT record for the domain to be validated so that Letsencrypt will know I am the owner of the domain. You can read more on Letsencrypt domain ownership validation <a href="https://letsencrypt.org/how-it-works/" title="Letsencrypt how it works">here</a>.</p>
<p>After the lego account is created, generally in <code>$HOME/.lego</code>, you will find private key and certificate is created for the domain(s); the certificate is a bundle of all the SAN submitted in the request. Then how to automate all of this on AWS?</p>
<h4 id="designing-automation">Designing automation</h4>
<p>The idea is that every service that needs TLS encryption will be able to fetch the certificate from an AWS service. This was not possible until AWS Certificate Manager was released a while ago: ACM will create or store a certificate to be used in other AWS services via the AWS API. No more magic to spread certificates via S3 or other tricks, you now have an <code>awscli</code> command!
So here I chose to use Terraform to bootstrap all the infrastructure to run lego and run Letsencrypt automatically via AWS Autoscaling group.</p>
<p>I first imported the certificate created with lego in ACM in us-east-1 region (N. Virginia): this is due to API Gateway limitations to be able to integrate natively with ACM only in us-east-1. I get the certificate ARN for the certificate just uploaded and I run <code>terraform plan -var 'cf_distribution_id=EX4MPL3D15TR0' -var 'certificate_arn=...'</code>.
Once completed I can see a new Autoscaling group with scheduled policies, a launch configuration that starting from the base Amazon AMI with a user-data script at each boot will:</p>
<ul>
<li>download and install lego binary</li>
<li>sync the TLS account with the S3 bucket created</li>
<li>split the certificate from the chain, as they need to be uploaded separately</li>
<li>call the <code>acm</code> subcommand of <code>awscli</code> to update the certificate</li>
</ul>
<p>This is amazing, I can now forget about TLS management for all of my websites!</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2016-11-17-4pres-goes-serverless/">4pres goes #serverless</a></h1>
  <p>Last month I felt I was a little late for the <a href="https://charity.wtf/2016/05/31/wtf-is-operations-serverless/" title="#serverless">#serverless</a> party going on all over the internet and I started taking a look at what the pros and cons would be to actually not manage any server myself.
Shutting down my VPS hosting my apps I will lose my mail server, my MySQL instances and my Docker registry but: who cares? There are cloud services I can use with hundreds of times more availability and for a fraction of the cost.</p>
<h1 id="why-serverless">Why #serverless?</h1>
<p>We are moving more and more rapidly to a developer friendly world: all cloud providers tend to relief companies from the burden of managing complex cluster architectures and it&rsquo;s not a coincidence that things like <a href="https://landing.google.com/sre/interview/ben-treynor.html" title="SRE at Google">SRE at Google</a> exist. Systadmins are no longer required where they can be substituted with far more performance by declarative syntax clusters (Kubernetes, Docker Data Center) and reliabile, consistent configuration deployment (Ansible, Puppet) in managing high volumes of phisycal or virtual machines.</p>
<p>This shift to a developer-centric world forces who embraces it to &ldquo;trust&rdquo; the IaaS, PaaS and FaaS providers but in the same time let them focus on core and valuable development processes.</p>
<p>That&rsquo;s why.</p>
<p>Take the simplest app in the world: <a href="https://4pr.es">4pres</a>, a URL shortener. It needs a presentation, computation and data layer as most of apps. Traditionally and depending on your budget and needs, you would spin up one or more VPS and deploy software on them (containers or not, doesn&rsquo;t matter here). The setup of nginx as reverse proxy to your application already requires skills that most of developers don&rsquo;t have, but in first hand why should anybody have them when there is a service like AWS API Gateway that lets you deploy one in seconds?
Having the possiility to do so, you <em>may</em> want to forget about everything not strictly related to your app, so focusing only on building and maintaining functionalities of your app.</p>
<h1 id="how-you-can-migrate-your-app-today">How you can migrate your app today</h1>
<p>In terms of cloud provider I have a lot of experience with AWS therefore my first thought is for them when trying to do something like this: they probably already have enough mature services supporting what I need. Don&rsquo;t take this as a sponsorship: you can do the same with any other provider.</p>
<h4 id="traditional-architecture">Traditional Architecture</h4>
<ul>
<li>NGINX reverse proxy</li>
<li>Golang application</li>
<li>MySQL database</li>
</ul>
<p>NGINX terminates SSL and proxy back to app every request. The <a href="https://github.com/inge4pres/4pr.es" title="4pres/master">Golang app</a> finds out what to do from a combination of HTTP Method and URL Path:</p>
<ul>
<li><code>GET /</code> renders the landing page template</li>
<li><code>GET /{url}</code> queries the DB and redirect to long url or render 404 template</li>
<li><code>POST /</code> create a short link from <code>form-data url=http://alongurul/</code> and display the result template</li>
</ul>
<h4 id="serverless-architecture-aws">Serverless Architecture (AWS)</h4>
<ul>
<li>API Gateway expose a request/response mapping endpoint with integration to other services</li>
<li>S3 to store and serve the static content</li>
<li>Lambda executes Golang functions thanks to the amazing <a href="https://github.com/eawsy/aws-lambda-go" title="eawsy/aws-go-lambda">eawsy/aws-go-lambda</a> framework</li>
<li>DynamoDB stores data in schema-less fashion (JSON)</li>
</ul>
<p>The API Gateway definition acts as proxy and:</p>
<ul>
<li><code>GET /</code> serves static page <code>index.html</code> from S3 bucket with &lsquo;Website Hosting&rsquo; option enabled</li>
<li><code>GET /{url}</code> runs a Lamdba function <a href="https://github.com/inge4pres/4pr.es/blob/serverless/cmd/4pres_get/get.go" title="4pres_get">4pres_get</a> that fetches the URL Path parameter from DynamoDB and redirect the client or renders a 404 template</li>
<li><code>GET /s?{urlencodedURL}</code> runs a Lambda function <a href="https://github.com/inge4pres/4pr.es/blob/serverless/cmd/4pres_post/post.go" title="4pres_post">4pres_post</a> that creates a short URL and tries to store it in DynamoDB, returning the result template or the 500 template.</li>
</ul>
<p>Not that big change in the overall design, but the code for the Golang app only shares a function to shorten the URL between the 2 implementations: that is understandable because we no longer manage HTTP requests attributes and delegate that to API Gateway, we don&rsquo;t display static content anymore and leave that to S3. At the core we only have 2 things to worry about:</p>
<ul>
<li>store a URL (4pres_post)</li>
<li>get  URL to redirect the client</li>
</ul>
<p>and then we can focus on extending new features:</p>
<ul>
<li>URL expiration</li>
<li>user registration</li>
<li>whatever!</li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<p>Thanks to this development model our craftsmen effort can be 100% dedicated to building features and forget about the rest: this can be a great advantage at large scale. Recently I have come across another interesting framework for Function as a Service called <a href="https://www.iron.io/" title="iron.io">iron.io</a> and I am willing to try it as soon as possible so stay tuned!</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2016-10-10-moving-the-blog-to-hugo/">Moving the blog to hugo</a></h1>
  <p>In times of experimenting, I am now having a lot of fun with docker, rkt, kubernetes and containers ecosystem in general. But one thing I never forget to play with is content editing and publishing! So here I am, trying to migrate all my blog and website to Hugo :)</p>
<p>So instead of a bare VPS I am moving my blog to AWS S3 + Cloudfront CDN. This will be more scalable and far less expensive. And Hugo generates static HTML so no more patching security issues.</p>
<h3 id="how-i-did-it">How I did it</h3>
<h5 id="migrate-contents">Migrate contents</h5>
<p>First I found that there is a page on Hugo site that explains how to migrate from any CMS know to human to Hugo. I used the wordpress export plugin to convert the site from Wordpress to markdown and make it work ith Hugo: it did the the job pretty well but still I had to convert some of the posts.</p>
<h5 id="https">HTTPS</h5>
<p>Then the hard part: HTTPS! I chose to use <a href="http://letsencrypt.org" title="lestencrypt">letsencrypt</a> to automate the TLS certificate handling. The powerful <a href="https://github.com/xenolf/lego" title="lego">lego</a> is way more multi-platform than the official certbot from EFF written in Python, and has built-in support for DNS challenge on Route53! Generating a new certificate of renewing is <a href="https://gist.github.com/inge4pres/597bb9350ff3e9cc43ecb476a10e636b" title="gist">a couple of commands away</a>.</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2016-09-18-a-benchmark-of-aws-efs/">A benchmark of AWS EFS</a></h1>
  <p>Amazon Web Services <!-- raw HTML omitted -->Elastic File System<!-- raw HTML omitted --> has been to my knowledge the service to have the longest beta testing period: reason for this may be that not as many client as expected tested it and AWS received too few feedback on it or that there were issues not to release GA. I don’t want to speculate on which one is correct but now that it has been <!-- raw HTML omitted -->officially released<!-- raw HTML omitted --> I decided to give it a try and of course compare it to a self-managed solution on the same platform.</p>
<p>If you followed AWS evolution you may agree that EFS has been introduced to fill the gap between EBS storage and S3: before EFS was live there was no “easy” way of having a distributed file system in AWS, you could only <!-- raw HTML omitted -->set up your own<!-- raw HTML omitted --> using  a combination of EC2 instances mounting Elastic Block Storage volumes and S3. Now with EFS you can have a AWS-managed distributed file system to be used in your cloud environment or even across the internet (will try that on a public subnet) with all the benefits of offloading the high-availability and replication burden to Amazon, and at a reasonable price. Will performance be enough compared to a self-managed solution?</p>
<h5 id="playground">Playground</h5>
<p>I use <!-- raw HTML omitted -->terraform<!-- raw HTML omitted --> to create an infrastructure template to run the tests, you can see it <!-- raw HTML omitted -->here<!-- raw HTML omitted -->. Once</p>
<!-- raw HTML omitted -->
<p>has finished, you’ll end up with:</p>
<ul>
<li>An EFS with General Purpose performance mode</li>
<li>An EFS mount target for 1 Availability Zone</li>
<li>1 EC2 instance named “client” to mount remote file systems</li>
<li>2 EC2 instances named “server_X” each one with a 10 GB General Purpose EBS, they will serve a self-managed distributed, replicated file system</li>
</ul>
<p>This is the terraform output and the steps to run on the 2 server nodes to have a running GlusterFS replicated volume; to configure the NFS export on server1, I used <!-- raw HTML omitted -->this guide<!-- raw HTML omitted -->.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>On the client I mount the EFS target with NFS4.1, the GlusterFS volume from the server <em>in the same subnet</em> via the GlusterFS native client_ _and the NFS export on the client’s designated mount points. I use the server on the same subnet as the client is, because the EFS target exposes a mount point in the same subnet and latency is a key factor in remote file system.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h5 id="benchmark-tests">Benchmark Tests</h5>
<p>I used <!-- raw HTML omitted -->fio<!-- raw HTML omitted --> installed on the client box with a command suggested by this <!-- raw HTML omitted -->BinaryLane post<!-- raw HTML omitted --> and run it against the mount point for EFS, GlusterFS and NFS v4.0 with the following command</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>changing the target directory each time I tun the test; each test is run isolated.</p>
<p>I did not customize any storage option for GlusterFS or NFS, so I’m using the default options.</p>
<h5 id="benchmarkresults">Benchmark Results</h5>
<h6 id="efs">EFS</h6>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h6 id="glusterfs">GlusterFS</h6>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h6 id="nfs-40-not-replicated">NFS 4.0 (not replicated)</h6>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h6 id="pricing-considerations">Pricing considerations</h6>
<p>EFS pricing is linear, you get billed a fixed amount for GB/month; this is not true with a self-managed cluster where you can surely reach higher performance, but the TCO is increasing every time you add capacity. If you’re not satisfied with EFS throughput you need a dedicated team to  manage a distributed file system cluster and its operations and maintenance.</p>
<h6 id="conclusions">Conclusions</h6>
<p>If you need a stable, realiable file system in AWS to be shared between EC2 instances, go and use EFS and don’t reinvent the wheel: GlusterFS is outperformed in both read and write IOPS and bandwith with the default options! The workload simulated here is showing poor write performance, so if your use case is a lot of concurrent writes on many files, consider another solution. A good workload could be a WORM (Write Once Read Many) share for permanent stored contents (images, archives?).</p>
<p>The performance are still low in absolute terms or compared to a non-replicated mount such as a stock NFS v4.0 server without the high-availability burden, but if you don’t care about 100% uptime you should definitely set up NFS with <!-- raw HTML omitted -->DRDB<!-- raw HTML omitted --> to a secondary node, and switch your mounts when the primary node fails. But still: why manage all of this if AWS can do it for you?</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2016-04-06-aws-iam-policy-to-let-users-manage-their-own-mfa/">AWS IAM policy to let users manage their own MFA</a></h1>
  <p>If you’re an AWS administrator you know that managing web console security is pretty tough unless you know what you want and you know what you’re doing. So if what you want is let each AWS user manage their own MFA device configuration without you and force them to have MFA active to use the web console, here is your solution.</p>
<p><strong>TL;DR</strong></p>
<ul>
<li>Create one or more groups with your web users</li>
<li>Create a new policy using <!-- raw HTML omitted -->this JSON<!-- raw HTML omitted --></li>
<li>Attach the policy to the group(s)</li>
</ul>
<p><strong>How does it work?</strong></p>
<p>The policy has this logic:</p>
<ul>
<li>Allow basic operations on IAM without having MFA set up</li>
<li>Allow setup and management of MFA for own user in IAM (create, delete, resync device)</li>
<li>Deny every action on every resource when MFA is not setup</li>
<li>Allow user to access IAM without MFA – this is necessary to sub-segment the previous rule</li>
</ul>
<p>The magic lies in the use of ARN policy variables which is a poorly documented feature of IAM. Notice how in some case the statement makes use of <code>${aws:username}</code> to confine the action executed on the only user receiving the policy grants.</p>
<p>This IAM policy blocks every serice usage when MFA is not setup, and in conjunction with default IAM behavior will deny access on every action if not explicitly given. You should combine this “base” policy with other group/service oriented policies to confine web users on certain functionalities. For example if you want a set of users self-managing their own MFA and access the EC2 service only after having setup MFA, you should execute the following after having setup the IAMUsersMFAManagement policy.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>``Reference: <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html">AWS IAM variables documentation</a></p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2016-01-24-implement-a-generic-data-list-structure/">Implement a generic data list structure</a></h1>
  <p>As a coding challenge I was asked to provide a generic list implementation using a language of my choice and using only primitive types, avoiding the use of high level built-ins. I chose <!-- raw HTML omitted -->Go<!-- raw HTML omitted --> because I want to learn it and I know it can be useful to create an abstract, generic implementation.</p>
<p>The challenge request to implement at least 4 methods on the generic type:</p>
<ul>
<li>Filter() –  returns a subset of the List satisfying an operation</li>
<li>Map() – returns the List objects’ map</li>
<li>Reverse() – reverse the ordering of the List objects</li>
<li>FoldLeft() – join the objects from left to right using a join character</li>
</ul>
<p>As a bonus question I was asked to code unit tests for the aforementioned methods and give an explanation on how the implementation guarantees concurrent access on resources.</p>
<p>So here is my implementation: the type List has only one attribute, an array of type <code>interface{}</code></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Every type will be convertible to the <code>interface{}</code> type, but as Golang has strong types the conversion is not implicit and must be declared.</p>
<p>Reverse() will create a new array of <code>interface{}</code> to hold the reversed list</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Map() returns the List elements’ array, so it can be accessed as a whole</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>This two function types will help define a custom operation to be used in Filter() and FoldLeft(): <!-- raw HTML omitted -->functions are types<!-- raw HTML omitted --> in Go and this enable a great level of abstraction.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Filter() will use a filter function, without the need to define it (!), and return a portion of the List data array.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>FoldLeft() will use a fold function, again not yet defined, the return a single element made of the entire list.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>You can find all the code <!-- raw HTML omitted -->here<!-- raw HTML omitted -->, any comment is welcome on how to improve the abstraction or efficiency of the implementation.</p>
<p>The opportunity to dig into the language ability to abstract is a very helpful way to better understand the language itself, so this coding challenge was a great opportunity to learn a little bit more Go!</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2016-01-24-virtualize-an-old-windows-pc/">Virtualize an old Windows PC</a></h1>
  <p>A friend asked me if I was able to get back working a Windows 98 PC he had in his house; I have never done it so I said “sure I can!” just to have the opportunity to learn something new, and of course do a friend a favour.</p>
<p>My idea was to copy the whole PC and get it running on a virtual machine thus doing what I later discovered is called a “P2V” (Physical to Virtual); the result of which would have been a portable VM which I could then install in my friend laptop to have his old PC back. The idea was indeed good and I am writing this blog just after finishing the job in the hope I will save someone the headache I’ve had in the last 4 days to get this done.</p>
<p>My tools for this job are :</p>
<ul>
<li>an ATA/SATA/IDE to USB adapter (<!-- raw HTML omitted -->this<!-- raw HTML omitted --> exactly) to mount the old drive</li>
<li><!-- raw HTML omitted -->winImage<!-- raw HTML omitted -->: a fantastic freeware by Gilles Vollant (runs on Windows only)</li>
<li>Windows 98 ISO (get one <!-- raw HTML omitted -->here<!-- raw HTML omitted -->)</li>
<li>VMWare Workstation: to run winImage on a pre-installed Windows VM, configure and test the new Windows 98 VM</li>
</ul>
<p>Why VMWare? It is by my knowledge the most reliable and portable hypervisor, with a vast documentation and huge community support; I also had it already installed in my Fedora box running a virtual Windows 10. Don’t worry if you don’t have VMWare: VirtualBox or Virtual-PC will do the same… So let’s get it started!</p>
<p>Verify the drive is working connecting it to the USB adapter: connect the USB to your PC first, connect the power adapter to te disk then finally plug the power cable; if a green light is on and you hear the disk spinning, move ahead. If the disk has no vitals, consider calling a data recovery expert. If you’re on Windows the disk should be visibile in your drives: press the Windows logo and E key to open explorer and the drive should be visible and browseable. Otherwise check the Disk Utility to verify it has been recognized and why is not mounted. On Linux, use <code>fdisk -l</code> to know the device and <code>mount -t filesystemtype /dev/deviceid /your/mount/point</code> to mount on a folder specifying the filesystem type. To know the file system type use <code>file -sL /dev/deviceid</code> using the device identifier.</p>
<p>Now that the disk is operational I recommend to take a backup with a raw image: this will help if any trouble happens with the disk during the P2V process. On windows you can use ShadowCopy or Ghost or Acronis Image, it all depends on your budget. On Linux I will use</p>
<p><code>dd if=/dev/deviceid of=/my/backup/destination/disk.img</code></p>
<p>Now I feel more confident because any action can be reverted with the original disk image backup: the next step is to virtualize the disk and all of its content with WinImage. From the menu select “Convert physical to virtual”, choose the input drive, a destination folder and the type of virtual disk (.vmdk if you want to use VMWare); WinImage will ask if you want to make a backup of your drive and if you are paranoic like me, answer yes and save another backup. Depending on the disk size the convertion process may take about 30 minutes long, so relax and wait; once WinImage is done you will notice it because the disk will lower down the noise.</p>
<p>Open VMWare (or any other hypervisor you like) and create a new virtual machine: for Windows 98 I choose 1 vCPU with 512MB RAM; attach the virtual drive created with WinImage to the guest and start it up. If you’re lucky enough you should end up with a Windows boot screen and the operating system loading.</p>
<p>Now the fun part! As we took a raw image of the old disk the drivers the hypervisor will use won’t be recognized! So in my case I had to click through a lot of driver reinstall, but they were all already present. A couple of reboots and the Windows 98 system is back up and running. Don’t be discouraged by what seems an infinite loop of driver reinstall! Continue installing the missing drivers and you willl succeed! This is definitely the most important thing to do: never give up!</p>
<p> </p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2015-08-02-golang-message-queue-a-simple-tcp-message-bus/">Golang Message Queue: a simple TCP message bus</a></h1>
  <p><strong>[TL;DR]</strong></p>
<p>I wrote a Pub/Sub message queue in <!-- raw HTML omitted -->Go<!-- raw HTML omitted -->, branch “master” is stable but missing some interesting feature like distributed memory synchronization between nodes in a cluster and encryption. Code at</p>
<p><!-- raw HTML omitted --><a href="https://github.com/inge4pres/gmq">https://github.com/inge4pres/gmq</a><!-- raw HTML omitted --></p>
<p>Being a cloud system engineer, my work is to design and implement distributed systems: one of the key principles in designing such architectures is <em>decoupling</em>, which means ensuring the many parts composing the system are able to share informations and complete a sequence of operations without being tied together. You can read more about cloud architectures and decoupling here.</p>
<p>One of the most common scenario in a cloud application is a series of asynchronous operations executed by many nodes on different layers: for example a front end server tier receiving  files and a backend server tier doing analysis on them; a good practice is to have a message queue between the two serving as an orchestration component. Each web server node will post a message in the queue for every files received, each backend node will consume a message from the queue to complete his operations on the files. In this way the two tiers are independent one from each other: in case of backend failure or over-capacity, the web servers will keep receiving files and storing message in the queue. If the two operations where done synchronously, the backend failure would stop the whole system to work.</p>
<p>A lot of <em>off-the-shelf</em> message queue software is already available, but I felt like writing my own would give me a good point of view on system programming with Go, so I wrote it, and the result is pretty awesome too. In a few days I was able to have a configurable message queue storing messages in memory, on filesystem or database (MySQL); communication is based on JSON via TCP, and the server can be configured to support a maximum number of queues, a maximum message length and queue capacity: combination of the configured parameters will have performance effects on the single node.</p>
<p>The roadmap of “develoment” branch is:</p>
<ul>
<li>adding cluster mode</li>
<li>adding memory synchronization in cluster mode</li>
<li>adding encryption: TLS over TCP</li>
<li>adding client authentication</li>
</ul>
<p>As you may have guessed from the above list, security of GMQ is not implemented at the moment, be careful!</p>
<p>Feel free to try it out and give suggestions!</p>
<p>Cheers 😀</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2015-06-11-my-first-golang-web-project/">My first Golang web project is online</a></h1>
  <p>It is true: I fell in love with <!-- raw HTML omitted -->Go<!-- raw HTML omitted -->, not because I love Google and his products, but because it really fits my ideology of simplicity and power in a programming language. I started experimenting with the language and thank to his web-oriented approach I quickly came up with one of the simplest single task web application I could write: a URL shortener.</p>
<p>What is a URL shortener? It’s a service that will give you a short link for a long URL.</p>
<p>Why should I use it? A short URL is easier to remember and to copy and paste, it lets you write more on social media where characters are limited (Twitter).</p>
<p>Why writing one when there are plenty of them already? Other shortener have an expiration date on short links, <!-- raw HTML omitted -->4pr.es<!-- raw HTML omitted --> doesn’t!</p>
<p>Take a look at the <!-- raw HTML omitted -->code<!-- raw HTML omitted --> and you’ll see it is very simple: it takes a URL as text input filed of a form</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>generates a random string of 6 charachters, checking that the string is not in the database</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>and keep the URL – string association in the database for further redirection; once the random short URL is visited the client gets redirected to the origianl long URL!</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>So please try it and if you have any suggestion to increase performance or security please let me know!</p>
<p>Cheers</p>
<p> </p>
<p> </p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2015-03-11-glusterfs-is-it-suitable-for-me/">GlusterFS: is it suitable for me?</a></h1>
  <p>During the last years I’ve been experimenting with <!-- raw HTML omitted -->GlusterFS<!-- raw HTML omitted --> and his functionalities as distributed object store; a lot has changed in the software, overall since <!-- raw HTML omitted -->Red Hat acquired it<!-- raw HTML omitted -->. I have been using it and find it useful for many projects but not for others: what I love is the community oriented approach with a very responsive team and support for any kind of users (meaning from the 2 nodes web server to a RAID10 Infiniband cluster for high end storage).</p>
<p>My personal story with Gluster starts with a porting of a on-premise architecture in the cloud: moving an existing application to the cloud, instead of redesigning it from scratch, involves a lot of engineering to adapt the current system settings to a scalable infrastructure. Gluster comes handy when talking about scaling: the latest milestone has a very simple and efficient way of reconfiguring the underlying hardware, adding and removing nodes in the storage pool is as simple as inputting a couple of commands from any of the peers in the cluster.</p>
<p>If you’re unfamiliar with Gluster concepts (storage pool, peers, etc…) I suggest you RTFM on <!-- raw HTML omitted -->Gluster’s website<!-- raw HTML omitted -->; in this post I will detail a few points you won’t find on documentation and you should definetely know before starting to evaluate Gluster adoption.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>If you find Gluster is not suitable for your application, consider analizying  a different solution like <!-- raw HTML omitted -->DRBD<!-- raw HTML omitted -->: it may not be as cutting edge as Gluster or <!-- raw HTML omitted -->Ceph<!-- raw HTML omitted --> but may be the right solution for the job.</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2014-12-19-set-up-a-private-masterslave-dns-using-bind/">Set up a private master/slave DNS using BIND</a></h1>
  <p>One of the very basic need of any startup is setting up a LAN in the workspace and configuring the Internet most used service: DNS. Relying on a public DNS may give you full functionality towards WAN connectivity, but when you need to address some hosts inside your LAN it can be handy to use names instead of IPs (especially with IPv6).</p>
<p>Here’s a straight forward guide to get you started with your private DNS in a few minutes.</p>
<p>OS filesystem’s path and package management utility may vary with the flavour of your distro, here I use CentOS.</p>
<hr>
<p><strong>Requirements</strong></p>
<ul>
<li>
<p>a router with DHCP ad WAN connectivity already setup</p>
</li>
<li>
<p>2 CentOS 6.x servers (physical or virtual with due availability concerns), enable to network with each others</p>
</li>
<li>
<p>a desktop PC</p>
<hr>
</li>
</ul>
<p><strong>Some general info</strong></p>
<p>I use this data as example, change them to your needs</p>
<!-- raw HTML omitted -->
<hr>
<p>**Configuring the primary DNS server **</p>
<p>On server <code>centos1</code></p>
<p><code>[root@centos1 ~]# yum update &amp;&amp; yum -y install bind bind-libs bind-utils</code></p>
<p>The BIND daemon is now installed; the base dir for the service is <code>/var/named</code> and the configuration file is <code>/etc/named.conf</code> ; modify the configuration file with your favourite editor</p>
<p><code>[root@centos1 ~]# vim /etc/named.conf</code></p>
<p>In the <code>options</code> section adjust the settings to your LAN configurations, changing the example values</p>
<pre><code>options {
       listen-on port 53 { 10.20.30.40 };             # inet address of centos1
       listen-on-v6 port 53 { ::1; };                 # comment this out to use IPv4 only
       directory &quot;/var/named&quot;;
       recursion yes;
       allow-recursion { 10.20.30.0/24; };            # recursion only in LAN, change this with your subnet
       allow-transfer { localhost; 10.20.30.50; };    # enable zone transfers only to secondary DNS sever
       forwarders {
                  208.67.222.222;
                  208.67.220.220;
       };                                             # OpenDNS used here, Google 8.8.8.8, 8.8.4.4 can be used
       dump-file &quot;/var/named/data/cache_dump.db&quot;;
       statistics-file &quot;/var/named/data/named_stats.txt&quot;;
       memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;;
       allow-query { 10.20.30.0/24; };                # accept queries only from LAN, change this with your subnet
};
</code></pre>
<p>Now comment the <code>include</code> lines following the <code>options</code> section and comment out all the rest. The end of the file should be (modify the domain)</p>
<pre><code># zone &quot;.&quot; IN {
#       type hint;
#       file &quot;named.ca&quot;;
#};

#include &quot;/etc/named.rfc1912.zones&quot;;
#include &quot;/etc/named.root.key&quot;;

zone &quot;startup.me&quot; IN {
      type master;
      file &quot;/var/named/startup.me.zone&quot;;
      allow-update { none; };
};
</code></pre>
<p>The latter lines create the definition of the domain and specify that DNS records should be looked up in the <code>/var/named/startup.me.zone </code>file which we are about to write. If you’re unfamiliar with DNS records and basic concepts, have a look at <!-- raw HTML omitted -->this<!-- raw HTML omitted -->. Now create and edit a new file</p>
<p><code>[root@centos1 ~]# vim /var/named/startup.me.zone</code></p>
<p>Insert this</p>
<pre><code>$TTL 8H
@      IN      SOA      centos1.startup.me. root.startup.me. (
               1        ; serial
               1D       ; refresh
               1H       ; retry
               1W       ; expire
               1H )     ; minimum TTL
; Name servers
        IN      NS      centos1.startup.me.
        IN      NS      centos2.startup.me.
; Resolvers
@       IN      A       10.50.30.10  ; default IP for your domain root startup.me
centos1 IN      A       10.20.30.40  ; the primary DNS server centos1
centos2 IN      A       10.20.30.50  ; the secondary DNS server centos2
www     IN      A       10.20.30.20  ; a web server in your LAN
ftp     IN      A       10.20.30.30  ; an FTP server in your LAN
git     IN      A       10.20.30.5   ; a GIT server in your LAN
</code></pre>
<p>Be careful when copying the above snippet into the config file: indentation must be respected!</p>
<p>You have configured the primary DNS with some DNS A records; the provided settings are not for a heavy load DNS server, nor they should be used in networks wih frequent IP address change: the time-to-live settings are high, therefore any IP mapping change should be followed by a clients’ resolver cache flush, which may be inconvenient for most users.</p>
<p>Now it’s time to enable the service; verify files permission</p>
<p><code>[root@centos1 ~]# chown named /var/named/startup.me.zone</code></p>
<p>and start the service</p>
<p><code>[root@centos1 ~]# service named start</code></p>
<p>If you want to have the service enabled at boot</p>
<p><code>[root@centos1 ~]# chkconfig named on &amp;&amp; chkconfig save</code></p>
<p>To test your primary DNS server you should first be sure to use it; check the resolver settings</p>
<p><code>[root@centos1 ~]# vim /etc/resolv.conf</code></p>
<p>it should contain only one line</p>
<p><code>nameserver 10.20.30.40</code></p>
<p>Verify the DNS is working with</p>
<p><code>[root@centos1 ~]# dig www.startup.me</code></p>
<p>if the output contains “AUTHORITY SECTION” you’re done.</p>
<hr>
<p>**Configuring the secondary DNS server **</p>
<p>On server <code>centos2</code> install BIND as you did for <code>centos1</code>.</p>
<p><code>[root@centos2 ~]# yum update &amp;&amp; yum -y install bind bind-libs bind-utils</code></p>
<p>Secure-CoPy the configuration files from <code>centos1</code> to <code>centos2</code></p>
<p><code>[root@centos2 ~]# scp centos1.startup.me:/etc/named.conf /etc/named.conf</code></p>
<p><code>[root@centos2 ~]# scp centos1.startup.me:/var/named/startup.me.zone /var/named/startup.me.zone</code></p>
<p>Edit the <code>/etc/named.conf</code> adjusting the IPv4 address of <code>centos2</code> in the <code>options</code> section, change <code>10.20.30.40</code> with <code>10.20.30.50</code>, and at the end of the file modify the zone settings to have a secondary (slave) DNS; as usual, change the IP with your actual primary DNS IP.</p>
<pre><code>zone &quot;startup.me&quot; IN {
      type slave;
      masters { 10.20.30.40; }; 
      file &quot;/var/named/startup.me.zone&quot;;
};
</code></pre>
<p>Start the BIND daemon on <code>centos2</code> and enjoy!</p>
<p><code>[root@centos2 ~]# chown named /var/named/startup.me.zone</code></p>
<p><code>[root@centos2 ~]# service named start</code></p>
<p>There it is! You are ready to test it from your desktop.</p>
<p>In your router settings change the primary and secondary DNS servers for the DHCP server, renew all adresses and try browsing the domain with any software. So name, much fast, wow!</p>
<p><em>Note: having a script doing an rsync to your secondary DNS will ease the pain when the primary DNS server goes down.</em></p>
<p>Cheers 😀</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2014-12-11-happy-birthday-server/">Happy birthday server!</a></h1>
  <p>In an attempt to make someone happier I wrote a script to notify a server admin when his child has gone through a year of uptime 🙂</p>
<p>Platform suggestions accepted, enjoy!</p>
<p><!-- raw HTML omitted -->happy_bday_server script<!-- raw HTML omitted --></p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2014-10-15-a-smooth-migration-to-the-cloud/">A smooth migration to the cloud</a></h1>
  <p>The last weekend my colleagues and I had a nice time moving an existing application from a bare-metal infrastructure to AWS. I would like to share some of the focal points involved in such process, in case you’d go through it and would like to know:</p>
<ul>
<li><strong>don’t expect everything to work as usual</strong>: you are changing the underlying hardware, moving to a virtualized environment. You can test every single part of the application but infrastructural side effects may occur in a second time</li>
<li><strong>relying on the provider</strong>: consider well which functionalities should be delegated to the cloud provider (AWS, in this case, offers a lot) or should be managed internally; for example S3 is not a distributed filesystem, and in some cases an RDS instance won’t have the same performance as database installed on an EC2 instance</li>
<li><strong>test application compliance, not hardware failure</strong>: instead of focusing on stress tests, you should focus first on functionality tests to ensure every part of te application is behaving as expected; hardware failure are easily handled in the cloud, that’s the primary purpose of IaaS. What is not handled by the cloud is that the application’s features will work on it!</li>
<li><strong>use a checklist</strong>: this may seem obvious, but having a clear and well written to-do list with a time table and activities’ details will help you analyze if anything is missing or needs to be done in advance</li>
</ul>
<p>Aside of this technical considerations, having the support of your coworkers and managers it is what really makes the difference: it keeps you focused in every step and at the same time helps if any problem comes up. That’s why my boss decided to take several videos with his phone and produced a “movie”,  hope you like it 😀</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2013-11-11-backup-and-restore-crontabs/">Backup and restore crontabs</a></h1>
  <p>Here’s a thing I came up with: you’re administering a Linux system with 100 users circa and you’re moving to a new server, you can save crontabs  per user with this:</p>
<p><code>mkdir crontabz &amp;&amp; cd crontabz; for user in `cat /etc/cron.allow`; do crontab -l -u $user &gt; cron_$user; done</code></p>
<p>you will end up with a list of files <em>cron_xxx</em>, each one has the users’ cron. Hopefully your cron version will use the <em>/etc/cron.allow</em> file to control  user based access to crontab.</p>
<p>Now, once users in the new system are all in place you can copy the directory <em>crontabz</em> and then restore their cron with:</p>
<p><code>cd crontabz;  for file in `ls -m1`; do echo `basename $file`|sed -s 's/cron_//' &gt;&gt; temp ; done ; \ </code></p>
<p><code>for user in `cat temp`; do  cat cron_$user | crontab -u $user -; done;</code></p>
<p>Optionally you can include an “rm -f temp” at the end to delete the file used to store user names.</p>
<p>I put the script on github <!-- raw HTML omitted -->here<!-- raw HTML omitted -->,</p>
<p>Cheers</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2013-10-07-zabbix-a-powerful-yet-simple-monitoring-software/">Zabbix: a powerful yet simple monitoring software</a></h1>
  <p>It may come in mind to any IT system engineer to know what is the status of the network, server by server, instance by instance; it happened to me when I was given the responsibility to manage my company’s infrastructure and I was wondering which tool could have helped to do the job.</p>
<p>I chose Zabbix to monitor my infrastructure because:</p>
<ol>
<li>despite it’s a bit difficult to install (you need a PHP enabled web server, a database and a C compiler), you will benefit a very user-friendly web interface with lots of functionalities</li>
<li>native agents for major OS release are already complied: FreeBSD, Linux, Windows, etc… Compiling to other OS just requires a “configure &amp;&amp; make &amp;&amp; make install”</li>
<li>it offers many monitoring methods via a unique interface: you can group SNMP, JMX, HTTP monitoring in one shot</li>
<li>it has multi-step HTTP/HTTPS monitoring, simulating different browsers and clients</li>
<li>you can build nice infographics bundling all kind of monitored datas</li>
<li>you can manage users and roles to give access to the web interface at your company’s employees</li>
<li>you can build custom monitoring scripts to your needs</li>
</ol>
<p>Well let’s see some action now: I would like to post a short tutorial on how to build a custom script to monitor resources used by a Glassfish application server. You can use this methodology for other application servers or services.</p>
<p><strong>Requirements:</strong> you have installed Zabbix server, deployed an agent to a host, set up the necessary networking stuff</p>
<p>On the host to be monitored, you will have a directory where the agent configuration file is located (usually /usr/local/etc or C:\Zabbix)</p>
<p><strong>Step 1: enable custom parameters <strong>parsing</strong></strong></p>
<p>Edit the file zabbix_agent.conf or zabbix_agentd.conf (depending if you’re usgin the daemon or not) and uncommment/add the following line:</p>
<p><em>Include=/usr/local/etc/zabbix_agentd.conf.d/  <em>or</em> <em>Include=/usr/local/etc/zabbix_agent.conf.d/</em></em></p>
<p><strong>Step 2: write the script</strong></p>
<p>Create a file, name it as you please and insert the script you want to be executed by the agent: I needed a script that would inetract with Glassfish, so I used th following:</p>
<blockquote>
<p># Flexible parameter to grab global variables. On the frontend side, use keys like glassfish.status[server.jvm.heapsize-current].</p>
</blockquote>
<blockquote>
<p># Key syntax is glassfish.status[monitoring-key].</p>
</blockquote>
<blockquote>
<p>UserParameter=glassfish.status[*],/opt/glassfish/bin/asadmin get –user admin –passwordfile /opt/glassfish/bin/.pwd -m $1 | cut -d “=” -f 2 | tr -d ‘ ‘ | bc</p>
</blockquote>
<p>The script syntax is always UsrParameter=name.of.script[*] followed by the code to be executed. This one uses the Glassfish utility “<em>asadmin</em>” and a couple of shell commands to trim the string output and translate it into an integer value. You can see the arguments array can be retrieved using $ and index of argument. In this example you will call the script with one argument only (the monitoring data you want from Glassfish).</p>
<p><strong>Step 3: start harvesting datas!</strong></p>
<p>Once finished editing the script, go back to the Zabbix monitoring console and add an Item to the host you are monitoring. You will add the key as shown in the picture below:</p>
<p> </p>
<p><a href="https://inge.4pr.es/blog/wp-content/uploads/2013/10/zabbix_custom_script1.png"><!-- raw HTML omitted --></a></p>
<p> </p>
<p> </p>
<p>Then go back to the dashboard and verify that the script just created is returning datas as expected. In the section <em>Monitoring-&gt;Latest Data</em> check if the item is giving the expected values. In this exemple I chose to monitor the current heap size used by the server. One cool thing is: once the script is done you can call it with all the parameters Glassfish has, and then combine datas in an infograph like the following:</p>
<p> </p>
<p><a href="https://inge.4pr.es/blog/wp-content/uploads/2013/10/zabbix_graphs1.png"><!-- raw HTML omitted --></a></p>
<p> </p>
<p>Here I put together two Glassfish parameters (JVM upper bound and current heap used) and a system parameter (free memory).</p>
<p>To get a list of all parameters you can monitor via Glassfish <em>asadmin</em> command see Glassfish documentation <!-- raw HTML omitted -->here<!-- raw HTML omitted -->.</p>
<p>Cheers,</p>
<p>inge4pres</p>
<p> </p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2013-02-28-quando-si-dice-problem-solving/">Quando si dice problem solving</a></h1>
  <p>Ieri in ufficio mi è capitata una disgrazia, una rara occasione in cui non c’è capacità o competenza che possano aiutarti a risolvere il caso, serve solo fortuna: nel pomeriggio rientro dalla sigaretta e mi sento dire “non abbiamo più rete”.</p>
<p>L’azienda dove lavoro si occupa di fornire servizi IT, senza la connessione internet siamo un’auto senza motore, una penna senza inchiostro, inutili.</p>
<p>La linea di backup?</p>
<p>All’inizio incolpo subito il nostro ISP (Fastweb), che a discapito di una fibra 100Mb ci ha giocato in passato scherzi del genere; quando la verifica evidenzia che non è un loro problema, il panico.</p>
<p>Attiviamo la linea di backup: una WiFi 7Mb Telecom, penosa per servire il traffico che ci serve.</p>
<p>Con il mio fedele collega Dario iniziamo a cercare la fonte del guasto. Andiamo alla sala macchine e tentiamo di capire dove sia il problema: un cavo collegato male? No. Saltata la corrente su uno degli interruttori di rete? No. L’impresa delle pulizie ha distrutto lo switch?!? No. Il router è fritto!!</p>
<p>L’unico apparato di rete per cui non abbiamo sostituti, l’unica macchina fondamentale di cui non si può fare a meno è guasta: e non c’è porta secondaria, configurazione o altro che ci salvi! Vegno subito preso d’assalto dal DG, che tra l’altro mi ricorda che l’indomani è prevista una riunione in sede con dei clienti. Rabbrividisco al pensiero.</p>
<p>Il tutto rimane in stand-by fino alla telefonata del mio capo, a casa malato, a cui devo comunicare la triste notizia. Lo chiamo e la cosa non lo rende affatto felice, e mentre sto per darmi per vinto lancio uno sguardo alla borsa sotto la mia scrivania: c’è un router lì dentro, il vecchio Dlink che usavo per simulare le reti dei clienti! Sarà anche una schifezza ma è sempre un router! Mi precipito al pc attacco il giocattolo e lo configuro pari al vecchio (con le sue minori funzioni) per servire la nostra LAN, vado in sala macchine, attacco i due cavi e…miracolo! Il problema è risolto! Ed è bastata un’occhiata al momento giusto dove non avresti guardato mai! Che dire…CULO!</p>

</article>

<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2012-05-21-privacy-e-riservatezza-il-digital-divide/">Privacy e riservatezza: il digital divide</a></h1>
  <p>Ho sempre vissuto in città di provincia (MI) dove è abitudine frequentare una cerchia ristretta di persone. Questo mi ha permesso di stringere legami molto forti con chi considero i miei amici, ma allo stesso tempo non mi ha favorito nello sviluppo di una  sensibilità verso la gestione delle informazioni personali nella vita reale; la mia lingua lunga ha solo fatto piovere sul bagnato…</p>
<blockquote>
<p>Il paese è piccolo e la gente mormora…</p>
</blockquote>
<p>recita il proverbio: ho constatato che è vero in molte occasioni.</p>
<p>Non tutti i  mali vengono per nuocere: nella mia vita digitale ho saputo imparare dagli errori commessi nela vita parallela e ho sempre dato un occhio di riguardo alla privacy online e tutto ciò che consegue allo scambio di informazioni tramite internet. Anche internet, infatti, con le sue milioni di miliardi di connessioni digitali può essere considerato un paesino di provincia, dove tutti possono venire a sapere tutto.</p>
<p>Consideriamo un esempio: Alice e Roberto (sempre chiamati in causa in questo <!-- raw HTML omitted -->genere di esempi<!-- raw HTML omitted -->), hanno una relazione che vogliono mantenere segreta. I due non si incontrano mai in pubblico, ma volentieri si scambiano messaggi passionali attraverso il telefonino e il computer. Un giorno Alice sta chattando con Roberto via Facebook quando per sbaglio il mouse le cade e viene cliccato questo <!-- raw HTML omitted -->link<!-- raw HTML omitted -->. Alice capita sulla pagina che descrive le regole con cui Facebook registra e archivia le informazioni a lei correlate e incuriosita legge interamente il contenuto. Scopre essenzialmente che:</p>
<ul>
<li>Facebook registra qualsiasi attività dell’utente svolta all’interno del sito (visite di profili, like, post <em>cancellati</em>, chat)</li>
<li>queste informazioni sono usate a fini statistici e sono condivise con altre aziende (pubblcitarie, di applicazioni, etc…)</li>
<li>è possibile richiedere una copia dei propri dati riempiendo un modulo e ricevenrli su un CD/DVD</li>
</ul>
<p>L’indomani Roberto torna da un viaggio di lavoro e chiede ad Alice di incontrarsi; la avvisa inoltre che il suo telefonino gli è stato rubato in aereoporto. In quel momento Alice è fulminata da un pensiero: e se il ladro del cellulare fosse a conoscenza del modulo per ricevere i dati da Facebook e usasse il telefono di Roberto per farne richiesta? A quel punto avrebbe a disposizione tutte le conversazioni tra i due e potrebbe approfittarne per ricattarli! Non vorrei essere al posto di Roberto quando si incontreranno…</p>
<p>Generalizzando dall’esempio, ogni compagnia che opera online ha il potere di acquisire dei dati: dalla sola mail e password in uso anni fa, al web 2.0 di oggi basato sui contenuti generati dagli utenti. Ogni compagnia si deve dotare quindi di politiche di detenzione e divulgazione di queste informazioni; vorrei che questo concetto fosse più ampiamente diffuso tra chi fa uso di questi servizi.</p>
<p>Non è raro infatti considerare più sicuro e riservato scambiarsi informazioni via internet che a parole; se domando “Ci sentiamo per organizzare la festa a sorpesa di X?” mi sento rispondere “Creo un gruppo su Facebook” e rimango perplesso: come si può pensare che sia meno rintracciabile o più segreta una serie di caratteri leggibili e che viaggiano andata e ritorno per l’oceano Atlantico, rispetto alle parole sussurrate direttamente all’orecchio di chi deve sentirle?</p>

</article>
 
    </main><footer class="w-full text-center p-4 text-xs text-gray-400">
  <p>
    Built with
    <a
      href="https://gohugo.io"
      target="_blank"
      rel="noopener noreferrer"
      class="underline hover:text-blue-800 dark:hover:text-blue-300"
      >Hugo</a
    >
    and
    <a
      href="https://github.com/apvarun/showfolio-hugo-theme"
      target="_blank"
      rel="noopener noreferrer"
      class="underline hover:text-blue-800 dark:hover:text-blue-300"
      >Showfolio</a
    >
  </p>
</footer>

</body>
</html>
