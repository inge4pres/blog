[{"content":"","href":"/categories/","title":"Categories"},{"content":" I\u0026rsquo;m in the train back from GoLab 2018 and I am so happy that I attended this conference! It\u0026rsquo;s been definitely one of most beautiful con I have attended in Italy, with tremendous speakers from all over the globe like Filippo Valsorda, Eleanor McHugh, Ron Evans and Bill Kennedy among many others; I have to say the organizers were just perfect in everything from the venue setup to the workshop organization, as if the quality of the talks ware not enough. I was so delighted I want to write a wrap-up immediately with my head full of ideas for the upcoming future.\nSpeakers layout It was 2 dense days with 45 minutes talks and 4 tracks, 2 each day: Patterns, Embedded (day 1); Web, DX (day 2). Before the talks a keynote each day (Eleanor McHugh and Bill Kennedy respectively), 30 minutes for lightning talks at the end of day 1, cocktails and networking at the end of day 2.\nThe community Poeple in the Gophers community is just awesome, and the environment was welcoming and warm in every part of the con; in the afternoon of day 1 a panel on diversity and inclusion was hoste by Cassandra Salisbury and it was really good in the sense that was not the regular D\u0026amp;I talk with practices and models to adopt to \u0026ldquo;pretend\u0026rdquo; that you are inclusive, it was actually a discussion and sharing of stories that enable a good community. Because a good community is inclusive by design, and the Go community is very good. During breaks I had the chance to shake hands and chat with many people I only had virtually met on Twitter before.\nThings I learnt Here\u0026rsquo;s some of the thing I got to know thanks to the amazing talks I saw:\n CERN uses Go too! They rebuild their DNS service, see cernops/golbd Google created a project called Flutter to build native mobile apps for multiple platforms [thanks @edoardo849] the go runtime is powerful but also can be tricky in some occasions, especially with closures; when using them =, make sure to clean up the resources they use and ensure local variable are scoped correctly to avoid data races and bugs. Always run tests with -race option and use channels and mutexes to orchestrate your pipelines [thanks @empijei] finite state machines are not only academic lecture, you can make them solve actual problems like decoding different variants of base64 [thanks @annaopss] there are 2 packages for auto-generating Go structs code from an arbitrary input in JSON and XML format: JSONGen and XMLGen because Go is written in Go, you can parse .go files before they are compiled and create your own language extensions (macro) to be then re-compiled in a final binary, crazy! [thanks Max Ghilardi, see cosmos72/gomacro] fnProject is a serverless platform to run function-as-a-service on Oracle cloud and on your private cloud on Kubernetes, as I covered it in this previous post you can manipulate network packets directly with Go! The Linux kernel has a feature to let programs in userspace filter packets based on custom logic, so Telefonica develop (and open sourced) a Go library to manipulate packets because C++ with libevent hadn\u0026rsquo;t enough throughput, so they moved from connection-based filtering to packet-based filtering (DPI) doubling throughput with 8 times less CPU! [See telefonica/nfqueue] you can close a buffered channel immediately after sending all the items into it, and you\u0026rsquo;ll still be able to read from it all the values; only then the close(channel) instruction will be magically executed by the runtime. So this is valid [thanks @goinggodotnet] instrumenting and monitoring applications in Kubernetes can be fun! When building an operator you can interact with the exported metrics directly in the controller, you can enrich your systems with distributed traces thanks to OpenTracing/OpenCensus using gRPC you can auto-generate Swagger documentation from protobuf service definitions, thus auto-generating web-browseable documentation of you service [thanks @pawel_slomka]\n io.Copy() from the standard library is the most efficient way of sending network data from 2 net.Conn instances; this thanks to interface upgrade in the Copy() implementation, ending in leveraging the kernel packet passing in the most efficient way - without involving the application at all! [thanks @filosottile]  And these are just a few (IMO the most important) things I got to know and discover these 2 days\u0026hellip; What a ride! Can\u0026rsquo;t wait for next year to be back in Florence!\nGoLab 2018 \n","href":"/2018/10/23/golab-2018-wrap-up/","title":"GoLab 2018: Wrap Up"},{"content":"","href":"/","title":"INGE4PRES RATIONAL THOUGHTS"},{"content":"","href":"/post/","title":"Posts"},{"content":"","href":"/tags/","title":"Tags"},{"content":"","href":"/tags/conferences/","title":"conferences"},{"content":"","href":"/categories/culture/","title":"culture"},{"content":"","href":"/tags/golang/","title":"golang"},{"content":"","href":"/categories/tech/","title":"tech"},{"content":" Cloud-native: sounds attractive right? What does it even mean? Wikipedia has no page on it already so anyone can give its own definition\u0026hellip; Here\u0026rsquo;s mine:\nA Cloud-native application has only concern on the functionalities that it has to deliver as it is completely decoupled from the infrastructure it runs on\nSo how can software delivery be cloud-native? Isn\u0026rsquo;t software delivery supposed to \u0026ldquo;install\u0026rdquo; software onto some infrastructure? Well if your infrastructure provider is cloud-native, you can transitively deliver software on it in a cloud-native way (counts of cloud-native is over 9000, so stopping here)!\nRecently RedHat acquired CoreOS, bold move if you ask me. CoreOS since the M\u0026amp;A has been very quite until a week ago when the operator-framework was announced through a blog post; this is a huge step forward for everyone as this new toolkit will empower the average developer with the ability to run operators on Kubernetes and package their applications as extensions to the Kubernetes API.\nNever heard of Custom Resource Definitions? You\u0026rsquo;d better get on track as this will be driving the next-gen wave of applications that will run as part of the platform that delivers them, with the ability to automate their management and simplify dramatically their operations which will be tightly integrated with the cluster management itself.\nAnd as usual I\u0026rsquo;m eager to try out this new toy and see what can be done with it: I want to build a cloud-native software delivery application that will enable CI/CD jobs to be running inside the cluster and managed by the same API server! Using a CRD for the \u0026ldquo;Pipeline\u0026rdquo; kind I can control the build/test/release flow of my application and moreover monitor the whole thing with the same tools with which I will monitor my application.\nCreating CD³ I decided to start a new project called CD³ (cd kube): it will be a Continuous-* software that will run in Kubernetes and will be dedicated to deliver software through Kubernetes. I found Weaveworks did something similar with Flux and since I don\u0026rsquo;t want to reinvent the wheel I\u0026rsquo;ll just try and replicate some functionality of running a continer in an operator.\nFirst things first: I installed the operator-sdk and I have the binary in my $GOPATH/bin. Running\n$GOPATH/bin/operator-sdk new cdkube --api-version=delivery.inge.4pr.es/v1alpha1 --kind=Pipeline  I am resulting in an auto-generated project with some scaffold code, as in this commit. Next I add some details which I think are at the core of a pipeline: the repository with code and configurations, the image to build the software and what version/name give to the resulting application artifact. When adding new items to the Spec and Status of CRD I will modify the pkg/apis/delivery/v1alpha1/types.go file, then the guide suggests to run operator-sdk generate k8s to update the code, in fact the zz_generated_deepcopy.go is updated, but I don\u0026rsquo;t see the deploy/cr.yaml changed as it should so probably there\u0026rsquo;s a bug\u0026hellip; Moving forward I have my operator logic to be defined now: I add some check whether the building pod is succeded and build the operator\noperator-sdk build inge4pres/cdkube:v0.0.1 docker push inge4pres/cdkube:v0.0.1  My operator is built, pushed to dockerhub and ready to be kicked in a k8s cluster, so I fire up one in GKE\ngcloud beta container --project \u0026quot;inge4pres-gcp\u0026quot; clusters create \u0026quot;operator-framework-test\u0026quot; --zone \u0026quot;europe-west1-b\u0026quot; --machine-type \u0026quot;g1-small\u0026quot; --image-type \u0026quot;COS\u0026quot; --disk-size \u0026quot;25\u0026quot; gcloud container clusters get-credentials operator-framework-test --zone europe-west1-b --project inge4pres-gcp  and when it\u0026rsquo;s ready I can deploy the auto-generated resources to the cluster. An amazing result is that once the depoloyment is done I can create my CRD with the kubectl command and see my custom-type declared, running kubectl create -f deploy/crd.yaml I have a new object created in k8s\n➜ ~ kubectl get pipeline NAME AGE doing-nothing 16s ➜ ~ kubectl describe pipeline Name: doing-nothing Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: delivery.inge.4pr.es/v1alpha1 Kind: Pipeline Metadata: Cluster Name: Creation Timestamp: 2018-05-05T1831Z Deletion Grace Period Seconds: \u0026lt;nil\u0026gt; Deletion Timestamp: \u0026lt;nil\u0026gt; Initializers: \u0026lt;nil\u0026gt; Resource Version: 1528 Self Link: /apis/delivery.inge.4pr.es/v1alpha1/namespaces/default/pipelines/doing-nothing UID: 5160a990-508f-11e8-8f06-42010a8401f8 Spec: Spec: Build Arguments: building something... now I\u0026#39;m done Build Commands: echo Build Image: busybox Repo: http://github.com/inge4pres/just-a-test Target Name: tesApp Target Version: 0.1 Events: \u0026lt;none\u0026gt; ➜ ~ When I deploy my operator and create the custom resource applying the manifest for a pipeline, the operator picks it up and starts a container with the name of the pipeline\n➜ cdkube git:(master) ✗ kubectl apply -f deploy/cr.yaml ➜ cdkube git:(master) ✗ kubectl get po NAME READY STATUS RESTARTS AGE cdkube-57bcf65584-sbvd6 1/1 Running 0 12s ➜ cdkube git:(master) ✗ kubectl apply -f deploy/cr.yaml pipeline \u0026#34;doing-nothing\u0026#34; created ➜ cdkube git:(master) ✗ kubectl get po NAME READY STATUS RESTARTS AGE cdkube-57bcf65584-sbvd6 1/1 Running 0 29s testapp 0/1 ContainerCreating 0 2s ➜ cdkube git:(master) ✗ kubectl logs cdkube-57bcf65584-sbvd6 time=\u0026#34;2018-05-05T1859Z\u0026#34; level=info msg=\u0026#34;Go Version: go1.10.2\u0026#34; time=\u0026#34;2018-05-05T1859Z\u0026#34; level=info msg=\u0026#34;Go OS/Arch: linux/amd64\u0026#34; time=\u0026#34;2018-05-05T1859Z\u0026#34; level=info msg=\u0026#34;operator-sdk Version: 0.0.5+git\u0026#34; time=\u0026#34;2018-05-05T1859Z\u0026#34; level=info msg=\u0026#34;starting pipelines controller\u0026#34; time=\u0026#34;2018-05-05T1821Z\u0026#34; level=info msg=\u0026#34;build still running, status of builder container: Pending\u0026#34; ➜ cdkube git:(master) ✗ kubectl get po NAME READY STATUS RESTARTS AGE cdkube-57bcf65584-sbvd6 1/1 Running 0 49s testapp 0/1 Completed 2 22s and if I get the testapp logs I can see the pipeline execution\n➜ cdkube git:(master) ✗ kubectl logs testapp building something... now I\u0026#39;m done Bonus: if the custom resource is deleted with kubectl delete -f deploy/cr.yaml the pod testapp gets terminated automatically! I don\u0026rsquo;t know if this magic is done by Kubernetes or by the Operator Framework, but sure I love it as it will enable my resources to be managed with the k8s API just like any other.\nI still need to tune the logic of handling the container as I didn\u0026rsquo;t expect the restarts to happen, but hey this looks amazing! In a couple of hours I have an operator that can spawn containers when instructed to do so by custom manifests representing a pipeline!\nStay tuned 😎\n","href":"/2018/05/05/cloud-native-software-delivery/","title":"Cloud-native applications: Operator-Framework"},{"content":"","href":"/tags/continuous-delivery/","title":"continuous-delivery"},{"content":"","href":"/tags/platform/","title":"platform"},{"content":"","href":"/tags/software/","title":"software"},{"content":" Continuous Delivery should be a solved issue: the practice is well-defined and there is a plethora of tools implementing it with more or less peculiarities, but still many struggle implementing it. The dream of a perfect continuous deployment flow from the developer to the production environment with software quality gates based on automated tests is still alive in me, I tried and tried several times with multiple implementations on multiple platforms and never got to the point where I could say: \u0026ldquo;I\u0026rsquo;m done, this works exactly as I wanted\u0026rdquo;.\nSo I stumbled on drone and decided to give it a go: it\u0026rsquo;s an open-source project, written in Go and with a SaaS offering via their website. The concept I like is that every step of the build/deployment process runs through a container and this is very close to my idea of a modern CD tool, a platform where I can compose pipelines by chaining containers execution on a shared workspace. Love it already.\nInstalling the stack You can run the drone server and agent locally on your laptop with docker-compose as detailed here. Only issue is: for integrating with any of the big Git cloud provider (Github and Gitlab) you will need to expose your service to the internet, so I\u0026rsquo;ll use a local instance of gogs running in a docker container from the official image. All I need is in the docker-compose.yml file: I added just a couple of volumes directive compared to the original one and I am using the docker-for-mac internal hostname to resolve the bridge IP internally as detailed here. This is a lab setup and having a production-ready installation will require database setup and filesystem persistence, but I don\u0026rsquo;t have this requirement now. After a docker-compose up -d my stack is ready.\nInstalling the CLI As easy as following this guide. Once logged to the web UI I navigate to the ${DRONE_HOST}/account/token page where I can get a token to configure the CLI.\nAdding secrets There is a nice feature in drone: I can manage secrets directly from the command line and they can be scoped globally or be available only to one pipeline step (corresponding to an image). I will need to add Dockerhub username and password to the plugin/docker image to be able to push the image, so I add this 2 secrets with the following\ndrone secret add -repository=inge/goapp -image=plugins/docker -name=docker_username -value=inge4pres drone secret add -repository=inge/goapp -image=plugins/docker -name=docker_password -value=***************  A sample pipeline As many of the continuous delivery tools available on the market, drone uses a YAML configuration file in the root of the repository, so adding a .drone.yml hidden file is enough to start hooking every commit to the build system. I configured a 3 stages pipeline:\n test and build artifact publish the artifact deploy the application  It\u0026rsquo;s very simple to get started and the one-container-per-step architecture makes it trivial to glue together multiple steps. There is an implicit concept of shared workspace (configurable) that you can leverage to use Makefile and Dockerfile just as the build was happening on your local environment.\nSo I really recommend trying out drone and reporting some issues if you find any, for the time being I am very excited to have a CD product entirely written in Go - I think I will contribute to the project to have some enhancements available in the free version.\nBelow here some screenshots of the drone web UI and the pipeline resulting from the YAML config file: I will explore more complex workflows like promoted builds and gated builds - and build more in the tool if I need.\n  drone repository view     drone pipeline log     drone stage error     drone running   ","href":"/2018/02/25/continuous-delivery-with-drone/","title":"Continuous Delivery with Drone"},{"content":"","href":"/tags/containers/","title":"containers"},{"content":"","href":"/tags/devops/","title":"devops"},{"content":"","href":"/tags/docker/","title":"docker"},{"content":" Kubernetes is the de facto platform for running modern applications: its broad adoption in 2017 and the velocity of the project made it so and it\u0026rsquo;s been accepted as the standard for many companies, from small to planet scale. It was impossible that such an extensible platform would be left out the serverless party, so here are the 4 main players offering FaaS to be run via k8s.\nA premise If you\u0026rsquo;re new to serverless and FaaS and all the previous buzzwords sound like cacophony to your ears, I really recommend reading this post and watching this talk. You could also notice how I put FaaS and serverless under the same hat here, this is just a personal opinion although some might argue that FaaS is a subset of serverless: historically I approached the serverless world using AWS Lambda, and I really tied the idea of writing functions and let someone else manage the infrastructure to the serverless concept. Also Sam Newman gave a good talk on serverless that I really recommend watching.\nWhy serverless on k8s It seems like a natural evolution for distributed systems to be composed by smaller and smaller parts. When moving from SOA to microservices the size of the service was reduced to enable development of more fine-grained functionalities into smaller and more maintainable components; taken to the extreme, you can reduce a microservice to be dedicated to just one task or to be made of just one function, that\u0026rsquo;s where FaaS fits into. Kubernetes is a great activator for such modularity as it creates a very powerful abstraction over infrastructure, so when developing a function as a separate module of a distributed system you can scale both vertically and horizontally any building block, each one independently from another, or you could even let Kubernetes manage that (think Horizontal Pod Autoscaler).\nFour players offering FaaS on k8s  OpenFaaS Fission Kubeless Fn Project  Now there might be others, but this 4 are the ones I mostly heard of in the last 6 months, so they must be the right ones 😁.\nComparison criteria This is not a technical benchmark on the capabilities of this 4 frameworks: it\u0026rsquo;s a \u0026ldquo;look Ma, I can serverless on k8s\u0026rdquo; post where I try and highlight the pros and cons of adopting one or the other; the criteria will be installation methodology (client and server), languages support, cluster interoperability and developer experience, voted from 0 to 5 the higher the better. I will use Kubernetes 1.8.6 that is, at the moment of writing, the latest available stable version.\nThe target function to deploy will be a super-serious analytics and business intelligence tool that will read the incoming HTTP request body and save it in a JSON document alongside with a timestamp. The JSON will be stored on a REDIS using a random UUIDv4 as key. All the code that will be deployed as functions is in Github, while for installing the GCP cluster and REDIS I used the following\ngcloud beta container --project \u0026quot;${GCP_PROJECT}\u0026quot; clusters create \u0026quot;serverless-k8s\u0026quot; \\ --zone \u0026quot;europe-west2-c\u0026quot; --username \u0026quot;admin\u0026quot; --cluster-version \u0026quot;1.8.6-gke.0\u0026quot; \\ --machine-type \u0026quot;g1-small\u0026quot; --image-type \u0026quot;COS\u0026quot; --disk-size \u0026quot;50\u0026quot; --num-nodes \u0026quot;3\u0026quot; gcloud container clusters get-credentials serverless-k8s \\ --zone europe-west2-c --project \u0026quot;${GCP_PROJECT}\u0026quot; helm init helm install stable/redis  Fn Project Features  function configuration via YAML local development server via fn start and fn run uses DockerHub to store functions as containers web UI with function monitoring  Client installation: 4 The installation instructions are easy to read and execute, multiple platforms supported out of the box. User is required to set an environment variable with a DockerHub handle\nexport FN_REGISTRY=\u0026lt;DOCKERHUB_USERNAME\u0026gt;  Server installation: 3 A Helm chart is provided under fn-helm but it\u0026rsquo;s not immediately linked to the project\u0026rsquo;s page. The installation requires the user to export an environment variable with the command\nexport FN_API_URL=http://$(kubectl get svc --namespace default fn-release-fn-api -o jsonpath='{.status.loadBalancer.ingress[0].ip}'):80  Language support: 5 Built-in support for Java, Ruby, Go, Python. Runs any docker container as a function.\nCluster interoperability: 2 It requires a LoadBalancer resource, so you won\u0026rsquo;t be able to run it on minikube out of the box. It has MySQL and REDIS as dependency services and uses a DaemonSet for the API controller, which might impact node\u0026rsquo;s performance. No monitoring provided for the in-cluster components.\nDeveloper experience: 3 Very extensive CLI interface. Functions are pipes: they should read Stdin and write to Stdout; some environment variables are injected to the running code to detect request URL and other configurations. I was able to complete my function deployment in roughly 1 hour after digging the docs a while to find out how to add custom configurations to the functions via environment variables.\nOpenFaaS Features  sponsored by CNCF function grouping configuration via YAML (stack file) public function repository Web UI with function monitoring runs any docker container as a function  Client installation: 2 The CLI installation is straight-forward for Linux and Mac users but it\u0026rsquo;s not immediately available for Windows. I cannot find an easy way to set the cluster address to point the CLI to.\nServer installation: 1 Helm chart provided under faas-netes/helm but it\u0026rsquo;s failing the first time because of RBAC property not set and not rolling back, so I\u0026rsquo;m forced to delete and recreate the release. Even when installation is completed I cannot connect to the FaaS gateway as the service NodePort 31112, and the LoadBalancer creation errors out with\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply The Service \u0026quot;gateway\u0026quot; is invalid: spec.ports[0].nodePort: Invalid value: 31112: provided port is already allocated  Language support: 5 Built-in support for NodeJS, Ruby, Go, Python, C#, generic Dockerfile. Runs any docker container as a function.\nCluster interoperability: 4 OpenFaas provides Prometheus monitoring with alerting out of the box, plus the architecture is very lean: just 2 pods that serve the API gateway and the function runner. Each function runs as a Deployment object and therefore can be scaled independently.\nDeveloper experience: 2 After 2 hours trying to complete the setup on Kubernetes I\u0026rsquo;m still not able to run any function on my cluster; I run a port-froward to the OpenFaaS gateway kubectl port-forward gateway-pod-id 31112:8080 so I can run faas-cli deploy -f samples.yml --gateway http://127.0.0.1:31112 inside the just cloned faas-cli repository and see some function in action.\nFission Features  only 100ms function cold start (more on the topic here) natively built for Kubernetes  Client installation: 3 Guide suggests to download a binary distribution via curl and place the binary§ under /usr/local/bin, very straightforward for Linux and OSX. Windows support is via WSL or using a binary fission.exe with download link provided. Some environment variables need to be setup to point to the cluster, but the instructions are very well written.\nServer installation: 5 Also guide: a single helm command installs all the components in a dedicated namespace fission.\nLanguage support: 4 Built-in support for Linux binaries, Go, .NET, NodeJS, Perl, PHP 7, Python 3, Ruby as reported in the concepts section. Custom environment can be built and pushed to the cluster as containers.\nCluster interoperability: 2 No monitoring at all and no UI provided to verify the functions state of execution or list, CLI is the only source of truth I can get and it\u0026rsquo;s not easy to understand the architecture.\nDeveloper experience: 2 Setup is very straightforward but then the development looks cumbersome (at least for Go): environment variables cannot be set yet, so this means hard-coded values in the code to connect to external services. Plus logging and function debugging is really hard, after 1 hour digging in documentation and trying to understand a cryptic Internal server error (fission) message, I am not able to run my Go function, and it\u0026rsquo;s tough to tell why.\nKubeless Features  natively built for kubernetes web UI with function monitoring serverless framework plugin available  Client installation: 5 Binary distribution available for Linux, OSX and Windows in Github release page. No fuss, no hassle: download and run.\nServer installation: 3 Documentation warns to create a namespace kubeless and use that. Same release page offers 3 options to install: Kubernetes with or without RBAC and Openshift. Applying the YAML with kubectl apply -f kubeless-... gets the server part up and running, but if I\u0026rsquo;d like to install in a different namespace I would need to change the whole file. Providing a Helm chart is the standard Kubernetes packaging way so why not having one?\nLanguage support: 3 Currently only Python, NodeJS, Ruby and .NET Core are supported. Custom runtimes in the form of docker containers need to be built to run other languages, a feature in alpha which I\u0026rsquo;m forced to explore to run my Go app.\nCluster interoperability: 4 Monitoring provided via Prometheus integration; all backend services are run into dedicated namespace while functions are exposed using regular namespaces. It uses a StatefulSet to host Kafka and Zookeeper, they keep the functions state and there is a controller talking with Kubernetes API. I really like that it leverages Kubernetes-native primitives such as ConfigMaps and Secrets to manage function environment. It also uses a CustomResourceDefinition called functions so you can kubectl get functions -o yaml. What I don\u0026rsquo;t like instead is the use of the cluster\u0026rsquo;s etcd to store functions code when deploying from file.\nDeveloper experience: 5 Everything worked great even when running an experimental feature such as custom environment: I was able to inject configuration via environment variables, get the function logs either via kubeless CLI or kubectl and debug my way out of the configuration error I put into my first image. Second deploy I did I was able to call my function and I validated the results connecting to REDIS afterwards.\nWrapping up There are lots of people investing in serverless right now, almost as many as there are for Kubernetes; the integrations between the two will bring a new exciting technology scenario in the next years. To my experience building this post none of the previously listed framework is ready for production usage, to be honest most of them are not even ready for the average developer weekend project usage. You need to know Kubernetes quite a bit to troubleshoot issues happening during deployment and/or execution of your functions and this makes the whole serverless idea crumble, as you\u0026rsquo;ll be forced to dig into infrastructure details to have your code running.\nIf I\u0026rsquo;d have to bet on one of the project I tried so far, I\u0026rsquo;d do so on Kubeless; it\u0026rsquo;s been definitely the smoothest setup among all and the tight Kubernetes integration makes it a perfect candidate for community-driven growth. If you know any other framework that should be in this post please let me know it in the comments! I am always curious to see what\u0026rsquo;s around in all things serverless so don\u0026rsquo;t keep it for yourself!\n","href":"/2018/01/30/serverless-on-kubernetes/","title":"Serverless on Kubernetes"},{"content":"","href":"/tags/development/","title":"development"},{"content":"","href":"/categories/kubernetes/","title":"kubernetes"},{"content":"","href":"/tags/kubernetes/","title":"kubernetes"},{"content":"","href":"/categories/serverless/","title":"serverless"},{"content":"","href":"/tags/serverless/","title":"serverless"},{"content":" In the early days of Go the language was often tailored towards \u0026ldquo;system programming\u0026rdquo; due to its C-stlye syntax and ability to write high-performance applications. Few time after, Go adoption was starting to gain traction for distributed systems development and projects like etcd, docker and kubernetes revealed the power of the networking capabilities offered by the internals in the language. Along the way a lot of libraries have been built around the powerful primitives offered by Go but in my opinion there is not enough use literature around the Communicating Sequential Processes implementation available through channels and goroutines, they are not even widely used in the standard library. I\u0026rsquo;ll detail here some concurrency patterns that I found useful and hopefully they\u0026rsquo;ll be idiomatic enough to represent a good use case for you.\nA premise CSP it\u0026rsquo;s kind of a similar feature to threading but there are some differences; to know more on CSP I really recommend watching Rob Pike\u0026rsquo;s excellent talk on the topic.\nMy experience Personally it took me a while to find my way out of the issues I ran into when first using concurrency features in Go: they are definitely the most complicated part of using Go, which is on average simpler that any other language I tried. So for me, the biggest problem was to understand what it means to have a goroutine spawned and how to control its execution or get data out of it, so I put together a list of examples on how concurrent programs flow can be controlled with the primitives built in the language.\nChannel, channels, channels everywhere A channel in Go is a way to pass messages between functions and goroutines, the official definition from A Tour of Go is:\nChannels are a typed conduit through which you can send and receive values with the channel operator, \u0026lt;-\nSo what are they good for? They are actually not very helpful without goroutines: a goroutine is a lightweigth thread managed by the Go runtime (definition), think like a background process that can be spawned and does not need to be managed directly by you, I like the concept of \u0026ldquo;run and forget\u0026rdquo;. The easiest concurrency pattern available is thinking of a goroutine processing some data in the background and returning them through a channel to the main thread executing our code; this can be very powerful and scale well to multiple functions and channels.\nWaitGroups WaitGroups are part of the sync package from the Go standard lib: they are a way for waiting the execution of goroutines to end properly and ensure all the work done in the background is completed. WaitGroups are often used with defer to fill in the wait queue when the goroutine exits.\nSome examples For me the most difficult thing to understand when approaching concurrency was how to ensure all of my goroutines completed execution: to do this the easiest way is using WaitGroups as in waitgroup_test.go: wg.Add(1) adds one item in the wait queue and wg.Done() removes one item from it; using wg.Wait() in the main process makes the process wait until the wait group is emptied. If you run the tests with\ngo test -v . -race -run ^TestWaitGroup  you can see the execution time when using concurrency or not. Changing the value of ops variable in functions_test.go will make the tests process less or more items.\nWith channels there are more features and gotchas that need to be taken into account:\n a read from a closed channel returns the type\u0026rsquo;s zero-value a send to a closed channel will panic a read and a send alone to an unbuffered channel are blocking: they will generate a deadlock if there is not a corresponding send/read operation on the other side of the channel a send on a buffered channel will block when the buffer is full and no other read is happening on the other side  That being said, there are a couple of notable usages that I like to include in my concurrency-enabled Go software: the fan-out pattern where an input generates multiple goroutines that perform operations in the background and the output of the concurrent goroutines is fetched by a channel in the main thread. Another pattern is fan-in: multiple functions can return values to a channel as long as the type is consistent. Run tests with\ngo test -v . -race -run ^TestChannelBuffered  to see fan-out/fan-in patterns in action.\nAnother interesting feature is powered by the select statement: you can read from multiple channels in the same function and define behavior for any given channel message, it is another sample of fan-in pattern. Using select will block until one of the send/receive operation is available, the operation gets chosen randomly if multiple are available at the same time. select has a similar syntax to switch so case and default are the scenario selector. Running multiple channels lets you manage multiple types in a single point: run the test with\ngo test -v . -race -run ^TestMultipleChannelsSelect$  to check the execution of the multiple goroutines.\nConclusions My experience with Go concurrency primitives is still forming, I hope I can read and experiment more on the topic as it\u0026rsquo;s one of Go\u0026rsquo;s most powerful and at the same time less documented features! I\u0026rsquo;d really love to hear feedback from the read so if you get up to this point, take a step forward and leave a comment below, I\u0026rsquo;d really love to discuss.\nReferences  Worker pools Channels Concurrency made easy - Dave Cheney Share memory by communicating - Codewalk Go channels are bad and you should feel bad  ","href":"/2017/10/28/2017-10-28-golang-concurrency-pattern/","title":"Golang Concurrency Patterns"},{"content":"","href":"/page/","title":"Pages"},{"content":" Work experiences 9\u0026frasl;2017 - now() » DevOps Engineer @ lastminute.com group I am contributing mainly to the development of a CI/CD platform to enable developers to test, build and deploy software easily and with confidence; we are part of Platform team and we collaborate with SREs and DBAs to provide developers automated services to consume infrastructure. The end-user platform runs on Kubernetes and we leverage Jenkins and Docker to shorten the feedback loop and continuously improve product quality.\n11\u0026frasl;2016 - 8\u0026frasl;2017 » DevOps Tech Lead @ Accenture DVO Within a team of 7, I was to define a medium term strategy on monitoring tools and have it implemented. We aim to reduce the MTTR, improve the incident detection and the application performance analysis; we write plugins and APIs to make this tools interact seamlessly between each others and with other teams.\n11\u0026frasl;2015 - 10\u0026frasl;2016 » DevOps Engineer @ Accenture DVO Contributing to the operations for a Digital Video platform running on multiple cloud providers and serving 30 million end users worldwide. We aim at 0% downtime for our clients using high-availability techniques and continuous improvement practices. We develop our own monitoring, analytics and intelligence tool based on Splunk. We provide developers isolated test environments and manage continuous delivery pipelines with automated testing and deployment.\n6\u0026frasl;2010 - 10\u0026frasl;2015 » Systems Engineer @ Metel Working on an e-procurement platform and a proprietary EDI format to help businesses digitalize supply chain management. My job is to manage and maintain the IT infrastructure serving the core-business services both on-premise and on the Amazon AWS cloud platform. Daily activities span from infrastructure event monitoring to middleware and application deployment. In 2012 I started developing backend software with J2EE and in 2014 I introduced Go in the company as system language.\n1\u0026frasl;2014 - 9\u0026frasl;2014 » Technical writer @ miamammausalinux.org 8\u0026frasl;2009 - 3\u0026frasl;2014 » Audio Engineer @ freelance Working with audio equipment to achieve best audio quality for live events; I was audio designer for some bands based in Milan, working with them in studio recordings and during live sets.\nCertifications and education 2\u0026frasl;2017 AWS SysOps Administrator - Associate Level Amazon Web Services\n8\u0026frasl;2012 Oracle 11g Performance Tuning Oracle University Roma (trainer Alessandro Colonna)\n7\u0026frasl;2009 Bachelor Degree in Industrial Engineering Politecnico di Milano\n7\u0026frasl;2004 High School Degree Scientific Lyceum “Falcone e Borsellino”, Arese (MI)\nSkills Continuous Integration and Delivery. Infrastructure design, implementation and cost optimization. Cloud computing and distributed systems implementation and development. Monitoring and incident response. Backend software development. Collaborative Development. ChatOps.\n OS RHEL/CentOS, Fedora, Debian/Ubuntu, OS X\n Cloud AWS, DigitalOcean, Azure, GCP, AKAMAI, Kubernetes\n DB MySQL, Oracle 11gR2, MongoDB, REDIS\n Lang Go, Java, Python, BASH, PL/SQL\n Product HTTPd, NGINX, Varnish, Jenkins, GOCD, ELK, Zabbix, Splunk, Ansible, Packer, Terraform, Consul, Docker\n  Interests and Hobbies I play drums quite well and bass guitar at a low level, I listen to lots of music and I am trying to expand my musical culture more and more, with discs from the 70s-80s and also contemporary artists. I love running, it makes me feel free, and enjoy swimming in pool and sea.\n","href":"/resume/","title":"RESUME"},{"content":" One of the advantages of containerized applications is the standardization, some would say \u0026ldquo;write it once, runs everywhere\u0026rdquo; but that\u0026rsquo;s another motto for another product. Anyway with a new packaging technology the same problems are faced: build reproducibility, or the necessity for people doing Ops to know they are going to deploy the same exact piece of code the Dev team used in their tests. So to address this issue the container image needs to be immutable: once it\u0026rsquo;s built, it\u0026rsquo;s not going to be changed, ever. And the same image will be used for testing, QA, beta, preview, presales-demo, whatever environment you need to deploy the app to.\nBuilding it Docker has been around for a few years now, it\u0026rsquo;s mature and stable, but ask anyone using it if they\u0026rsquo;d allow images built on development workstation to run in production: not gonna happen! The artifact that will serve production traffic will pass through the CI/CD pipelines and pushed to the registry, this is the way of shipping containers. \u0026ldquo;That\u0026rsquo;s easy\u0026rdquo;, you\u0026rsquo;ll say, \u0026ldquo;I\u0026rsquo;ll stick my Dockerfile in the repo and let the build system do the magic!\u0026rdquo;, but that\u0026rsquo;s not the whole picture: there are lower layers to pull, tests to be run and only after they succeed you can build the container. So who is going to maintain all of this configurations? Can we store them into the repo too? Yes! With GCB you can write a declarative multi-step workflow that will compile, test and package the code in a container; the container will get immediately pushed to the Google Container Registry too, so it\u0026rsquo;s ready to be consumed (maybe Kubernetes on GKE?).\nA simple application There is quite a good number of examples in the cloud builder repository but I\u0026rsquo;d like to create a fresh one with Golang: a random number generator. The app will serve a random integer via HTTP, and you can see the code is very straightforward. Now I want to build a container to run into GCP with confidence so that every time I make a build I will have a new container image tagged with the version and ready to roll it out.\nUsing GCB All builds in GCB happen in a container, right now the only engine supported is Docker but more are going to be added. You can leverage Google pre-baked builders or use your own images as builders, in the example I am using Google\u0026rsquo;s golang-project image to compile and docker to build the final docker image and push it to registry. Note how some environment variables are injected to the container, like PROJECT_ID is your GCP running project as configured via\ngcloud auth login gcloud config set project your-gcp-project  Side note for gophers The golang-project image does some checks at setup to determine the workspace structure: there need to be a ./src folder or GOPATH must be passed, or the simplest way is to insert a comment next to package main in main.go\nRunning the build It\u0026rsquo;s as easy as executing\ngcloud container builds submit --config cloudbuild.yaml .  in the root of your project.\nSee it in action!\n As you can see the current folder (. as last parameter) is compressed and shipped to a Cloud Storage random location, then GCB starts the steps listed in the configuration YAML file, running step by step the containers with their arguments.\nConclusion GCB is fast and very easy to use but for what I\u0026rsquo;ve been able to test is bound to GCP right now, so if you are willing to deliver a service from Google Cloud and your application is containerized or \u0026ldquo;cloud native\u0026rdquo; you have a lightweight build system ready to go, but if you have a private registry or other integrations to do, GCB might still be a too small niche.\nI\u0026rsquo;ll make more tests and try to hack a bit GCB in the future so stay tuned!\n","href":"/2017/10/01/2017-10-01-getting-started-with-google-cloud-builder/","title":"Getting Started With Google Cloud Builder"},{"content":"","href":"/tags/cicd/","title":"cicd"},{"content":"","href":"/tags/gcp/","title":"gcp"},{"content":" Recently I received a mail pointing me to a post about DevOps culture and some anti-patterns and misconception on how to build and grow a DevOps culture in a company. Whoever like me works in the Enterprise (\u0026ldquo;the one with the big E\u0026rdquo; - Kelsey Hightower) knows that applying DevOps practices often is limited to the adoption of some tools or the creation of a \u0026ldquo;DevOps team\u0026rdquo; responsible of managing some continuous delivery pipeline. I would like to share my personal experience and possibly explain what to show to your boss when he thinks they are doing DevOps right.\nLuckily for me I had the chance to work 5 years in a small company before DevOps was a thing: we were a small team and were forced to handle development and operations together, scaling our services in feature and size was becoming impossible without the use of some practices that a few years later we discovered was called \u0026ldquo;DevOps\u0026rdquo;. When I changed job for an Enterprise company I was happy because I thought I would go and find a better implementation of the same practices, with people trained better than me to handle complex build and deployment systems; I actually found that a deployment system was not there and I was hired to help build one. My delusion was high because there was nothing for me to learn in there, I was stepping back into 3 years of work done automating JVM delivery with Jenkins. At the very same time the higher management wanted desperately to implement a DevOps organization because studies was talking about how it could increase team productivity and reduce the time to market of features. So this decisions were made:\n a dedicated DevOps team was created to take care of the automation of the build/deployment process (at that time 100% manual) a set of tools was chosen by the management and dictated to be used for monitoring, configuration management, etc\u0026hellip; no change to the rest of organization was made: developers department and operations department kept the very same separate structure no change on how the software was developed: waterfall methodology was not removed  Then once the automation of the release was almost complete and some of the hot-sounding technologies on the market (Ansible, Packer, Terraform) were used by some team members the company declared that \u0026ldquo;through DevOps\u0026rdquo; they could be the best in breed. The DevOps button was pressed!\nThere is enough literature around (see bottom of page) to have a common understanding on what a DevOps culture is, how it originated and the practices that make it powerful for modern application delivery; I think most of Enterprise companies struggle impementing a DevOps model because the higher management do not understand what the DevOps transformation requires: a complete rethinking of people, processes and organization. The technological tools that end up implemented in successful organizations implementing DevOps are only a consequence of such change, not the fuel of the change itself. You cannot switch from SVN to Git, install Jenkins and Gradle and call yourself a DevOps company for doing that.\nThere is no silver bullet, there is no one-size-fits-all solution; DevOps is about empowering people to understand continuous improvement and the value of collaboration. There is so much confusion in the higher management of the Enterprise companies that just changing your job title into \u0026ldquo;DevOps Engineer\u0026rdquo; will grant you a salary increase (happened to me).\nTake your stand for a truly cohesive organization that put \u0026ldquo;people over processes over tools\u0026rdquo; and find the courage to change, experiment and fail. This is what DevOps is about. Or you can keep doing it wrong.\nSuggested reads Cloud System Adminstration Volume 2\nEffective DevOps\nThe Phoenix Project\n","href":"/2017/05/28/2017-05-28-devops-you-re-doing-it-wrong/","title":"DevOps: you're doing it wrong"},{"content":"","href":"/tags/culture/","title":"culture"},{"content":"","href":"/tags/enterprise/","title":"enterprise"},{"content":"","href":"/categories/social/","title":"social"},{"content":"","href":"/categories/work/","title":"work"},{"content":" Letsencrypt is cool: automated, free TLS certificates for everybody! They are sponsored mainly by internet corps and they started a crowd-funding campaign to avoid the influence of this corps in the future of the project. I recently moved the blog to hugo on AWS and I\u0026rsquo;m now porting the TLS management scripts I wrote a while ago on AWS: this is a nice exercise to give a proper TLS automation valid for everyone on AWS.\nI used Terraform, the AWS CLI and the amazing (although still in beta) lego; the idea is to spin up an EC2 instance every month to query Letsencrypt to renew a certificate I already provisioned; the instance will have permissions to update the certificate using AWS Certificate Manager and other services like API Gateway custom domain names and Cloudfront are already bound to use ACM certificate of the domain. I tried and make the Terraform code as abstract and reusable as possible: cloudfront distribution id, domain name and issuer mail are all variable configurable via config file or at command line.\nSetting up the environment I prepared the TLS certificate and key running first lego once on my local box, using the DNS challenge against AWS Route53, lego will use Letsencrypt ACME secret and put it into a TXT record for the domain to be validated so that Letsencrypt will know I am the owner of the domain. You can read more on Letsencrypt domain ownership validation here.\nAfter the lego account is created, generally in $HOME/.lego, you will find private key and certificate is created for the domain(s); the certificate is a bundle of all the SAN submitted in the request. Then how to automate all of this on AWS?\nDesigning automation The idea is that every service that needs TLS encryption will be able to fetch the certificate from an AWS service. This was not possible until AWS Certificate Manager was released a while ago: ACM will create or store a certificate to be used in other AWS services via the AWS API. No more magic to spread certificates via S3 or other tricks, you now have an awscli command! So here I chose to use Terraform to bootstrap all the infrastructure to run lego and run Letsencrypt automatically via AWS Autoscaling group.\nI first imported the certificate created with lego in ACM in us-east-1 region (N. Virginia): this is due to API Gateway limitations to be able to integrate natively with ACM only in us-east-1. I get the certificate ARN for the certificate just uploaded and I run terraform plan -var 'cf_distribution_id=EX4MPL3D15TR0' -var 'certificate_arn=...'. Once completed I can see a new Autoscaling group with scheduled policies, a launch configuration that starting from the base Amazon AMI with a user-data script at each boot will: * download and install lego binary * sync the TLS account with the S3 bucket created * split the certificate from the chain, as they need to be uploaded separately * call the acm subcommand of awscli to update the certificate\nThis is amazing, I can now forget about TLS management for all of my websites!\n","href":"/2016/12/03/2016-12-03-automate-tls-management-on-aws-with-letsencrypt/","title":"Automate TLS management on AWS with LetsEncrypt"},{"content":"","href":"/tags/aws/","title":"aws"},{"content":"","href":"/tags/letsencrypt/","title":"letsencrypt"},{"content":"","href":"/tags/tls/","title":"tls"},{"content":" Last month I felt I was a little late for the #serverless party going on all over the internet and I started taking a look at what the pros and cons would be to actually not manage any server myself. Shutting down my VPS hosting my apps I will loose my mail server, my MySQL instances and my Docker registry but: who cares? There are cloud services I can use with hundreds of times more availability and for a fraction of the cost.\nWhy #serverless? We are moving more and more rapidly to a developer friendly world: all cloud providers tend to relief companies from the burden of managing complex cluster architectures and it\u0026rsquo;s not a coincidence that things like SRE at Google exist. Systadmins are no longer required where they can be substituted with far more performance by declarative syntax clusters (Kubernetes, Docker Data Center) and reliabile, consistent configuration deployment (Ansible, Puppet) in managing high volumes of phisycal or virtual machines.\nThis shift to a developer-centric world forces who embraces it to \u0026ldquo;trust\u0026rdquo; the IaaS, PaaS and FaaS providers but in the same time let them focus on core and valuable development processes.\nThat\u0026rsquo;s why.\nTake the simplest app in the world: 4pres, a URL shortener. It needs a presentation, computation and data layer as most of apps. Traditionally and depending on your budget and needs, you would spin up one or more VPS and deploy software on them (containers or not, doesn\u0026rsquo;t matter here). The setup of nginx as reverse proxy to your application already requires skills that most of developers don\u0026rsquo;t have, but in first hand why should anybody have them when there is a service like AWS API Gateway that lets you deploy one in seconds? Having the possiility to do so, you may want to forget about everything not strictly related to your app, so focusing only on building and maintaining functionalities of your app.\nHow you can migrate your app today In terms of cloud provider I have a lot of experience with AWS therefore my first thought is for them when trying to do something like this: they probably already have enough mature services supporting what I need. Don\u0026rsquo;t take this as a sponsorship: you can do the same with any other provider.\nTraditional Architecture  NGINX reverse proxy Golang application MySQL database  NGINX terminates SSL and proxy back to app every request. The Golang app finds out what to do from a combination of HTTP Method and URL Path:\n GET / renders the landing page template GET /{url} queries the DB and redirect to long url or render 404 template POST / create a short link from form-data url=http://alongurul/ and display the result template  Serverless Architecture (AWS)  API Gateway expose a request/response mapping endpoint with integration to other services S3 to store and serve the static content Lambda executes Golang functions thanks to the amazing eawsy/aws-go-lambda framework DynamoDB stores data in schema-less fashion (JSON)  The API Gateway definition acts as proxy and:\n GET / serves static page index.html from S3 bucket with \u0026lsquo;Website Hosting\u0026rsquo; option enabled GET /{url} runs a Lamdba function 4pres_get that fetches the URL Path parameter from DynamoDB and redirect the client or renders a 404 template GET /s?{urlencodedURL} runs a Lambda function 4pres_post that creates a short URL and tries to store it in DynamoDB, returning the result template or the 500 template.  Not that big change in the overall design, but the code for the Golang app only shares a function to shorten the URL between the 2 implementations: that is understandable because we no longer manage HTTP requests attributes and delegate that to API Gateway, we don\u0026rsquo;t display static content anymore and leave that to S3. At the core we only have 2 things to worry about:\n store a URL (4pres_post) get URL to redirect the client  and then we can focus on extending new features:\n URL expiration user registration whatever!  Conclusion Thanks to this development model our craftsmen effort can be 100% dedicated to building features and forget about the rest: this can be a great advantage at large scale. Recently I have come across another interesting framework for Function as a Service called iron.io and I am willing to try it as soon as possible so stay tuned!\n","href":"/2016/11/17/2016-11-17-4pres-goes-serverless/","title":"4pres goes #serverless"},{"content":" Bio I\u0026rsquo;m a passionate technology enthusiast, an engineer in love with Computer Science and a part-time musician and writer; I started my career in IT as a sysadmin and became a Go developer in 2013 when approaching distributed systems. I like to study how things work, understand the mechanics of technology products. In my spare time I play drums, bass guitar and acoustic guitar, and sometimes try and write some music; writing small poems is also a passion of mine, as it helps me find a different meaning to things.\nBackground I attended Scientific High School, graduated in Industrial Engineering at Politecnico Milano and started working in Computer Science a few months later. I really feel thankful towards my college education because it introduced me to the real-world side of science; computing and mathematics above all, Open Source, curiosity and love for music are the foundation of my life. I also worked several years as self-taught audio engineer: I really enjoyed helping people find the best possible sound.\nGet in touch You can reach me via any of the social media icons below, share with me thoughts and interests. I am keen to discuss anything tech-related, new ideas on software development, cloud computing and CS in general. Please have a look at the blog: tech, politics and contemporary culture is what I write about.\n","href":"/about/","title":"ABOUT"},{"content":" In times of experimenting, I am now having a lot of fun with docker, rkt, kubernetes and containers ecosystem in general. But one thing I never forget to play with is content editing and publishing! So here I am, trying to migrate all my blog and website to Hugo :)\nSo instead of a bare VPS I am moving my blog to AWS S3 + Cloudfront CDN. This will be more scalable and far less expensive. And Hugo generates static HTML so no more patching security issues.\nHow I did it Migrate contents First I found that there is a page on Hugo site that explains how to migrate from any CMS know to human to Hugo. I used the wordpress export plugin to convert the site from Wordpress to markdown and make it work ith Hugo: it did the the job pretty well but still I had to convert some of the posts.\nHTTPS Then the hard part: HTTPS! I chose to use letsencrypt to automate the TLS certificate handling. The powerful lego is way more multi-platform than the official certbot from EFF written in Python, and has built-in support for DNS challenge on Route53! Generating a new certificate of renewing is a couple of commands away.\n","href":"/2016/10/10/2016-10-10-moving-the-blog-to-hugo/","title":"Moving the blog to hugo"},{"content":" Amazon Web Services Elastic File System has been to my knowledge the service to have the longest beta testing period: reason for this may be that not as many client as expected tested it and AWS received too few feedback on it or that there were issues not to release GA. I don\u0026#8217;t want to speculate on which one is correct but now that it has been officially released I decided to give it a try and of course compare it to a self-managed solution on the same platform.\nIf you followed AWS evolution you may agree that EFS has been introduced to fill the gap between EBS storage and S3: before EFS was live there was no \u0026#8220;easy\u0026#8221; way of having a distributed file system in AWS, you could only set up your own using a combination of EC2 instances mounting Elastic Block Storage volumes and S3. Now with EFS you can have a AWS-managed distributed file system to be used in your cloud environment or even across the internet (will try that on a public subnet) with all the benefits of offloading the high-availability and replication burden to Amazon, and at a reasonable price. Will performance be enough compared to a self-managed solution?\nPlayground I use terraform to create an infrastructure template to run the tests, you can see it here. Once\nterraform apply has finished, you\u0026#8217;ll end up with:\n An EFS with General Purpose performance mode An EFS mount target for 1 Availability Zone 1 EC2 instance named \u0026#8220;client\u0026#8221; to mount remote file systems 2 EC2 instances named \u0026#8220;server_X\u0026#8221; each one with a 10 GB General Purpose EBS, they will serve a self-managed distributed, replicated file system  This is the terraform output and the steps to run on the 2 server nodes to have a running GlusterFS replicated volume; to configure the NFS export on server1, I used this guide.\nApply complete! Resources: 15 added, 0 changed, 0 destroyed. The state of your infrastructure has been saved to the path below. This state is required to modify and destroy your infrastructure, so keep it safe. To inspect the complete state use the `terraform show` command. State path: terraform.tfstate Outputs: client-public-ip = ec2-54-89-124-238.compute-1.amazonaws.com efs-mount-target-a = us-east-1b.fs-a6418fef.efs.us-east-1.amazonaws.com server-a-ip = 172.30.10.122 server-b-ip = 172.30.11.31 ssh ec2-user@ec2-54-89-124-238.compute-1.amazonaws.com # ssh to first server, need agent forwarding setup ssh 172.30.10.122 sudo service glusterd restart # probe the node 2 sudo gluster peer probe 172.30.11.31 sudo gluster peer status ** Number of Peers: 1 Hostname: 172.30.11.31 Uuid: 237bc59a-20b6-4b30-b133-abab34e36720 State: Peer in Cluster (Connected) ** sudo gluster volume create efs-bench replica 2 transport tcp \\ 172.30.10.122:/export/gluster/brick 172.30.11.31:/export/gluster/brick force # force required to use the root disk as export brick ** volume create: efs-bench: success: please start the volume to access data sudo gluster volume start efs-bench ** volume start: efs-bench: success On the client I mount the EFS target with NFS4.1, the GlusterFS volume from the server in the same subnet via the GlusterFS native client_ _and the NFS export on the client\u0026#8217;s designated mount points. I use the server on the same subnet as the client is, because the EFS target exposes a mount point in the same subnet and latency is a key factor in remote file system.\nsudo mount -t nfs4 -o vers=4.1 \\ us-east-1b.fs-a6418fef.efs.us-east-1.amazonaws.com:/ /mnt/efs sudo mount -t glusterfs 172.30.10.122:/efs-bench /mnt/gluster sudo mount -t nfs 172.30.10.122:/export/nfs /mnt/nfs -o user=ec2-user Benchmark Tests I used fio installed on the client box with a command suggested by this BinaryLane post and run it against the mount point for EFS, GlusterFS and NFS v4.0 with the following command\nfio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test \\ --filename=test --bs=4k --iodepth=64 --size=1G --readwrite=randrw --rwmixread=75 changing the target directory each time I tun the test; each test is run isolated.\nI did not customize any storage option for GlusterFS or NFS, so I\u0026#8217;m using the default options.\nBenchmark Results EFS test: (g=0): rw=randrw, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64 fio-2.1.5 Starting 1 process test: Laying out IO file(s) (1 file(s) / 1024MB) Jobs: 1 (f=1): [m] [100.0% done] [3263KB/1078KB/0KB /s] [815/269/0 iops] [eta 00m:00s] test: (groupid=0, jobs=1): err= 0: pid=8336: Fri Aug 26 17:30:05 2016 read : io=784996KB, bw=3185.4KB/s, iops=796, runt=246443msec write: io=263580KB, bw=1069.6KB/s, iops=267, runt=246443msec cpu : usr=0.31%, sys=0.58%, ctx=383567, majf=0, minf=5 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, \u0026gt;=64=100.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, \u0026gt;=64=0.0% issued : total=r=196249/w=65895/d=0, short=r=0/w=0/d=0 latency : target=0, window=0, percentile=100.00%, depth=64 Run status group 0 (all jobs): READ: io=784996KB, aggrb=3185KB/s, minb=3185KB/s, maxb=3185KB/s, mint=246443msec, maxt=246443msec WRITE: io=263580KB, aggrb=1069KB/s, minb=1069KB/s, maxb=1069KB/s, mint=246443msec, maxt=246443msec GlusterFS test: (g=0): rw=randrw, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64 fio-2.1.5 Starting 1 process test: Laying out IO file(s) (1 file(s) / 1024MB) Jobs: 1 (f=1): [m] [100.0% done] [1648KB/632KB/0KB /s] [412/158/0 iops] [eta 00m:00s] test: (groupid=0, jobs=1): err= 0: pid=2857: Sun Sep 18 16:20:46 2016 read : io=784996KB, bw=1929.2KB/s, iops=482, runt=406921msec write: io=263580KB, bw=663288B/s, iops=161, runt=406921msec cpu : usr=0.32%, sys=0.39%, ctx=262172, majf=0, minf=5 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, \u0026gt;=64=100.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, \u0026gt;=64=0.0% issued : total=r=196249/w=65895/d=0, short=r=0/w=0/d=0 latency : target=0, window=0, percentile=100.00%, depth=64 Run status group 0 (all jobs): READ: io=784996KB, aggrb=1929KB/s, minb=1929KB/s, maxb=1929KB/s, mint=406921msec, maxt=406921msec WRITE: io=263580KB, aggrb=647KB/s, minb=647KB/s, maxb=647KB/s, mint=406921msec, maxt=406921msec NFS 4.0 (not replicated) test: (g=0): rw=randrw, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64 fio-2.1.5 Starting 1 process test: Laying out IO file(s) (1 file(s) / 1024MB) Jobs: 1 (f=1): [m] [100.0% done] [35088KB/11640KB/0KB /s] [8772/2910/0 iops] [eta 00m:00s] test: (groupid=0, jobs=1): err= 0: pid=23020: Sun Sep 18 16:57:17 2016 read : io=784996KB, bw=40713KB/s, iops=10178, runt= 19281msec write: io=263580KB, bw=13670KB/s, iops=3417, runt= 19281msec cpu : usr=3.67%, sys=10.23%, ctx=356147, majf=0, minf=5 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, \u0026gt;=64=100.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, \u0026gt;=64=0.0% issued : total=r=196249/w=65895/d=0, short=r=0/w=0/d=0 latency : target=0, window=0, percentile=100.00%, depth=64 Run status group 0 (all jobs): READ: io=784996KB, aggrb=40713KB/s, minb=40713KB/s, maxb=40713KB/s, mint=19281msec, maxt=19281msec WRITE: io=263580KB, aggrb=13670KB/s, minb=13670KB/s, maxb=13670KB/s, mint=19281msec, maxt=19281msec Pricing considerations EFS pricing is linear, you get billed a fixed amount for GB/month; this is not true with a self-managed cluster where you can surely reach higher performance, but the TCO is increasing every time you add capacity. If you\u0026#8217;re not satisfied with EFS throughput you need a dedicated team to manage a distributed file system cluster and its operations and maintenance.\nConclusions If you need a stable, realiable file system in AWS to be shared between EC2 instances, go and use EFS and don\u0026#8217;t reinvent the wheel: GlusterFS is outperformed in both read and write IOPS and bandwith with the default options! The workload simulated here is showing poor write performance, so if your use case is a lot of concurrent writes on many files, consider another solution. A good workload could be a WORM (Write Once Read Many) share for permanent stored contents (images, archives?).\nThe performance are still low in absolute terms or compared to a non-replicated mount such as a stock NFS v4.0 server without the high-availability burden, but if you don\u0026#8217;t care about 100% uptime you should definitely set up NFS with DRDB to a secondary node, and switch your mounts when the primary node fails. But still: why manage all of this if AWS can do it for you?\n","href":"/2016/09/18/2016-09-18-a-benchmark-of-aws-efs/","title":"A benchmark of AWS EFS"},{"content":"","href":"/tags/efs/","title":"EFS"},{"content":"","href":"/tags/benchmark/","title":"benchmark"},{"content":"","href":"/tags/cloud/","title":"cloud"},{"content":"","href":"/tags/filesystem/","title":"filesystem"},{"content":"If you\u0026#8217;re an AWS administrator you know that managing web console security is pretty tough unless you know what you want and you know what you\u0026#8217;re doing. So if what you want is let each AWS user manage their own MFA device configuration without you and force them to have MFA active to use the web console, here is your solution.\nTL;DR\n Create one or more groups with your web users Create a new policy using this JSON Attach the policy to the group(s)  How does it work?\nThe policy has this logic:\n Allow basic operations on IAM without having MFA set up Allow setup and management of MFA for own user in IAM (create, delete, resync device) Deny every action on every resource when MFA is not setup Allow user to access IAM without MFA \u0026#8211; this is necessary to sub-segment the previous rule  The magic lies in the use of ARN policy variables which is a poorly documented feature of IAM. Notice how in some case the statement makes use of ${aws:username} to confine the action executed on the only user receiving the policy grants.\nThis IAM policy blocks every serice usage when MFA is not setup, and in conjunction with default IAM behavior will deny access on every action if not explicitly given. You should combine this \u0026#8220;base\u0026#8221; policy with other group/service oriented policies to confine web users on certain functionalities. For example if you want a set of users self-managing their own MFA and access the EC2 service only after having setup MFA, you should execute the following after having setup the IAMUsersMFAManagement policy.\naws iam create-group --group-name ec2webgroup aws iam create-user --user-name ec2webuser aws iam add-user-to-group --group-name ec2webgroup --user-name ec2webuser aws iam attach-group-policy --policy-arn arn:aws:iam::AWSACCOUNTID:policy/IAMUsersMFAManagement --group-name ec2webgroup aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name ec2webgroup  ``Reference: AWS IAM variables documentation\n","href":"/2016/04/06/2016-04-06-aws-iam-policy-to-let-users-manage-their-own-mfa/","title":"AWS IAM policy to let users manage their own MFA"},{"content":"","href":"/tags/iam/","title":"iam"},{"content":"","href":"/tags/mfa/","title":"mfa"},{"content":"","href":"/tags/policy/","title":"policy"},{"content":"As a coding challenge I was asked to provide a generic list implementation using a language of my choice and using only primitive types, avoiding the use of high level built-ins. I chose Go because I want to learn it and I know it can be useful to create an abstract, generic implementation.\nThe challenge request to implement at least 4 methods on the generic type:\n Filter() \u0026#8211; returns a subset of the List satisfying an operation Map() \u0026#8211; returns the List objects\u0026#8217; map Reverse() \u0026#8211; reverse the ordering of the List objects FoldLeft() \u0026#8211; join the objects from left to right using a join character  As a bonus question I was asked to code unit tests for the aforementioned methods and give an explanation on how the implementation guarantees concurrent access on resources.\nSo here is my implementation: the type List has only one attribute, an array of type interface{}\ntype List struct { data []interface{} } Every type will be convertible to the interface{} type, but as Golang has strong types the conversion is not implicit and must be declared.\nReverse() will create a new array of interface{} to hold the reversed list\nfunc (m *List) Reverse() *List { var ret []interface{} for i := 1; i \u0026lt;= len(m.data); i++ { ret = append(ret, m.data[len(m.data)-i]) } return \u0026List{ data: ret, } } Map() returns the List elements\u0026#8217; array, so it can be accessed as a whole\nfunc (m *List) Map() []interface{} { return m.data }  This two function types will help define a custom operation to be used in Filter() and FoldLeft(): functions are types in Go and this enable a great level of abstraction.\ntype filterFn func(interface{}) interface{} type foldFn func([]interface{}) interface{}  Filter() will use a filter function, without the need to define it (!), and return a portion of the List data array.\nfunc (m *List) Filter(filter filterFn) *List { var ret []interface{} for d := range m.data { if data := filter(m.data[d]); data != nil { ret = append(ret, data) } } return \u0026List{ data: ret, } } FoldLeft() will use a fold function, again not yet defined, the return a single element made of the entire list.\nfunc (m *List) FoldLeft(fold foldFn) *List { var ret []interface{} ret = append(ret, fold(m.data)) return \u0026List{ data: ret, } } You can find all the code here, any comment is welcome on how to improve the abstraction or efficiency of the implementation.\nThe opportunity to dig into the language ability to abstract is a very helpful way to better understand the language itself, so this coding challenge was a great opportunity to learn a little bit more Go!\n","href":"/2016/01/24/2016-01-24-implement-a-generic-data-list-structure/","title":"Implement a generic data list structure"},{"content":"","href":"/tags/challenge/","title":"challenge"},{"content":"","href":"/tags/coding/","title":"coding"},{"content":"","href":"/tags/design/","title":"design"},{"content":"A friend asked me if I was able to get back working a Windows 98 PC he had in his house; I have never done it so I said \u0026#8220;sure I can!\u0026#8221; just to have the opportunity to learn something new, and of course do a friend a favour.\nMy idea was to copy the whole PC and get it running on a virtual machine thus doing what I later discovered is called a \u0026#8220;P2V\u0026#8221; (Physical to Virtual); the result of which would have been a portable VM which I could then install in my friend laptop to have his old PC back. The idea was indeed good and I am writing this blog just after finishing the job in the hope I will save someone the headache I\u0026#8217;ve had in the last 4 days to get this done.\nMy tools for this job are :\n an ATA/SATA/IDE to USB adapter (this exactly) to mount the old drive winImage: a fantastic freeware by Gilles Vollant (runs on Windows only) Windows 98 ISO (get one here) VMWare Workstation: to run winImage on a pre-installed Windows VM, configure and test the new Windows 98 VM  Why VMWare? It is by my knowledge the most reliable and portable hypervisor, with a vast documentation and huge community support; I also had it already installed in my Fedora box running a virtual Windows 10. Don\u0026#8217;t worry if you don\u0026#8217;t have VMWare: VirtualBox or Virtual-PC will do the same\u0026#8230; So let\u0026#8217;s get it started!\nVerify the drive is working connecting it to the USB adapter: connect the USB to your PC first, connect the power adapter to te disk then finally plug the power cable; if a green light is on and you hear the disk spinning, move ahead. If the disk has no vitals, consider calling a data recovery expert. If you\u0026#8217;re on Windows the disk should be visibile in your drives: press the Windows logo and E key to open explorer and the drive should be visible and browseable. Otherwise check the Disk Utility to verify it has been recognized and why is not mounted. On Linux, use fdisk -l to know the device and mount -t filesystemtype /dev/deviceid /your/mount/point to mount on a folder specifying the filesystem type. To know the file system type use file -sL /dev/deviceid using the device identifier.\nNow that the disk is operational I recommend to take a backup with a raw image: this will help if any trouble happens with the disk during the P2V process. On windows you can use ShadowCopy or Ghost or Acronis Image, it all depends on your budget. On Linux I will use\ndd if=/dev/deviceid of=/my/backup/destination/disk.img\nNow I feel more confident because any action can be reverted with the original disk image backup: the next step is to virtualize the disk and all of its content with WinImage. From the menu select \u0026#8220;Convert physical to virtual\u0026#8221;, choose the input drive, a destination folder and the type of virtual disk (.vmdk if you want to use VMWare); WinImage will ask if you want to make a backup of your drive and if you are paranoic like me, answer yes and save another backup. Depending on the disk size the convertion process may take about 30 minutes long, so relax and wait; once WinImage is done you will notice it because the disk will lower down the noise.\nOpen VMWare (or any other hypervisor you like) and create a new virtual machine: for Windows 98 I choose 1 vCPU with 512MB RAM; attach the virtual drive created with WinImage to the guest and start it up. If you\u0026#8217;re lucky enough you should end up with a Windows boot screen and the operating system loading.\nNow the fun part! As we took a raw image of the old disk the drivers the hypervisor will use won\u0026#8217;t be recognized! So in my case I had to click through a lot of driver reinstall, but they were all already present. A couple of reboots and the Windows 98 system is back up and running. Don\u0026#8217;t be discouraged by what seems an infinite loop of driver reinstall! Continue installing the missing drivers and you willl succeed! This is definitely the most important thing to do: never give up!\n\u0026nbsp;\n","href":"/2016/01/24/2016-01-24-virtualize-an-old-windows-pc/","title":"Virtualize an old Windows PC"},{"content":"","href":"/tags/geek/","title":"geek"},{"content":"","href":"/tags/guide/","title":"guide"},{"content":"","href":"/tags/oldies/","title":"oldies"},{"content":"","href":"/tags/virtualization/","title":"virtualization"},{"content":"","href":"/tags/windows/","title":"windows"},{"content":"[TL;DR]\nI wrote a Pub/Sub message queue in Go, branch \u0026#8220;master\u0026#8221; is stable but missing some interesting feature like distributed memory synchronization between nodes in a cluster and encryption. Code at\nhttps://github.com/inge4pres/gmq\nBeing a cloud system engineer, my work is to design and implement distributed systems: one of the key principles in designing such architectures is decoupling, which means ensuring the many parts composing the system are able to share informations and complete a sequence of operations without being tied together. You can read more about cloud architectures and decoupling here.\nOne of the most common scenario in a cloud application is a series of asynchronous operations executed by many nodes on different layers: for example a front end server tier receiving files and a backend server tier doing analysis on them; a good practice is to have a message queue between the two serving as an orchestration component. Each web server node will post a message in the queue for every files received, each backend node will consume a message from the queue to complete his operations on the files. In this way the two tiers are independent one from each other: in case of backend failure or over-capacity, the web servers will keep receiving files and storing message in the queue. If the two operations where done synchronously, the backend failure would stop the whole system to work.\nA lot of off-the-shelf message queue software is already available, but I felt like writing my own would give me a good point of view on system programming with Go, so I wrote it, and the result is pretty awesome too. In a few days I was able to have a configurable message queue storing messages in memory, on filesystem or database (MySQL); communication is based on JSON via TCP, and the server can be configured to support a maximum number of queues, a maximum message length and queue capacity: combination of the configured parameters will have performance effects on the single node.\nThe roadmap of \u0026#8220;develoment\u0026#8221; branch is:\n adding cluster mode adding memory synchronization in cluster mode adding encryption: TLS over TCP adding client authentication  As you may have guessed from the above list, security of GMQ is not implemented at the moment, be careful!\nFeel free to try it out and give suggestions!\nCheers 😀\n","href":"/2015/08/02/2015-08-02-golang-message-queue-a-simple-tcp-message-bus/","title":"Golang Message Queue: a simple TCP message bus"},{"content":"","href":"/tags/go/","title":"GO"},{"content":"It is true: I fell in love with Go, not because I love Google and his products, but because it really fits my ideology of simplicity and power in a programming language. I started experimenting with the language and thank to his web-oriented approach I quickly came up with one of the simplest single task web application I could write: a URL shortener.\nWhat is a URL shortener? It\u0026#8217;s a service that will give you a short link for a long URL.\nWhy should I use it? A short URL is easier to remember and to copy and paste, it lets you write more on social media where characters are limited (Twitter).\nWhy writing one when there are plenty of them already? Other shortener have an expiration date on short links, 4pr.es doesn\u0026#8217;t!\nTake a look at the code and you\u0026#8217;ll see it is very simple: it takes a URL as text input filed of a form\nshort, err := createUrl(req.FormValue(\"url\"))  generates a random string of 6 charachters, checking that the string is not in the database\n... for urlPresent(coder.Shrt) { coder.Shrt = shorten(coder.Length) } ... func shorten(c uint) string { rand.Seed(time.Now().UnixNano()) b := make([]rune, c) for i := range b { b[i] = letters[rand.Intn(len(letters))] } return string(b) } and keep the URL \u0026#8211; string association in the database for further redirection; once the random short URL is visited the client gets redirected to the origianl long URL!\n... err := getUrl(params[\"short\"], w, req) ... func getUrl(short string, w http.ResponseWriter, req *http.Request) error { var redir string err := db.QueryRow(\"SELECT url FROM urls WHERE short = ?\", short).Scan(\u0026redir) if err != nil { return err } http.Redirect(w, req, redir, 301) return nil } So please try it and if you have any suggestion to increase performance or security please let me know!\nCheers\n\u0026nbsp;\n\u0026nbsp;\n","href":"/2015/06/11/2015-06-11-my-first-golang-web-project/","title":"My first Golang web project is online"},{"content":"","href":"/tags/url/","title":"URL"},{"content":"","href":"/tags/shortener/","title":"shortener"},{"content":"During the last years I\u0026#8217;ve been experimenting with GlusterFS and his functionalities as distributed object store; a lot has changed in the software, overall since Red Hat acquired it. I have been using it and find it useful for many projects but not for others: what I love is the community oriented approach with a very responsive team and support for any kind of users (meaning from the 2 nodes web server to a RAID10 Infiniband cluster for high end storage).\nMy personal story with Gluster starts with a porting of a on-premise architecture in the cloud: moving an existing application to the cloud, instead of redesigning it from scratch, involves a lot of engineering to adapt the current system settings to a scalable infrastructure. Gluster comes handy when talking about scaling: the latest milestone has a very simple and efficient way of reconfiguring the underlying hardware, adding and removing nodes in the storage pool is as simple as inputting a couple of commands from any of the peers in the cluster.\nIf you\u0026#8217;re unfamiliar with Gluster concepts (storage pool, peers, etc\u0026#8230;) I suggest you RTFM on Gluster\u0026#8217;s website; in this post I will detail a few points you won\u0026#8217;t find on documentation and you should definetely know before starting to evaluate Gluster adoption.\nGluster is not a replacement for disaster recovery and backups \nIf you think that data redundancy mechanisms built in Gluster (replication and georeplication) are substitutes for backups you\u0026#8217;re doing it wrong: in Gluster there is no way of recovering data present only in failed drives or unavailable portions of the pool. There is no SPOF free implementation that will avoid you regular backups, unless you can tollerate loss of data. Gluster has limited configuration options \nGluster has been developed to \u0026#8220;take common hardware and turn it into scalable high performance storage solution\u0026#8221;. Gluster is great when availability and durability are performance indicators because it has been thinked for horizontal scalability, but scaling vertically in system resources will not have the desired outcome. There are phisycal thresholds in a node\u0026#8217;s configuration that make huge hardware resources useless (eg, limit to the number of CPU threads for the transaltor). Do consider the application scenario \nIf you are uncertain about Gluster capabilities, try it out yourself installing the software on at least two virtual machines and test if your application works well with the native FUSE module. As storage layer for I/O intensive applications Gluster is useful when average file size is bigger than the minimum size of the read cache (4MB). Currently the Gluster community is discussing how (relatively) small files should be handled in the next major release of milestone 3 (release 3.7) scheduled for the end of April 2015, but for now if your application scenario has lots of small files written frequently, Gluster may not be the right chioce. If you find Gluster is not suitable for your application, consider analizying a different solution like DRBD: it may not be as cutting edge as Gluster or Ceph but may be the right solution for the job.\n","href":"/2015/03/11/2015-03-11-glusterfs-is-it-suitable-for-me/","title":"GlusterFS: is it suitable for me?"},{"content":"","href":"/tags/gluster/","title":"gluster"},{"content":"","href":"/tags/storage/","title":"storage"},{"content":"","href":"/tags/bind/","title":"BIND"},{"content":"","href":"/tags/centos/","title":"CentOS"},{"content":"","href":"/tags/dns/","title":"DNS"},{"content":"","href":"/tags/linux/","title":"Linux"},{"content":"One of the very basic need of any startup is setting up a LAN in the workspace and configuring the Internet most used service: DNS. Relying on a public DNS may give you full functionality towards WAN connectivity, but when you need to address some hosts inside your LAN it can be handy to use names instead of IPs (especially with IPv6).\nHere\u0026#8217;s a straight forward guide to get you started with your private DNS in a few minutes.\nOS filesystem\u0026#8217;s path and package management utility may vary with the flavour of your distro, here I use CentOS.\nRequirements\n a router with DHCP ad WAN connectivity already setup 2 CentOS 6.x servers (physical or virtual with due availability concerns), enable to network with each others a desktop PC\n  Some general info\nI use this data as example, change them to your needs\nstartup.me the domain name you want to use in your LAN  * 10.20.30.0/24 the subnet of your LAN * 10.20.30.40 the static IP address assigned to CentOS server 1, with hostname centos1.startup.me * 10.20.30.50 the static IP address assigned to CentOS server 2, with hostname centos2.startup.me\nConfiguring the primary DNS server \nOn server centos1\n[root@centos1 ~]# yum update \u0026amp;\u0026amp; yum -y install bind bind-libs bind-utils\nThe BIND daemon is now installed; the base dir for the service is /var/named and the configuration file is /etc/named.conf ; modify the configuration file with your favourite editor\n[root@centos1 ~]# vim /etc/named.conf\nIn the options section adjust the settings to your LAN configurations, changing the example values\noptions { listen-on port 53 { 10.20.30.40 }; # inet address of centos1 listen-on-v6 port 53 { ::1; }; # comment this out to use IPv4 only directory \u0026quot;/var/named\u0026quot;; recursion yes; allow-recursion { 10.20.30.0/24; }; # recursion only in LAN, change this with your subnet allow-transfer { localhost; 10.20.30.50; }; # enable zone transfers only to secondary DNS sever forwarders { 208.67.222.222; 208.67.220.220; }; # OpenDNS used here, Google 8.8.8.8, 8.8.4.4 can be used dump-file \u0026quot;/var/named/data/cache_dump.db\u0026quot;; statistics-file \u0026quot;/var/named/data/named_stats.txt\u0026quot;; memstatistics-file \u0026quot;/var/named/data/named_mem_stats.txt\u0026quot;; allow-query { 10.20.30.0/24; }; # accept queries only from LAN, change this with your subnet };  Now comment the include lines following the options section and comment out all the rest. The end of the file should be (modify the domain)\n# zone \u0026quot;.\u0026quot; IN { # type hint; # file \u0026quot;named.ca\u0026quot;; #}; #include \u0026quot;/etc/named.rfc1912.zones\u0026quot;; #include \u0026quot;/etc/named.root.key\u0026quot;; zone \u0026quot;startup.me\u0026quot; IN { type master; file \u0026quot;/var/named/startup.me.zone\u0026quot;; allow-update { none; }; };  The latter lines create the definition of the domain and specify that DNS records should be looked up in the /var/named/startup.me.zone file which we are about to write. If you\u0026#8217;re unfamiliar with DNS records and basic concepts, have a look at this. Now create and edit a new file\n[root@centos1 ~]# vim /var/named/startup.me.zone\nInsert this\n$TTL 8H @ IN SOA centos1.startup.me. root.startup.me. ( 1 ; serial 1D ; refresh 1H ; retry 1W ; expire 1H ) ; minimum TTL ; Name servers IN NS centos1.startup.me. IN NS centos2.startup.me. ; Resolvers @ IN A 10.50.30.10 ; default IP for your domain root startup.me centos1 IN A 10.20.30.40 ; the primary DNS server centos1 centos2 IN A 10.20.30.50 ; the secondary DNS server centos2 www IN A 10.20.30.20 ; a web server in your LAN ftp IN A 10.20.30.30 ; an FTP server in your LAN git IN A 10.20.30.5 ; a GIT server in your LAN  Be careful when copying the above snippet into the config file: indentation must be respected!\nYou have configured the primary DNS with some DNS A records; the provided settings are not for a heavy load DNS server, nor they should be used in networks wih frequent IP address change: the time-to-live settings are high, therefore any IP mapping change should be followed by a clients\u0026#8217; resolver cache flush, which may be inconvenient for most users.\nNow it\u0026#8217;s time to enable the service; verify files permission\n[root@centos1 ~]# chown named /var/named/startup.me.zone\nand start the service\n[root@centos1 ~]# service named start\nIf you want to have the service enabled at boot\n[root@centos1 ~]# chkconfig named on \u0026amp;\u0026amp; chkconfig save\nTo test your primary DNS server you should first be sure to use it; check the resolver settings\n[root@centos1 ~]# vim /etc/resolv.conf\nit should contain only one line\nnameserver 10.20.30.40\nVerify the DNS is working with\n[root@centos1 ~]# dig www.startup.me\nif the output contains \u0026#8220;AUTHORITY SECTION\u0026#8221; you\u0026#8217;re done.\nConfiguring the secondary DNS server \nOn server centos2 install BIND as you did for centos1.\n[root@centos2 ~]# yum update \u0026amp;\u0026amp; yum -y install bind bind-libs bind-utils\nSecure-CoPy the configuration files from centos1 to centos2\n[root@centos2 ~]# scp centos1.startup.me:/etc/named.conf /etc/named.conf\n[root@centos2 ~]# scp centos1.startup.me:/var/named/startup.me.zone /var/named/startup.me.zone\nEdit the /etc/named.conf adjusting the IPv4 address of centos2 in the options section, change 10.20.30.40 with 10.20.30.50, and at the end of the file modify the zone settings to have a secondary (slave) DNS; as usual, change the IP with your actual primary DNS IP.\nzone \u0026quot;startup.me\u0026quot; IN { type slave; masters { 10.20.30.40; }; file \u0026quot;/var/named/startup.me.zone\u0026quot;; };  Start the BIND daemon on centos2 and enjoy!\n[root@centos2 ~]# chown named /var/named/startup.me.zone\n[root@centos2 ~]# service named start\nThere it is! You are ready to test it from your desktop.\nIn your router settings change the primary and secondary DNS servers for the DHCP server, renew all adresses and try browsing the domain with any software. So name, much fast, wow!\nNote: having a script doing an rsync to your secondary DNS will ease the pain when the primary DNS server goes down.\nCheers 😀\n","href":"/2014/12/19/2014-12-19-set-up-a-private-masterslave-dns-using-bind/","title":"Set up a private master/slave DNS using BIND"},{"content":"In an attempt to make someone happier I wrote a script to notify a server admin when his child has gone through a year of uptime 🙂\nPlatform suggestions accepted, enjoy!\nhappy_bday_server script\n","href":"/2014/12/11/2014-12-11-happy-birthday-server/","title":"Happy birthday server!"},{"content":"","href":"/tags/admin/","title":"admin"},{"content":"","href":"/tags/bash/","title":"bash"},{"content":"","href":"/tags/server/","title":"server"},{"content":"The last weekend my colleagues and I had a nice time moving an existing application from a bare-metal infrastructure to AWS. I would like to share some of the focal points involved in such process, in case you\u0026#8217;d go through it and would like to know:\n don\u0026#8217;t expect everything to work as usual: you are changing the underlying hardware, moving to a virtualized environment. You can test every single part of the application but infrastructural side effects may occur in a second time relying on the provider: consider well which functionalities should be delegated to the cloud provider (AWS, in this case, offers a lot) or should be managed internally; for example S3 is not a distributed filesystem, and in some cases an RDS instance won\u0026#8217;t have the same performance as database installed on an EC2 instance test application compliance, not hardware failure: instead of focusing on stress tests, you should focus first on functionality tests to ensure every part of te application is behaving as expected; hardware failure are easily handled in the cloud, that\u0026#8217;s the primary purpose of IaaS. What is not handled by the cloud is that the application\u0026#8217;s features will work on it! use a checklist: this may seem obvious, but having a clear and well written to-do list with a time table and activities\u0026#8217; details will help you analyze if anything is missing or needs to be done in advance  Aside of this technical considerations, having the support of your coworkers and managers it is what really makes the difference: it keeps you focused in every step and at the same time helps if any problem comes up. That\u0026#8217;s why my boss decided to take several videos with his phone and produced a \u0026#8220;movie\u0026#8221;, hope you like it 😀\n","href":"/2014/10/15/2014-10-15-a-smooth-migration-to-the-cloud/","title":"A smooth migration to the cloud"},{"content":"Here\u0026#8217;s a thing I came up with: you\u0026#8217;re administering a Linux system with 100 users circa and you\u0026#8217;re moving to a new server, you can save crontabs per user with this:\nmkdir crontabz \u0026amp;\u0026amp; cd crontabz; for user in `cat /etc/cron.allow`; do crontab -l -u $user \u0026gt; cron_$user; done\nyou will end up with a list of files _cronxxx, each one has the users\u0026#8217; cron. Hopefully your cron version will use the /etc/cron.allow file to control user based access to crontab.\nNow, once users in the new system are all in place you can copy the directory crontabz and then restore their cron with:\ncd crontabz; for file in `ls -m1`; do echo `basename $file`|sed -s 's/cron_//' \u0026gt;\u0026gt; temp ; done ; \\ \nfor user in `cat temp`; do cat cron_$user | crontab -u $user -; done;\nOptionally you can include an \u0026#8220;rm -f temp\u0026#8221; at the end to delete the file used to store user names.\nI put the script on github here,\nCheers\n","href":"/2013/11/11/2013-11-11-backup-and-restore-crontabs/","title":"Backup and restore crontabs"},{"content":"","href":"/tags/cron/","title":"cron"},{"content":"","href":"/tags/scripting/","title":"scripting"},{"content":"","href":"/tags/shell/","title":"shell"},{"content":"It may come in mind to any IT system engineer to know what is the status of the network, server by server, instance by instance; it happened to me when I was given the responsibility to manage my company\u0026#8217;s infrastructure and I was wondering which tool could have helped to do the job.\nI chose Zabbix to monitor my infrastructure because:\n despite it\u0026#8217;s a bit difficult to install (you need a PHP enabled web server, a database and a C compiler), you will benefit a very user-friendly web interface with lots of functionalities native agents for major OS release are already complied: FreeBSD, Linux, Windows, etc\u0026#8230; Compiling to other OS just requires a \u0026#8220;configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install\u0026#8221; it offers many monitoring methods via a unique interface: you can group SNMP, JMX, HTTP monitoring in one shot it has multi-step HTTP/HTTPS monitoring, simulating different browsers and clients you can build nice infographics bundling all kind of monitored datas you can manage users and roles to give access to the web interface at your company\u0026#8217;s employees you can build custom monitoring scripts to your needs  Well let\u0026#8217;s see some action now: I would like to post a short tutorial on how to build a custom script to monitor resources used by a Glassfish application server. You can use this methodology for other application servers or services.\nRequirements: you have installed Zabbix server, deployed an agent to a host, set up the necessary networking stuff\nOn the host to be monitored, you will have a directory where the agent configuration file is located (usually /usr/local/etc or C:\\Zabbix)\nStep 1: enable custom parameters parsing****\nEdit the file zabbix_agent.conf or zabbix_agentd.conf (depending if you\u0026#8217;re usgin the daemon or not) and uncommment/add the following line:\n_Include=/usr/local/etc/zabbix_agentd.conf.d/ _or_ _Include=/usr/local/etc/zabbix_agent.conf.d/__\nStep 2: write the script\nCreate a file, name it as you please and insert the script you want to be executed by the agent: I needed a script that would inetract with Glassfish, so I used th following:\n # Flexible parameter to grab global variables. On the frontend side, use keys like glassfish.status[server.jvm.heapsize-current].\n# Key syntax is glassfish.status[monitoring-key].\nUserParameter=glassfish.status[*],/opt/glassfish/bin/asadmin get \u0026#8211;user admin \u0026#8211;passwordfile /opt/glassfish/bin/.pwd -m $1 | cut -d \u0026#8220;=\u0026#8221; -f 2 | tr -d \u0026#8216; \u0026#8216; | bc\n The script syntax is always UsrParameter=name.of.script[*] followed by the code to be executed. This one uses the Glassfish utility \u0026#8220;asadmin\u0026#8221; and a couple of shell commands to trim the string output and translate it into an integer value. You can see the arguments array can be retrieved using $ and index of argument. In this example you will call the script with one argument only (the monitoring data you want from Glassfish).\nStep 3: start harvesting datas!\nOnce finished editing the script, go back to the Zabbix monitoring console and add an Item to the host you are monitoring. You will add the key as shown in the picture below:\n\u0026nbsp;\n\n\u0026nbsp;\n\u0026nbsp;\nThen go back to the dashboard and verify that the script just created is returning datas as expected. In the section Monitoring-\u0026gt;Latest Data check if the item is giving the expected values. In this exemple I chose to monitor the current heap size used by the server. One cool thing is: once the script is done you can call it with all the parameters Glassfish has, and then combine datas in an infograph like the following:\n\u0026nbsp;\n\n\u0026nbsp;\nHere I put together two Glassfish parameters (JVM upper bound and current heap used) and a system parameter (free memory).\nTo get a list of all parameters you can monitor via Glassfish asadmin command see Glassfish documentation here.\nCheers,\ninge4pres\n\u0026nbsp;\n","href":"/2013/10/07/2013-10-07-zabbix-a-powerful-yet-simple-monitoring-software/","title":"Zabbix: a powerful yet simple monitoring software"},{"content":"","href":"/tags/infrastructure/","title":"infrastructure"},{"content":"","href":"/tags/monitoring/","title":"monitoring"},{"content":"","href":"/tags/zabbix/","title":"zabbix"},{"content":"Ieri in ufficio mi è capitata una disgrazia, una rara occasione in cui non c\u0026#8217;è capacità o competenza che possano aiutarti a risolvere il caso, serve solo fortuna: nel pomeriggio rientro dalla sigaretta e mi sento dire \u0026#8220;non abbiamo più rete\u0026#8221;.\nL\u0026#8217;azienda dove lavoro si occupa di fornire servizi IT, senza la connessione internet siamo un\u0026#8217;auto senza motore, una penna senza inchiostro, inutili.\nLa linea di backup?\nAll\u0026#8217;inizio incolpo subito il nostro ISP (Fastweb), che a discapito di una fibra 100Mb ci ha giocato in passato scherzi del genere; quando la verifica evidenzia che non è un loro problema, il panico.\nAttiviamo la linea di backup: una WiFi 7Mb Telecom, penosa per servire il traffico che ci serve.\nCon il mio fedele collega Dario iniziamo a cercare la fonte del guasto. Andiamo alla sala macchine e tentiamo di capire dove sia il problema: un cavo collegato male? No. Saltata la corrente su uno degli interruttori di rete? No. L\u0026#8217;impresa delle pulizie ha distrutto lo switch?!? No. Il router è fritto!!\nL\u0026#8217;unico apparato di rete per cui non abbiamo sostituti, l\u0026#8217;unica macchina fondamentale di cui non si può fare a meno è guasta: e non c\u0026#8217;è porta secondaria, configurazione o altro che ci salvi! Vegno subito preso d\u0026#8217;assalto dal DG, che tra l\u0026#8217;altro mi ricorda che l\u0026#8217;indomani è prevista una riunione in sede con dei clienti. Rabbrividisco al pensiero.\nIl tutto rimane in stand-by fino alla telefonata del mio capo, a casa malato, a cui devo comunicare la triste notizia. Lo chiamo e la cosa non lo rende affatto felice, e mentre sto per darmi per vinto lancio uno sguardo alla borsa sotto la mia scrivania: c\u0026#8217;è un router lì dentro, il vecchio Dlink che usavo per simulare le reti dei clienti! Sarà anche una schifezza ma è sempre un router! Mi precipito al pc attacco il giocattolo e lo configuro pari al vecchio (con le sue minori funzioni) per servire la nostra LAN, vado in sala macchine, attacco i due cavi e\u0026#8230;miracolo! Il problema è risolto! Ed è bastata un\u0026#8217;occhiata al momento giusto dove non avresti guardato mai! Che dire\u0026#8230;CULO!\n","href":"/2013/02/28/2013-02-28-quando-si-dice-problem-solving/","title":"Quando si dice problem solving"},{"content":"Ieri mi sono trovato con parte della mia famiglia nella loro sala a suonare un po\u0026#8217; il pianoforte, mio fratello Gianmaria e io ci siamo esibiti riproducendo la semplice (ma efficace) musica che accompagna la pubblicità dell\u0026#8217;iPad Mini.\nIl motivetto è in Do maggiore così come viene proposto suonando i due iPad nella pubblicità, di seguito il video. Enjoy!\n\u0026nbsp;\n","href":"/2012/11/26/2012-11-26-ipad-mini-la-musica-dello-spot-tv/","title":"iPad Mini: la musica dello spot TV"},{"content":"","href":"/tags/ipad-mini-music-spot-tv/","title":"ipad mini music spot tv"},{"content":"","href":"/categories/music/","title":"music"},{"content":"In Italia abbiamo molti problemi, ma l\u0026#8217;ultimo dei nostri problemi era l\u0026#8217;istruzione che vantava uno dei primi posti al mondo fino a pochi anni fa. Ora invece il declino strutturale del nostro Paese ha colpito anche questo settore che era di valore. Parlo di mancanza di fondi per università e ricerca, di formazione non orientata al lavoro e tutto ciò che ne consegue: l\u0026#8217;aumento della disoccupazione giovanile e della sfiducia nel futuro è preoccupante perché è nella stesso ordine di grandezza di stati molto indietro rispetto alla media europea.\nPer fortuna l\u0026#8217;Italia è nota per le sue particolarità e anche in questo caso voglio esaltare un esempio di eccellenza di cui sono venuto a conoscenza per puro caso: pochi giorni fa infatti ho preso i mezzi per andare a seguire un lavoro fuori ufficio (evento più che raro) e durante il viaggio di ritorno ho incontrato la sorella di un\u0026#8217;amica d\u0026#8217;infanzia. Maddalena è parecchio cresciuta dall\u0026#8217;ultima volta che l\u0026#8217;avevo vista, si è laureata in psicologia clinica da un anno ed è in procinto di finire il praticantato che le permetterà di fare l\u0026#8217;esame di stato e raggiungere l\u0026#8217;abilitazione alla professione. Senza stare a discutere di questo ulteriore scempio di tempo (12 mesi in cui, per prassi, non si percepisce stipendio) imposto dalla legge italiana, voglio raccontare di quale coincidenza abbia voluto che proprio il giorno precedente mia sorella mi avesse dichiarato \u0026#8220;mi piacerebbe fare psicologia all\u0026#8217;università\u0026#8221;. Parlando mi ha spiegato che il corso universitario è diverso da come ce lo si aspetta: al primo anno ci sono molti esami scientifici, che sono un vero e proprio scoglio per le matricole, con conseguente scrematura degli iscritti; il \u0026#8220;divertimento\u0026#8221; per gli appassionati inizierà dal terzo anno e culminerà con la tesi.\nMaddalena è stata tanto gentile da darmi il permesso di mettere a disposizione di chi volesse informarsi sulla facoltà di psicologia una presentazione che lei stessa ha scritto per degli alunni del liceo: la trovate allegato in fondo al post. Il contenuto è sua proprietà intellettuale. Questi sono gli esempi da cui tutti dovrebbero prendere ispirazione: dedicare del tempo a divulgare informazioni su un corso universitario aiuta molto chi deve fare una scelta importante come quella dell\u0026#8217;università.\nGrazie Maddalena\nPresentazione_psicologia\n","href":"/2012/10/05/2012-10-05-universita-facolta-di-psicologia/","title":"Università: facoltà di psicologia"},{"content":"Stamattina ho lavorato all\u0026#8217;Arena Civica di Milano come tecnico del suono. La manifestazione che ho seguito era la cerimonia di chiusura del Ramadan islamico, l\u0026#8217;equivalente della Pasqua cristiana. La cerimonia di per sè è stata breve, circa un\u0026#8217;ora, ed è stato affascinante assistere a una manifestazione religiosa a cui non avevo mai assistito prima. Il programma è stato il seguente:\n raduno dei fedeli nel campo dell\u0026#8217;Arena presentazione dei celebranti del rito preghiera sermone sfollaggio  Innanzitutto ciò che mi ha colpito è stata la mole di persone che erano presenti: circa 15000 tra uomini donne e bambini; da notare che la religione islamica non conta solo fedeli dal medio oriente, gran parte infatti erano di origine africana e indonesiana. La seconda cosa che mi ha pietrificato è stata che all\u0026#8217;inizio dei canti, un inserviente della comunità musulmana mi è venuto a chiedere di avere dei microfoni per la platea. Normalmente alle funzioni religiose è chi celebra che diffonde l\u0026#8217;audio verso la folla, mentre loro mi chiedevano addirittura di zittire durante i canti il loro \u0026#8220;parroco\u0026#8221; perchè non aveva una bella voce! Stupito, e un po\u0026#8217; impaurito di commettere un errore, ho comunque eseguito l\u0026#8217;ordine e il risultato è stato apprezzatto perfino dal parroco stesso. Il momento più solenne e sicuramente il più spettacolare è stato quello della shallah: è una preghiera liturgica, che è intonata dal loro \u0026#8220;vescovo\u0026#8221; e a cui tutta la folla risponde in coro, rigorosamente in arabo. E qui ho avuto la terza sorpresa: qualunque musulmano prega in arabo, cioè non hanno una traduzione delle loro preghiere nelle altre lingue. Affascinante il fatto che nigeriani, bangra, e marocchini stessero intonando le stesse litanie in un\u0026#8217;unica lingua: qualcosa di impensabile per altre religioni ampiamente diffuse nel mondo. I video della preghiera principale qui di seguito:\n \n \n Quindi al termine della cerimonia è iniziato il deflusso e io ho cominciato a preparare il mio lavoro per smontare l\u0026#8217;impianto. Mi aspettavo di trovare l\u0026#8217;Arena vuota in una manciata di minuti, invece un buon migliaio di persone è rimasto per aiutare gli organizzatori a pulire, raccogliere i gazebi, accompagnare i bambini smarriti verso i propri genitori, con uno spirito di fratellanza che raramente ho osservato in passato. Un paio di ragazzi si sono anche offerti per aiutarmi nel mio lavoro\u0026#8230; Questa è stata la cosa più bella della giornata: la partecipazione dei fedeli oltre la cerimonia, oltre la preghiera cantata e nel concreto delle azioni per la comunità.\nA chi non avesse mai visto una cerimonia del genere, consiglio di assisterci. A chi non avesse mai pensato a partecipare attivamente e concretamente nella vita dei propri simili, consiglio di rifletterci.\n","href":"/2012/08/20/2012-08-20-ho-capito-il-significato-della-parola-islam/","title":"Il significato della parola Islam"},{"content":"","href":"/categories/politics/","title":"politics"},{"content":"Una cosa che adoro di questo millennio è l\u0026#8217;abbondanza di dati: nella storia l\u0026#8217;uomo ha sempre cercato di tramandare ai posteri la propria vita e io credo che i dati raccolti e archiviati dall\u0026#8217;umanità nel secolo scorso siano in valore di molte volte superiori a ogni calendario Maya o testo sacro. Trovo fantastica la sensazione che si prova guardando un pezzo comico di Valter Chiari di 50 anni fa e immedesimandomi nei miei genitori per esempio, che hanno assistito alle stesse scene. Certo gli usi e costumi sono cambiati, la comicità, la musica e l\u0026#8217;arte in genere non possono avere le stesse rappresentazioni del passato perchè non rifletterebbero la realtà.\nProprio pensando a questo mi sono trovato qualche giorno fà a riflettere su una canzone di quel periodo: \u0026#8220;Il ballo del Mattone\u0026#8221; di Rita Pavone (1963). Chiunque può essere certo di conoscere questo grande successo del passato, se gliene intoni il ritornello. Qui sotto la canzone, fondamentale ascoltarla prima di continuare a leggere l\u0026#8217;articolo.\n\u0026nbsp;\nhttps:////www.youtube.com/embed/zPDTWuJqkAg\nNon c\u0026#8217;è che dire, un capolavoro, il cui successo popolare è giustificato dallo stesso movente di tutte le canzoni moderne: l\u0026#8217;amore. Questa è una canzone d\u0026#8217;amore e lo si capisce prima dal testo e poi dalla musica. Dal testo ovviamente si capisce che seppur Rita si diverta a ballare con altri ragazzi, ci sia solo un uomo nel suo cuore, con cui lei balla il ballo del mattone. Inoltre la musica, che inizia come un classico twist, ha un cambiamento di ritmo e tonalità proprio quando Rita spiega come si balla il ballo del mattone: \u0026#8220;\u0026#8230;lentamente, guancia a guancia, io ti dico che ti amo, tu mi dici che son bella, dondolando, dondolando sulla setssa mattonella\u0026#8221;. Questo pezzo è quello che mi ha fulminato dopo un\u0026#8217;attenta riflessione che voglio condividere: musicalmente è facile notare come dal movimento e allegria del twist si passi a tonalità più romantiche e morbide, come quelle di un lento. Ma è nel testo che bisogna ricercare il vero significato: la mattonella, secondo me è un materasso, che del mattone può avere la forma. E cosa fanno i due innamorati su quel mattone, lentamente, guancia a guancia, mentre si dicono che si amano? L\u0026#8217;amore.\nUna metafora fantastica, accompagnata musicalmente e trasmessa senza un filo di volgarità. Ecco cosa ascoltavano i nostri genitori quando avevano la nostra età, forse anche dieci anni di meno. I tempi sono cambiati, oggi una canzone del genere non avrebbe senso musicalmente e nemmeno verbalmente: non è più obbligatorio nascondere temi emotivi o sessuali dietro ad analogie così raffinate. Tutto molto più semplice e diretto, i messaggi verbali e musicali sono diventati usa e getta, come i prodotti che consumiamo. Oggi si può comunicare lo stesso episodio con quest\u0026#8217;altra canzone. Godetevela.\n\u0026nbsp;\n","href":"/2012/07/30/2012-07-30-musica-e-linguaggio-uninvoluzione-al-passo-coi-tempi/","title":"Musica e linguaggio: un'involuzione al passo coi tempi"},{"content":"Ed eccolo!!\nIl mashup che stavate aspettando dopo la cena con più argomenti di economia della mia vita!\nImpressionante\u0026#8230;\nhttp://inge.4pr.es/files/2014/07/tranx-mashup.ogg\n","href":"/2012/07/10/2012-07-10-discorsi-economici-finiti-in-un-mashup/","title":"Discorsi economici finiti in un mashup!"},{"content":"Mio amato ferro da stiro, è da 4 mesi che ci conosciamo e il nostro rapporto è intenso e caloroso tanto quanto la tua piastra in acciaio Inox. Sei un compagno di molte sere in cui, poichè non ho proprio niente da fare, ti accendo e passiamo insieme momenti fantastici mentre lisci le pieghe dei miei vestiti\u0026#8230; Devo dire che era da molto che non avevo un rapporto così intenso e infatti, parafrasando la fantastica canzone di Marco Ferradini\n \u0026#8230;chi è troppo amato amore non dà\u0026#8230;\n io te lo devo proprio dire: quando le camicie hanno iniziato a invadere il nostro tempo insieme, è cominciata una caduta libera dell\u0026#8217;affetto che provavo per te. Un\u0026#8217;inesorabile odio verso te è venuto a galla, tu che sei strumento dal contenuto tecnologico elevatissimo, baluardo di una rivoluzione industriale da cui sono passati oltre cento anni.\nSo che queste parole potranno ferire i tuoi circuiti ma anche io ho una dignità e non voglio più essere succube di questa relazione: certo avremo ancora momenti per incontrarci, tra una una maglietta e un asciugamano, e perchè no anche per delle lenzuola, ma non sarà mai come prima. Non voglio che tu soffra, non permetterei mai che tu arrugginissi all\u0026#8217;ombra di un cassetto umido, ma non posso continuare così. Mi hai dato momenti di grande soddisfazione in passato, delle soddisfazioni che solo i fornelli possono dire di aver provocato (ebben sì ti ho tradito molte volte) ma è giunto il momento che io lasci spazio a chi ti sa capire, a chi ti può dare quello di cui hai bisogno: una mano ferma e una passata precisa, tanta acqua osmotizzata per la tua salute e un posto in primo piano come io non ho mai saputo darti\u0026#8230;\nC\u0026#8217;è chi ti usa per fare un mestiere, chi ha le capacità e l\u0026#8217;ambizione di volerti utilizzare 6-8 ore al giorno e sentirsi appagato da ciò e io devo portare rispetto per questo. Devo riconoscere che esiste una tecnica, che in fondo stirare è come suonare uno strumento e io proprio non riesco a sentire l\u0026#8217;energia necessaria ad esprimere le tue capacità.\nCon affetto,\ninge4pres  goodbye ferro   \n","href":"/2012/07/09/2012-07-09-ferro-da-stiro-che-passione/","title":"Ferro da stiro, mon amour\u0026#8230;"},{"content":"Ho appena avuto la fortuna di fare un incontro, un incontro speciale di quelli che ti restano impressi per sempre. Non credo nel destino o fato o casualità che dir si voglia, seppure una serie di circostanze che hanno portato a questo incontro potrebbero farmi cambiare idea.\nLa storia è questa: da pochi mesi ho deciso di inziare con mio fratello Gianmaria la ristrutturazione di uno studio musicale. Lo studio è stato costruito da mio padre 25 anni fa ed era in disuso. Iniziando a fare pulizia sono state ritrovate due casse (termine tecnico diffusori) impolverate, che si pensava fosse giusto buttare. Quando mio padre le ha viste ha detto che saremmo stati pazzi a volerle buttare in quanto sono un capolavoro di tecnologia degli anni \u0026#8217;70 e che la cosa più saggia sarebbe stata farle riparare per usarle nel futuro studio.\n Mio padre si prese anche la briga di suggerire il riparatore a cui avremmo dovuto portarle. Ripensandoci ora, sono davvero felice che mi abbia dato quel consiglio; il sig. Chiesa (Riky, come vuol essere chiamato) è davvero una persona che è in grado di farti respirare la sua passione per i diffusori ad ogni parola. Sono andato nel suo laboratorio dopo il lavoro convinto che avrei potuto passare il resto della giornata seguendo i miei programmi, ne sono uscito un\u0026#8217;ora più tardi dimenticando tutto quello che avevo da fare: Riky è un appassionato e vive la sua passione ogni giorno nel suo laboratorio, facendo quello che ama fare.\nAppena entrato mi ha dato una stretta di mano e ha guardato subito i diffusori: ha iniziato a raccontarmi la loro storia nei dettagli che solo un appassionato può conoscere e ricordare, le loro caratteristiche tecniche, i punti di forza e le pecche. Sono rimasto ammaliato dalla sua voce e dalla sua padronanza dell\u0026#8217;argomento e ho inziato a fargli domande tecniche e non. Non sono rimasto sorpreso quando, rispondendo ad ogni domanda con precisione, cercava con gesti ed esempi di farmi capire quanto più potesse riguardo l\u0026#8217;argomento: chi vive le proprie passioni ha sempre voglia di condividere la sua conoscenza, perchè vuole trasmettere quel senso di attaccamento all\u0026#8217;interlocutore che ha davanti.\nQuesto episodio mi ha fatto pensare a come io vivo la mia passione per la musica, a come cerco di trasmetterla e condividerla con chi mi sta intorno. Mi ha fatto riflettere su questo blog e il senso che ha comunicare e vivere la propria passione; sicuramente mi ha cambiato la giornata in meglio!\n\u0026nbsp;\n Il suo sito: http://www.rikychiesa.com/\n","href":"/2012/05/23/2012-05-23-vivere-le-proprie-passioni-con-passione/","title":"Vivere le proprie passioni, con passione!"},{"content":"","href":"/tags/equitalia/","title":"Equitalia"},{"content":"Come dice Oscar Giannino, giornalista e conduttore radiofonico, nonchè grande conoscitore dei meccanismi dell\u0026rsquo;economia italiana, lo stato italiano invece di salvaguardare i propri cittadini, li deruba, li sfrutta, si prende gioco di loro; per utilizzare una frase del sempre impeccabile editorialista bisogna cercare di difendersi dallo \u0026#8220;Stato LADRO!!!\u0026#8221;.\nLa situazione dell\u0026#8217;economia italiana non è delle migliori, anzi, proprio oggi l\u0026#8217;ISTAT ha pubblicato un report che indica come il reddito pro capite è uguale a quello del 1992 e fatti estremi stanno accadendo sempre più numerosi giorno dopo giorno.\nFatta questa premessa, il mio interesse era portare alla luce un fatto che dimostra quanto lo Stato sia veramente indecente nei riguardi dei cittadini onesti, ed invece che stare dalla loro parte, fomenta insistentemente un clima di rancore e odio che poi porta in certi casi ad azioni che si vorrebbe evitare.\nIl sottoscritto nel 2009 riceve una multa di 86 Euro per ingresso in zona Ecopass non seguita all\u0026#8217;acquisto del ticket da 5 euro che lo avrebbe salvato dal già esagerato importo della contravvenzione. Per ingenuità infatti, non avendo visto la telecamera, e avendo attraversato una via di 60 metri nei pressi di Piazza Conciliazione, non mi ero accorto di essere entrato nella \u0026#8220;zona proibita\u0026#8221; per 5 secondi di una giornata, per poi uscirne immediatamente quando la strada confluiva nella piazza di questa zone centrale di Milano.\nBene, dopo aver pagato la multa (86 Euro) dopo 67 giorni (ebbene si, 7 giorni di ritardo dopo i 60 previsti, anche qui per ingenuità), sempre il sottoscritto si è visto recapitare in data odierna (MERCOLEDI 22 MAGGIO 2012!!!!! Ripeto\u0026#8230; MERCOLEDI 22 MAGGIO 2012!!!), dopo quasi 3 anni, una cartella esattoriale da parte di Equitalia che indica come debba pagare 105 Euro (avete capito bene\u0026#8230; CENTOCINQUE EURO!!!) entro 60 giorni, comprensivi di interessi di mora (importo destinato ad aumentare qualora non paghi entro i 2 mesi prestabiliti), per via del ritardo (di 7 giorni!!!!!) nel pagamento della suddetta multa!!\nOra io dico\u0026#8230;. se mi fossi fatto prestare i soldi l\u0026#8217;ultimo giorno disponibile per pagare, dallo strozzino peggiore che possa esistere sulla faccia del pianeta, e glieli avessi restituiti dopo 7 giorni, probabilmente, sommando quanto devo pagare adesso, avrei potuto fare una bella cena di pesce in un ristorante nel centro di Milano e alla fine aver pagato gli stessi soldi di adesso\u0026#8230;\nA voi le riflessioni. A me l\u0026#8217;amaro in bocca\u0026#8230;\nAndiamo avanti cosi\u0026#8230; in questo paese sempre più di merda\u0026#8230;\n","href":"/2012/05/22/2012-05-22-lennseima-dimostrazione-dello-stato-ladro/","title":"L'ennesima dimostrazione dello stato ladro!"},{"content":"","href":"/tags/ecopass/","title":"ecopass"},{"content":"","href":"/tags/multa/","title":"multa"},{"content":"Ho sempre vissuto in città di provincia (MI) dove è abitudine frequentare una cerchia ristretta di persone. Questo mi ha permesso di stringere legami molto forti con chi considero i miei amici, ma allo stesso tempo non mi ha favorito nello sviluppo di una sensibilità verso la gestione delle informazioni personali nella vita reale; la mia lingua lunga ha solo fatto piovere sul bagnato\u0026#8230;\n Il paese è piccolo e la gente mormora\u0026#8230;\n recita il proverbio: ho constatato che è vero in molte occasioni.\nNon tutti i mali vengono per nuocere: nella mia vita digitale ho saputo imparare dagli errori commessi nela vita parallela e ho sempre dato un occhio di riguardo alla privacy online e tutto ciò che consegue allo scambio di informazioni tramite internet. Anche internet, infatti, con le sue milioni di miliardi di connessioni digitali può essere considerato un paesino di provincia, dove tutti possono venire a sapere tutto.\nConsideriamo un esempio: Alice e Roberto (sempre chiamati in causa in questo genere di esempi), hanno una relazione che vogliono mantenere segreta. I due non si incontrano mai in pubblico, ma volentieri si scambiano messaggi passionali attraverso il telefonino e il computer. Un giorno Alice sta chattando con Roberto via Facebook quando per sbaglio il mouse le cade e viene cliccato questo link. Alice capita sulla pagina che descrive le regole con cui Facebook registra e archivia le informazioni a lei correlate e incuriosita legge interamente il contenuto. Scopre essenzialmente che:\n Facebook registra qualsiasi attività dell\u0026#8217;utente svolta all\u0026#8217;interno del sito (visite di profili, like, post cancellati, chat) queste informazioni sono usate a fini statistici e sono condivise con altre aziende (pubblcitarie, di applicazioni, etc\u0026#8230;) è possibile richiedere una copia dei propri dati riempiendo un modulo e ricevenrli su un CD/DVD  L\u0026#8217;indomani Roberto torna da un viaggio di lavoro e chiede ad Alice di incontrarsi; la avvisa inoltre che il suo telefonino gli è stato rubato in aereoporto. In quel momento Alice è fulminata da un pensiero: e se il ladro del cellulare fosse a conoscenza del modulo per ricevere i dati da Facebook e usasse il telefono di Roberto per farne richiesta? A quel punto avrebbe a disposizione tutte le conversazioni tra i due e potrebbe approfittarne per ricattarli! Non vorrei essere al posto di Roberto quando si incontreranno\u0026#8230;\nGeneralizzando dall\u0026#8217;esempio, ogni compagnia che opera online ha il potere di acquisire dei dati: dalla sola mail e password in uso anni fa, al web 2.0 di oggi basato sui contenuti generati dagli utenti. Ogni compagnia si deve dotare quindi di politiche di detenzione e divulgazione di queste informazioni; vorrei che questo concetto fosse più ampiamente diffuso tra chi fa uso di questi servizi.\nNon è raro infatti considerare più sicuro e riservato scambiarsi informazioni via internet che a parole; se domando \u0026#8220;Ci sentiamo per organizzare la festa a sorpesa di X?\u0026#8221; mi sento rispondere \u0026#8220;Creo un gruppo su Facebook\u0026#8221; e rimango perplesso: come si può pensare che sia meno rintracciabile o più segreta una serie di caratteri leggibili e che viaggiano andata e ritorno per l\u0026#8217;oceano Atlantico, rispetto alle parole sussurrate direttamente all\u0026#8217;orecchio di chi deve sentirle?\n","href":"/2012/05/21/2012-05-21-privacy-e-riservatezza-il-digital-divide/","title":"Privacy e riservatezza: il digital divide"},{"content":"Il blog è tornato in vita dopo 24 ore dalla sua distruzione totale a causa di un mio errore di configurazione. Ho anche perso i primi tre post perchè le copie di backup non funzionavano\u0026#8230; :\u0026lsquo;(\nBeh tutti sbagliano: alla PIXAR hanno distrutto il lavoro di 4 mesi su Toy Story 2 per un comando sbagliato lanciato sui server di storage\u0026#8230;guarda il video!\n","href":"/2012/05/18/2012-05-18-blog-re-start/","title":"Blog RE-start"},{"content":"","href":"/categories/uncategorized/","title":"Uncategorized"},{"content":"","href":"/","title":""}]
