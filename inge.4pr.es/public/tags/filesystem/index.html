<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    filesystem - inge4pres
  </title><meta name="generator" content="Hugo 0.98.0" /><link
    rel="stylesheet"
    href="https://inge.4pr.es/css/styles.css"
    integrity=""
  />
  
  <script>
    
    if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
      document.documentElement.classList.add("dark");
    } else {
      document.documentElement.classList.remove("dark");
    }
  </script>
  
</head>

  <body
    class="flex flex-col min-h-screen dark:bg-gray-900 dark:text-gray-100 transition-colors duration-500"
  ><header class="w-full px-4 pt-4 max-w-5xl mx-auto">
  <nav class="flex items-center justify-between flex-wrap">
    <div class="flex gap-2 items-center">
      
      <a href="mailto:fgualazzi@gmail.com" aria-label="EMail">
        <svg
          xmlns="http://www.w3.org/2000/svg"
          width="20"
          height="20"
          viewBox="0 0 24 24"
          stroke-width="1.5"
          stroke="currentColor"
          fill="none"
          stroke-linecap="round"
          stroke-linejoin="round"
        >
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <circle cx="12" cy="12" r="4" />
          <path d="M16 12v1.5a2.5 2.5 0 0 0 5 0v-1.5a9 9 0 1 0 -5.5 8.28" />
        </svg>
      </a>
      
      <a href="https://inge.4pr.es/" class="flex items-center font-bold">
        inge4pres
      </a>
    </div>

    <ul id="nav-menu" class="flex w-auto mt-0 space-x-2">
      
      <li>
        <a href="https://inge.4pr.es/about/" class="hover:text-blue-800 dark:hover:text-blue-300">You are what you is (F. Zappa)</a>
      </li>
      
      
      <li>
        <a href="https://inge.4pr.es/categories/blog/" class="hover:text-blue-800 dark:hover:text-blue-300">blog</a>
      </li>
      
    </ul>
  </nav>
</header>
<main class="flex-1 mx-4 md:mx-12 lg:mx-24 mt-8 sm:mt-16"> 
<article class="sm:mx-12 mb-16 prose lg:prose-lg">
  <h1><a href="https://inge.4pr.es/post/2016-09-18-a-benchmark-of-aws-efs/">A benchmark of AWS EFS</a></h1>
  <p>Amazon Web Services <!-- raw HTML omitted -->Elastic File System<!-- raw HTML omitted --> has been to my knowledge the service to have the longest beta testing period: reason for this may be that not as many client as expected tested it and AWS received too few feedback on it or that there were issues not to release GA. I don’t want to speculate on which one is correct but now that it has been <!-- raw HTML omitted -->officially released<!-- raw HTML omitted --> I decided to give it a try and of course compare it to a self-managed solution on the same platform.</p>
<p>If you followed AWS evolution you may agree that EFS has been introduced to fill the gap between EBS storage and S3: before EFS was live there was no “easy” way of having a distributed file system in AWS, you could only <!-- raw HTML omitted -->set up your own<!-- raw HTML omitted --> using  a combination of EC2 instances mounting Elastic Block Storage volumes and S3. Now with EFS you can have a AWS-managed distributed file system to be used in your cloud environment or even across the internet (will try that on a public subnet) with all the benefits of offloading the high-availability and replication burden to Amazon, and at a reasonable price. Will performance be enough compared to a self-managed solution?</p>
<h5 id="playground">Playground</h5>
<p>I use <!-- raw HTML omitted -->terraform<!-- raw HTML omitted --> to create an infrastructure template to run the tests, you can see it <!-- raw HTML omitted -->here<!-- raw HTML omitted -->. Once</p>
<!-- raw HTML omitted -->
<p>has finished, you’ll end up with:</p>
<ul>
<li>An EFS with General Purpose performance mode</li>
<li>An EFS mount target for 1 Availability Zone</li>
<li>1 EC2 instance named “client” to mount remote file systems</li>
<li>2 EC2 instances named “server_X” each one with a 10 GB General Purpose EBS, they will serve a self-managed distributed, replicated file system</li>
</ul>
<p>This is the terraform output and the steps to run on the 2 server nodes to have a running GlusterFS replicated volume; to configure the NFS export on server1, I used <!-- raw HTML omitted -->this guide<!-- raw HTML omitted -->.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>On the client I mount the EFS target with NFS4.1, the GlusterFS volume from the server <em>in the same subnet</em> via the GlusterFS native client_ _and the NFS export on the client’s designated mount points. I use the server on the same subnet as the client is, because the EFS target exposes a mount point in the same subnet and latency is a key factor in remote file system.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h5 id="benchmark-tests">Benchmark Tests</h5>
<p>I used <!-- raw HTML omitted -->fio<!-- raw HTML omitted --> installed on the client box with a command suggested by this <!-- raw HTML omitted -->BinaryLane post<!-- raw HTML omitted --> and run it against the mount point for EFS, GlusterFS and NFS v4.0 with the following command</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>changing the target directory each time I tun the test; each test is run isolated.</p>
<p>I did not customize any storage option for GlusterFS or NFS, so I’m using the default options.</p>
<h5 id="benchmarkresults">Benchmark Results</h5>
<h6 id="efs">EFS</h6>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h6 id="glusterfs">GlusterFS</h6>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h6 id="nfs-40-not-replicated">NFS 4.0 (not replicated)</h6>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h6 id="pricing-considerations">Pricing considerations</h6>
<p>EFS pricing is linear, you get billed a fixed amount for GB/month; this is not true with a self-managed cluster where you can surely reach higher performance, but the TCO is increasing every time you add capacity. If you’re not satisfied with EFS throughput you need a dedicated team to  manage a distributed file system cluster and its operations and maintenance.</p>
<h6 id="conclusions">Conclusions</h6>
<p>If you need a stable, realiable file system in AWS to be shared between EC2 instances, go and use EFS and don’t reinvent the wheel: GlusterFS is outperformed in both read and write IOPS and bandwith with the default options! The workload simulated here is showing poor write performance, so if your use case is a lot of concurrent writes on many files, consider another solution. A good workload could be a WORM (Write Once Read Many) share for permanent stored contents (images, archives?).</p>
<p>The performance are still low in absolute terms or compared to a non-replicated mount such as a stock NFS v4.0 server without the high-availability burden, but if you don’t care about 100% uptime you should definitely set up NFS with <!-- raw HTML omitted -->DRDB<!-- raw HTML omitted --> to a secondary node, and switch your mounts when the primary node fails. But still: why manage all of this if AWS can do it for you?</p>

</article>
 
    </main><footer class="w-full text-center p-4 text-xs text-gray-400">
  <p>
    Built with
    <a
      href="https://gohugo.io"
      target="_blank"
      rel="noopener noreferrer"
      class="underline hover:text-blue-800 dark:hover:text-blue-300"
      >Hugo</a
    >
    and
    <a
      href="https://github.com/apvarun/showfolio-hugo-theme"
      target="_blank"
      rel="noopener noreferrer"
      class="underline hover:text-blue-800 dark:hover:text-blue-300"
      >Showfolio</a
    >
  </p>
</footer>

</body>
</html>
